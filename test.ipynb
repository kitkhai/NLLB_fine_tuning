{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\wong.kk\\AppData\\Local\\anaconda3\\envs\\nllb_env\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\wong.kk\\AppData\\Local\\anaconda3\\envs\\nllb_env ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# !pip install datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.load('nllb-200-600M-onmt.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model', 'generator', 'vocab', 'opt'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoder.embeddings.make_embedding.emb_luts.0.weight': tensor([[-0.0321,  0.0348,  0.0181,  ...,  0.0312, -0.0099, -0.0133],\n",
       "         [-0.0039,  0.0104, -0.0156,  ...,  0.0290, -0.0138, -0.0134],\n",
       "         [-0.0245, -0.0283, -0.0295,  ...,  0.9712, -0.0255, -0.0273],\n",
       "         ...,\n",
       "         [-0.0123, -0.0031, -0.0089,  ...,  0.0645, -0.0182, -0.0740],\n",
       "         [ 0.0085, -0.0088, -0.0091,  ...,  0.0571, -0.0035, -0.1298],\n",
       "         [-0.0076, -0.0107, -0.0051,  ...,  1.0264, -0.0338, -0.1175]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.embeddings.make_embedding.pe.pe': tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  1.0000,  1.0000]],\n",
       " \n",
       "         [[ 0.8415,  0.8317,  0.8218,  ...,  1.0000,  1.0000,  1.0000]],\n",
       " \n",
       "         [[ 0.9093,  0.9236,  0.9365,  ...,  1.0000,  1.0000,  1.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.9563,  0.5417,  0.7653,  ...,  0.8688,  0.8733,  0.8777]],\n",
       " \n",
       "         [[ 0.2705,  0.9999,  0.9649,  ...,  0.8687,  0.8733,  0.8777]],\n",
       " \n",
       "         [[-0.6639,  0.5685,  0.3343,  ...,  0.8687,  0.8732,  0.8776]]]),\n",
       " 'encoder.transformer.0.self_attn.linear_keys.weight': tensor([[ 5.9570e-01,  6.5479e-01,  5.9863e-01,  ...,  1.2024e-01,\n",
       "          -1.4905e-01, -1.1023e-01],\n",
       "         [ 3.1909e-01, -1.8079e-01, -2.6294e-01,  ..., -2.0264e-01,\n",
       "           1.9861e-01,  4.2529e-01],\n",
       "         [ 5.2148e-01,  8.4229e-01,  9.9951e-01,  ...,  2.0679e-01,\n",
       "           2.0203e-01, -5.2261e-03],\n",
       "         ...,\n",
       "         [ 1.3794e-01, -1.1102e-01, -5.5176e-01,  ..., -3.5400e-03,\n",
       "           4.2267e-02, -3.9399e-05],\n",
       "         [-1.5588e-01,  6.3293e-02,  1.2901e-02,  ..., -3.8452e-01,\n",
       "           1.5308e-01,  2.8540e-01],\n",
       "         [-2.2595e-01, -9.3994e-02,  3.0981e-01,  ..., -1.6748e-01,\n",
       "          -7.5378e-02, -1.4185e-01]], dtype=torch.float16),\n",
       " 'encoder.transformer.0.self_attn.linear_keys.bias': tensor([ 0.0179,  0.0283,  0.0196,  ..., -0.0025, -0.0250,  0.0193],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.0.self_attn.linear_values.weight': tensor([[ 0.0630,  0.0343, -0.0231,  ..., -0.0668, -0.1157, -0.0081],\n",
       "         [ 0.0947,  0.0685,  0.0461,  ..., -0.0442, -0.1248, -0.0464],\n",
       "         [ 0.0397,  0.0829,  0.0338,  ...,  0.0314, -0.0782, -0.1476],\n",
       "         ...,\n",
       "         [-0.2808, -0.0279,  0.0878,  ...,  0.0007, -0.0660, -0.0214],\n",
       "         [ 0.0296,  0.0190, -0.0400,  ...,  0.0379, -0.0027, -0.2717],\n",
       "         [-0.0427, -0.0203, -0.0764,  ...,  0.0536, -0.0583,  0.1981]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.0.self_attn.linear_values.bias': tensor([-0.0758, -0.1368, -0.0085,  ..., -0.0319,  0.0159, -0.0054],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.0.self_attn.linear_query.weight': tensor([[ 0.1394, -0.0477, -0.5269,  ..., -0.0635,  0.2659,  0.1715],\n",
       "         [-0.2218, -0.3704, -0.5278,  ..., -0.2737, -0.0536, -0.3032],\n",
       "         [ 1.0117,  1.0215,  0.9995,  ..., -0.0361, -0.1837,  0.1039],\n",
       "         ...,\n",
       "         [-0.1182, -0.1311, -0.4141,  ...,  0.3547, -0.1899,  0.1781],\n",
       "         [-0.1536,  0.0272,  0.0038,  ...,  0.2732, -0.1216,  0.3218],\n",
       "         [-0.2279, -0.1813,  0.2534,  ...,  0.2791, -0.2869, -0.0817]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.0.self_attn.linear_query.bias': tensor([ 0.0445, -0.6504,  0.0458,  ..., -0.0200, -0.2649, -0.0813],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.0.self_attn.final_linear.weight': tensor([[-0.0432, -0.1025, -0.0201,  ...,  0.0115, -0.0321, -0.1892],\n",
       "         [-0.0038, -0.0169, -0.0073,  ...,  0.0214,  0.0050,  0.0173],\n",
       "         [ 0.0013, -0.0362,  0.0130,  ..., -0.0028, -0.0018,  0.0048],\n",
       "         ...,\n",
       "         [-0.0644,  0.3677,  0.1345,  ...,  0.2361, -0.3904,  0.1660],\n",
       "         [ 0.0199, -0.1136, -0.2612,  ..., -0.1603,  0.0699, -0.0349],\n",
       "         [-0.1918, -0.1236, -0.0550,  ...,  0.0294,  0.1755,  0.0011]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.0.self_attn.final_linear.bias': tensor([-1.2274e-01, -5.3802e-02, -8.2552e-05,  ..., -1.1182e-01,\n",
       "          1.1978e-02, -4.8340e-02], dtype=torch.float16),\n",
       " 'encoder.transformer.0.layer_norm.weight': tensor([0.1989, 0.7793, 1.0020,  ..., 0.0681, 0.1104, 0.1191],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.0.layer_norm.bias': tensor([ 0.0027,  0.0109,  0.0106,  ..., -0.0605,  0.0066,  0.0079],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.0.feed_forward.w_1.weight': tensor([[-0.0822, -0.0664, -0.0421,  ..., -0.2252, -0.0174, -0.0901],\n",
       "         [-0.2766,  0.0712, -0.0408,  ...,  0.0138,  0.1761,  0.0470],\n",
       "         [-0.0440,  0.0100, -0.1147,  ...,  0.3438, -0.1202, -0.3132],\n",
       "         ...,\n",
       "         [-0.0398, -0.0275, -0.0571,  ..., -0.1705, -0.0421,  0.1415],\n",
       "         [-0.0609, -0.7812, -1.0000,  ..., -0.1626,  0.0497,  0.0238],\n",
       "         [ 0.0217, -0.0949,  0.0983,  ...,  0.0842,  0.1940, -0.0874]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.0.feed_forward.w_1.bias': tensor([-0.3264, -0.1189, -0.1158,  ..., -0.1076, -0.3582,  0.1184],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.0.feed_forward.w_2.weight': tensor([[ 5.4474e-02, -3.5706e-02,  7.3608e-02,  ..., -6.0089e-02,\n",
       "          -4.7217e-01,  1.3208e-01],\n",
       "         [ 1.6308e-04,  2.6016e-02, -9.5901e-03,  ..., -2.8137e-02,\n",
       "          -1.6527e-03, -2.5024e-02],\n",
       "         [-5.9280e-03,  3.3844e-02, -2.1988e-02,  ...,  1.0384e-02,\n",
       "          -3.5498e-01,  1.5053e-02],\n",
       "         ...,\n",
       "         [-3.8116e-02, -2.1011e-02,  9.9854e-02,  ..., -1.0352e-01,\n",
       "           7.6103e-04, -2.3364e-01],\n",
       "         [ 2.1960e-01,  2.2803e-01, -4.5837e-02,  ..., -3.4943e-02,\n",
       "          -1.2585e-01,  8.1848e-02],\n",
       "         [-8.0322e-02, -2.6505e-02, -3.4448e-01,  ..., -8.9417e-02,\n",
       "          -2.1942e-02, -1.9287e-02]], dtype=torch.float16),\n",
       " 'encoder.transformer.0.feed_forward.w_2.bias': tensor([-0.0136, -0.3655,  0.3784,  ..., -0.4990, -0.1392, -0.7456],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.0.feed_forward.layer_norm.weight': tensor([0.3259, 1.1016, 1.6602,  ..., 0.3540, 0.1896, 0.2242],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.0.feed_forward.layer_norm.bias': tensor([ 0.0026, -0.0082, -0.0087,  ...,  0.1504,  0.0718,  0.1245],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.1.self_attn.linear_keys.weight': tensor([[ 0.3132,  0.0303,  0.0594,  ..., -0.1047,  0.2419, -0.0555],\n",
       "         [-0.0422,  0.0921,  0.0128,  ..., -0.0507,  0.2769,  0.0223],\n",
       "         [ 0.4136,  0.1260, -0.0191,  ...,  0.1329, -0.0108, -0.2230],\n",
       "         ...,\n",
       "         [-0.1825,  0.0161, -0.1770,  ..., -0.0454,  0.0595, -0.0764],\n",
       "         [-0.0319, -0.1340,  0.1174,  ...,  0.0326,  0.0575, -0.1683],\n",
       "         [-0.1371, -0.0249,  0.2296,  ..., -0.0196,  0.2529,  0.0146]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.1.self_attn.linear_keys.bias': tensor([ 0.0179,  0.0086, -0.0042,  ..., -0.0084, -0.0040, -0.0065],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.1.self_attn.linear_values.weight': tensor([[-0.0923, -0.0101, -0.0208,  ..., -0.2000,  0.0222,  0.2756],\n",
       "         [ 0.0516, -0.0055,  0.0196,  ...,  0.0775,  0.0753,  0.2461],\n",
       "         [-0.0670, -0.0357, -0.0576,  ...,  0.0538, -0.1516, -0.2198],\n",
       "         ...,\n",
       "         [-0.3870,  0.1213, -0.1592,  ...,  0.2500,  0.0792, -0.0464],\n",
       "         [ 0.0688,  0.0403,  0.0655,  ...,  0.2209,  0.2373,  0.0286],\n",
       "         [-0.0528, -0.0808,  0.0141,  ...,  0.0310,  0.0302,  0.0488]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.1.self_attn.linear_values.bias': tensor([ 0.0793, -0.0319,  0.0125,  ...,  0.0307, -0.0509, -0.0137],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.1.self_attn.linear_query.weight': tensor([[ 0.2812,  0.1462,  0.0362,  ..., -0.0667,  0.0091,  0.1052],\n",
       "         [-0.1653,  0.0444, -0.0217,  ..., -0.1318, -0.0861, -0.2842],\n",
       "         [ 0.2281, -0.0016,  0.1289,  ...,  0.1704, -0.0197,  0.5918],\n",
       "         ...,\n",
       "         [-0.2090,  0.0235, -0.0010,  ..., -0.0251, -0.0866,  0.5020],\n",
       "         [-0.0695,  0.1296, -0.1885,  ..., -0.3889, -0.0210,  0.4143],\n",
       "         [ 0.3230, -0.0293, -0.0364,  ..., -0.0184,  0.3271,  0.4307]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.1.self_attn.linear_query.bias': tensor([ 0.1562, -0.1987,  0.2474,  ..., -0.0505, -0.3267,  0.2563],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.1.self_attn.final_linear.weight': tensor([[-0.1606,  0.1558, -0.1584,  ..., -0.0085,  0.0523, -0.3730],\n",
       "         [-0.0123,  0.0967,  0.0272,  ...,  0.0414,  0.1367,  0.1249],\n",
       "         [ 0.0454, -0.0834, -0.0182,  ...,  0.0917, -0.2466,  0.0072],\n",
       "         ...,\n",
       "         [-0.1824, -0.0740,  0.1194,  ...,  0.0551, -0.1820, -0.1041],\n",
       "         [ 0.2206, -0.1924,  0.2886,  ..., -0.0296, -0.3135, -0.0388],\n",
       "         [ 0.0279,  0.1779, -0.1296,  ..., -0.4524,  0.0889, -0.1515]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.1.self_attn.final_linear.bias': tensor([-0.5601, -0.4797,  0.5430,  ..., -0.7354, -0.2346, -0.3242],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.1.layer_norm.weight': tensor([0.2800, 0.6963, 1.0000,  ..., 0.1310, 0.1289, 0.0534],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.1.layer_norm.bias': tensor([-0.0067, -0.0167,  0.0038,  ..., -0.0364,  0.0032,  0.0068],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.1.feed_forward.w_1.weight': tensor([[ 0.0288,  0.0228, -0.0608,  ..., -0.0198, -0.1741,  0.0707],\n",
       "         [ 0.1438,  0.0527,  0.0828,  ..., -0.2949, -0.1907, -0.1910],\n",
       "         [ 0.0360,  0.2744,  0.0928,  ..., -0.1534,  0.1022, -0.0169],\n",
       "         ...,\n",
       "         [ 0.1482,  0.1733, -0.1611,  ..., -0.0764,  0.1055, -0.1361],\n",
       "         [ 0.2598,  0.2203,  0.1808,  ..., -0.0300,  0.2542,  0.1102],\n",
       "         [ 0.2976,  0.0646,  0.2065,  ..., -0.0663, -0.1564, -0.0345]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.1.feed_forward.w_1.bias': tensor([-0.0112, -0.2332, -0.1604,  ..., -0.1494, -0.0759, -0.0137],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.1.feed_forward.w_2.weight': tensor([[ 0.0429,  0.0976,  0.2073,  ..., -0.2070,  0.0483, -0.0598],\n",
       "         [ 0.0475,  0.0685, -0.1361,  ..., -0.0488,  0.0395,  0.2761],\n",
       "         [-0.0568, -0.0254,  0.1608,  ...,  0.1666,  0.0410,  0.0114],\n",
       "         ...,\n",
       "         [ 0.1724,  0.2063,  0.0209,  ...,  0.3467,  0.2316,  0.0891],\n",
       "         [ 0.1193,  0.0363, -0.2162,  ..., -0.0956,  0.0388,  0.0544],\n",
       "         [-0.1222,  0.2642, -0.2076,  ..., -0.0162, -0.0597, -0.1100]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.1.feed_forward.w_2.bias': tensor([-0.4980, -0.6235,  0.3506,  ..., -0.8696,  0.0837,  0.0706],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.1.feed_forward.layer_norm.weight': tensor([0.3640, 0.5825, 0.7876,  ..., 0.3674, 0.2639, 0.7192],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.1.feed_forward.layer_norm.bias': tensor([-0.0160, -0.1505,  0.0006,  ...,  0.2961,  0.1514,  0.3875],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.2.self_attn.linear_keys.weight': tensor([[-0.3447, -0.4392, -0.2729,  ...,  0.1851,  0.1113, -0.1281],\n",
       "         [-0.5283, -0.3757, -0.2126,  ...,  0.0692, -0.0856,  0.1705],\n",
       "         [ 0.0643, -0.1009,  0.0264,  ..., -0.1879, -0.1941, -0.0309],\n",
       "         ...,\n",
       "         [ 0.1146, -0.1388,  0.0802,  ...,  0.0249, -0.2125, -0.0556],\n",
       "         [ 0.0025,  0.0030,  0.4084,  ...,  0.2128,  0.1897,  0.1888],\n",
       "         [-0.2020, -0.0406, -0.2024,  ...,  0.0584,  0.1917,  0.2167]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.2.self_attn.linear_keys.bias': tensor([-0.0259, -0.0300,  0.0098,  ...,  0.0023,  0.0161, -0.0312],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.2.self_attn.linear_values.weight': tensor([[ 0.0021,  0.0291,  0.2048,  ...,  0.3103, -0.0507, -0.1099],\n",
       "         [ 0.0806,  0.0523, -0.0200,  ...,  0.1108, -0.0761,  0.0262],\n",
       "         [-0.0192,  0.1956,  0.0440,  ..., -0.2407, -0.1000, -0.0016],\n",
       "         ...,\n",
       "         [ 0.0179,  0.0394, -0.0177,  ...,  0.1243,  0.0283,  0.0198],\n",
       "         [ 0.0754, -0.0627, -0.0371,  ...,  0.2119, -0.3503,  0.0305],\n",
       "         [ 0.1018,  0.2954,  0.0267,  ...,  0.0017,  0.0053, -0.0506]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.2.self_attn.linear_values.bias': tensor([ 0.1757, -0.1736,  0.0048,  ...,  0.0027, -0.0123,  0.0012],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.2.self_attn.linear_query.weight': tensor([[-0.2394, -0.2335, -0.0950,  ...,  0.1547,  0.1436,  0.1914],\n",
       "         [-0.0340, -0.1176, -0.3926,  ...,  0.1010, -0.0010, -0.0776],\n",
       "         [ 0.1042, -0.2820,  0.0054,  ..., -0.1914,  0.0817,  0.1759],\n",
       "         ...,\n",
       "         [-0.0563, -0.0400,  0.0502,  ..., -0.0404,  0.2847,  0.0385],\n",
       "         [ 0.1459, -0.0163,  0.0189,  ...,  0.0794,  0.2352,  0.1910],\n",
       "         [ 0.1403,  0.0652,  0.1050,  ..., -0.0582, -0.0510, -0.1772]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.2.self_attn.linear_query.bias': tensor([-0.1339,  0.7612,  0.0226,  ...,  0.0240,  0.1281,  0.1191],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.2.self_attn.final_linear.weight': tensor([[ 0.3418, -0.0677,  0.2137,  ...,  0.0499,  0.2649,  0.1776],\n",
       "         [-0.3020, -0.0951,  0.0147,  ...,  0.0165,  0.0594,  0.0526],\n",
       "         [-0.2974,  0.2010,  0.0136,  ...,  0.1761,  0.2007, -0.0710],\n",
       "         ...,\n",
       "         [ 0.0853,  0.2075,  0.0961,  ..., -0.2235, -0.2292,  0.0688],\n",
       "         [-0.0068, -0.0393, -0.1675,  ..., -0.1058, -0.2266, -0.1720],\n",
       "         [ 0.2888,  0.1907,  0.2939,  ...,  0.1801,  0.4055,  0.1396]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.2.self_attn.final_linear.bias': tensor([-0.3291,  0.1842, -0.0803,  ..., -0.4951, -0.2440, -0.1320],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.2.layer_norm.weight': tensor([0.2551, 0.3916, 0.4587,  ..., 0.1593, 0.1455, 0.0640],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.2.layer_norm.bias': tensor([ 0.0016, -0.0094,  0.0044,  ..., -0.0253,  0.0004, -0.1724],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.2.feed_forward.w_1.weight': tensor([[-0.0267, -0.2546, -0.1114,  ...,  0.0208, -0.4014, -0.0889],\n",
       "         [ 0.1504,  0.1133, -0.3638,  ..., -0.0380, -0.0839, -0.0189],\n",
       "         [-0.3149,  0.2098, -0.0369,  ..., -0.0750,  0.1387, -0.1759],\n",
       "         ...,\n",
       "         [ 0.1409,  0.0983,  0.0416,  ...,  0.1759, -0.0855,  0.1470],\n",
       "         [-0.1570,  0.0397, -0.2314,  ..., -0.1272,  0.0359,  0.0133],\n",
       "         [-0.1584, -0.0536, -0.2089,  ..., -0.1642, -0.5425, -0.1512]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.2.feed_forward.w_1.bias': tensor([-0.0205, -0.1761, -0.1024,  ..., -0.1119, -0.2397, -0.1015],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.2.feed_forward.w_2.weight': tensor([[ 0.2343,  0.1005, -0.0427,  ..., -0.0775, -0.1626, -0.0025],\n",
       "         [-0.2034, -0.2035, -0.0301,  ..., -0.2466,  0.1137,  0.2327],\n",
       "         [ 0.0221,  0.0497,  0.0061,  ...,  0.0156,  0.4207,  0.0813],\n",
       "         ...,\n",
       "         [ 0.0395,  0.2274,  0.0329,  ...,  0.0589,  0.1022, -0.2118],\n",
       "         [-0.0251, -0.1895,  0.0290,  ...,  0.5098,  0.1008, -0.3828],\n",
       "         [ 0.3801,  0.1416, -0.0853,  ...,  0.0865,  0.0880, -0.0945]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.2.feed_forward.w_2.bias': tensor([-0.2042, -0.3538,  0.0340,  ..., -0.5020, -0.2240, -0.4961],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.2.feed_forward.layer_norm.weight': tensor([0.4373, 0.5439, 0.6646,  ..., 0.4412, 0.3557, 0.5312],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.2.feed_forward.layer_norm.bias': tensor([ 0.0157, -0.1758,  0.0136,  ...,  0.3103,  0.2247,  0.3567],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.3.self_attn.linear_keys.weight': tensor([[ 0.1273,  0.2241, -0.0561,  ..., -0.1205,  0.2671,  0.1118],\n",
       "         [-0.1213,  0.0095, -0.0512,  ..., -0.0240,  0.2441,  0.0936],\n",
       "         [-0.1697, -0.1659,  0.1774,  ...,  0.1174, -0.1907,  0.1812],\n",
       "         ...,\n",
       "         [-0.2524, -0.0491, -0.2900,  ..., -0.0951,  0.3584,  0.0298],\n",
       "         [ 0.3196, -0.3335,  0.0198,  ..., -0.2542,  0.2593, -0.2198],\n",
       "         [-0.2167, -0.0964, -0.0109,  ...,  0.1262,  0.2217,  0.2588]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.3.self_attn.linear_keys.bias': tensor([ 0.0045, -0.0249,  0.0055,  ...,  0.0129,  0.0192, -0.0107],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.3.self_attn.linear_values.weight': tensor([[-0.1000, -0.0194, -0.0435,  ..., -0.2358,  0.0946,  0.0695],\n",
       "         [ 0.0120,  0.0917, -0.1650,  ...,  0.1097, -0.0242, -0.0247],\n",
       "         [ 0.0066, -0.0302,  0.0870,  ..., -0.0209,  0.4578, -0.0189],\n",
       "         ...,\n",
       "         [ 0.3220, -0.0787, -0.2520,  ...,  0.1698,  0.2107,  0.0212],\n",
       "         [ 0.0778,  0.0969,  0.0474,  ...,  0.1864, -0.0052, -0.0772],\n",
       "         [-0.0798,  0.2871,  0.1097,  ..., -0.0649,  0.0545,  0.0087]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.3.self_attn.linear_values.bias': tensor([-0.0052,  0.1276, -0.0572,  ..., -0.1461, -0.2104, -0.0482],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.3.self_attn.linear_query.weight': tensor([[ 0.1786, -0.3074, -0.1509,  ...,  0.1709, -0.3638, -0.1603],\n",
       "         [-0.1456, -0.0075, -0.0852,  ..., -0.1757,  0.4004,  0.3643],\n",
       "         [ 0.1526,  0.0436, -0.1224,  ...,  0.0580,  0.4180, -0.2462],\n",
       "         ...,\n",
       "         [-0.2957, -0.0009, -0.4045,  ...,  0.2405, -0.2418,  0.0562],\n",
       "         [-0.1401, -0.1481, -0.1549,  ..., -0.3357,  0.0062, -0.2097],\n",
       "         [ 0.1744, -0.2991, -0.4023,  ...,  0.0531, -0.3079,  0.1447]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.3.self_attn.linear_query.bias': tensor([ 0.3250,  0.2773,  0.2637,  ...,  0.4526, -0.1826,  0.5928],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.3.self_attn.final_linear.weight': tensor([[ 0.2375,  0.4453, -0.1666,  ..., -0.0097, -0.2749,  0.2230],\n",
       "         [-0.1320,  0.1287, -0.1155,  ..., -0.0981, -0.1682,  0.0881],\n",
       "         [ 0.2158, -0.1592,  0.2389,  ..., -0.2021,  0.0601,  0.0741],\n",
       "         ...,\n",
       "         [-0.2094,  0.2705,  0.3247,  ..., -0.3774, -0.0038,  0.0504],\n",
       "         [ 0.1333, -0.0093,  0.0922,  ..., -0.2559, -0.2120, -0.0821],\n",
       "         [ 0.1238, -0.1958, -0.2073,  ...,  0.0613,  0.0382, -0.0040]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.3.self_attn.final_linear.bias': tensor([-0.1768,  0.0324, -0.4902,  ..., -0.1864, -0.1381, -0.0352],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.3.layer_norm.weight': tensor([0.2632, 0.3484, 0.3823,  ..., 0.1754, 0.1694, 0.0759],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.3.layer_norm.bias': tensor([-0.0017, -0.0178,  0.0143,  ..., -0.0205, -0.0016, -0.1876],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.3.feed_forward.w_1.weight': tensor([[ 0.2700,  0.1938,  0.2998,  ...,  0.1562,  0.1998,  0.3589],\n",
       "         [ 0.1078,  0.3862,  0.1381,  ..., -0.1604, -0.1088,  0.1871],\n",
       "         [-0.0421,  0.3840, -0.1287,  ...,  0.0122,  0.1149, -0.0363],\n",
       "         ...,\n",
       "         [ 0.0747,  0.2203, -0.2174,  ..., -0.3176, -0.0036, -0.1372],\n",
       "         [ 0.1874,  0.1069, -0.1216,  ..., -0.2939,  0.4365,  0.1703],\n",
       "         [ 0.1702,  0.2842,  0.0290,  ...,  0.0687, -0.0826,  0.0793]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.3.feed_forward.w_1.bias': tensor([-0.0987, -0.1757, -0.0022,  ..., -0.2773, -0.2390, -0.1774],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.3.feed_forward.w_2.weight': tensor([[-0.0603, -0.1033,  0.0816,  ..., -0.3188, -0.2788, -0.1556],\n",
       "         [ 0.0388,  0.0523, -0.2107,  ..., -0.2145,  0.0859,  0.1127],\n",
       "         [-0.0334, -0.1051, -0.0261,  ...,  0.1476, -0.2554, -0.1295],\n",
       "         ...,\n",
       "         [-0.0571, -0.1379, -0.1825,  ..., -0.0071, -0.1080, -0.2607],\n",
       "         [-0.2651, -0.1130,  0.1184,  ..., -0.3130,  0.2365, -0.1627],\n",
       "         [-0.2324, -0.0607,  0.2191,  ...,  0.4524,  0.0908,  0.0570]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.3.feed_forward.w_2.bias': tensor([-0.1322, -0.2524, -0.2625,  ..., -0.6270, -0.4980,  0.1598],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.3.feed_forward.layer_norm.weight': tensor([0.5571, 0.6401, 0.6841,  ..., 0.5356, 0.4375, 0.5571],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.3.feed_forward.layer_norm.bias': tensor([ 0.0726, -0.2047,  0.0892,  ...,  0.3430,  0.1608,  0.3596],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.4.self_attn.linear_keys.weight': tensor([[-0.1092, -0.0028, -0.1350,  ..., -0.1427,  0.2861, -0.0529],\n",
       "         [ 0.1046,  0.1780, -0.0298,  ..., -0.0937,  0.0679, -0.0216],\n",
       "         [-0.1721, -0.1276,  0.2137,  ...,  0.0411, -0.0165, -0.1064],\n",
       "         ...,\n",
       "         [ 0.1192,  0.1671,  0.2939,  ...,  0.1614, -0.0975,  0.0159],\n",
       "         [-0.1638, -0.1135, -0.0382,  ...,  0.0856, -0.1583, -0.0257],\n",
       "         [-0.1689,  0.3257, -0.1021,  ...,  0.0256,  0.0517,  0.1899]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.4.self_attn.linear_keys.bias': tensor([ 0.0186, -0.0253, -0.0241,  ...,  0.0240,  0.0197,  0.0040],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.4.self_attn.linear_values.weight': tensor([[-0.0698, -0.0300,  0.1226,  ..., -0.2991,  0.0958,  0.0717],\n",
       "         [ 0.1055,  0.0383, -0.1597,  ..., -0.2573,  0.1906,  0.0244],\n",
       "         [ 0.0713, -0.3877, -0.1547,  ...,  0.0464, -0.0274,  0.0415],\n",
       "         ...,\n",
       "         [ 0.0169,  0.1136, -0.2030,  ..., -0.1438,  0.3379,  0.1071],\n",
       "         [-0.2815,  0.1594,  0.4478,  ...,  0.1702, -0.4883,  0.0009],\n",
       "         [-0.2661, -0.0815,  0.4202,  ..., -0.1316, -0.3354,  0.0138]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.4.self_attn.linear_values.bias': tensor([-0.0147, -0.1171,  0.1478,  ..., -0.0172, -0.0030, -0.1357],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.4.self_attn.linear_query.weight': tensor([[-0.3167,  0.2019,  0.2131,  ...,  0.0332, -0.3059, -0.3101],\n",
       "         [ 0.0149, -0.0750,  0.2404,  ..., -0.1279, -0.1794,  0.1234],\n",
       "         [-0.0178, -0.0139, -0.2239,  ..., -0.1698, -0.0556, -0.1077],\n",
       "         ...,\n",
       "         [-0.1376,  0.0729, -0.2188,  ...,  0.0415, -0.1272,  0.3601],\n",
       "         [ 0.0909,  0.0670, -0.0636,  ..., -0.1163, -0.2096, -0.0964],\n",
       "         [ 0.2123, -0.3875, -0.1823,  ..., -0.0737, -0.0089, -0.1017]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.4.self_attn.linear_query.bias': tensor([-0.1918,  0.3306,  0.0953,  ..., -0.0160, -0.1255,  0.0032],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.4.self_attn.final_linear.weight': tensor([[ 0.3052,  0.0142, -0.2898,  ...,  0.0723,  0.0241,  0.1895],\n",
       "         [-0.2295, -0.2798, -0.0404,  ...,  0.1388,  0.2332,  0.2834],\n",
       "         [ 0.1124,  0.0247,  0.2905,  ...,  0.0372, -0.3989, -0.0923],\n",
       "         ...,\n",
       "         [ 0.0914,  0.4182,  0.0561,  ..., -0.3076, -0.0513,  0.0178],\n",
       "         [-0.0081, -0.1631,  0.3223,  ...,  0.0817, -0.0063,  0.0272],\n",
       "         [-0.0464,  0.0386, -0.3821,  ..., -0.2686,  0.2832, -0.3052]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.4.self_attn.final_linear.bias': tensor([-0.2148, -0.2712, -0.4692,  ..., -0.4463, -0.2773, -0.1874],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.4.layer_norm.weight': tensor([0.2480, 0.2932, 0.3071,  ..., 0.1970, 0.1952, 0.0856],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.4.layer_norm.bias': tensor([ 0.0061, -0.0186,  0.0067,  ..., -0.0136,  0.0044, -0.1420],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.4.feed_forward.w_1.weight': tensor([[-0.0947, -0.3777,  0.1453,  ...,  0.2079,  0.1239, -0.1075],\n",
       "         [-0.0224, -0.1260,  0.0084,  ..., -0.1088, -0.1128,  0.4102],\n",
       "         [-0.1868,  0.2651, -0.2267,  ..., -0.2732,  0.2051,  0.1484],\n",
       "         ...,\n",
       "         [-0.1461, -0.2017,  0.1912,  ..., -0.0526, -0.1860,  0.0266],\n",
       "         [ 0.3555,  0.1292, -0.0576,  ..., -0.0812, -0.0305, -0.1492],\n",
       "         [ 0.0394, -0.0345,  0.0431,  ...,  0.0626,  0.0320,  0.5259]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.4.feed_forward.w_1.bias': tensor([-0.1158, -0.1146, -0.0953,  ..., -0.0958, -0.1493,  0.1250],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.4.feed_forward.w_2.weight': tensor([[ 9.1736e-02, -3.4851e-02,  6.6711e-02,  ...,  1.7236e-01,\n",
       "          -2.4426e-01, -1.8814e-02],\n",
       "         [ 2.3706e-01,  1.6125e-01, -1.7261e-01,  ...,  1.8347e-01,\n",
       "           3.2921e-03,  2.4902e-02],\n",
       "         [ 1.3171e-01,  1.8689e-01, -5.2910e-03,  ..., -8.6365e-02,\n",
       "          -4.3640e-02,  3.5248e-03],\n",
       "         ...,\n",
       "         [-3.4094e-04, -2.9468e-01,  1.6211e-01,  ...,  2.6520e-02,\n",
       "          -4.6265e-02,  5.0537e-02],\n",
       "         [-2.1143e-01, -1.3879e-01,  4.9731e-01,  ...,  7.4158e-03,\n",
       "           2.5854e-01,  2.4811e-02],\n",
       "         [-8.2092e-02,  2.2675e-02, -2.8735e-01,  ...,  6.6528e-02,\n",
       "           2.5195e-01,  3.7183e-01]], dtype=torch.float16),\n",
       " 'encoder.transformer.4.feed_forward.w_2.bias': tensor([-0.0294, -0.0543, -0.1670,  ..., -0.4087, -0.3076, -0.2484],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.4.feed_forward.layer_norm.weight': tensor([0.8389, 0.8896, 0.9126,  ..., 0.8003, 0.6929, 0.9141],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.4.feed_forward.layer_norm.bias': tensor([ 0.0629, -0.1814,  0.1497,  ...,  0.3884,  0.2186,  0.6089],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.5.self_attn.linear_keys.weight': tensor([[-0.1027,  0.1290,  0.2034,  ...,  0.0239,  0.0739,  0.1147],\n",
       "         [ 0.0134,  0.0109,  0.0040,  ..., -0.1880, -0.0925,  0.1240],\n",
       "         [ 0.1074, -0.2524,  0.1588,  ..., -0.0675, -0.1399, -0.1630],\n",
       "         ...,\n",
       "         [-0.1376,  0.0674, -0.1263,  ..., -0.0240, -0.1779,  0.2888],\n",
       "         [-0.0420, -0.0428, -0.0071,  ...,  0.1560,  0.2296, -0.0860],\n",
       "         [ 0.2198,  0.2279, -0.0904,  ..., -0.0740,  0.1239, -0.1846]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.5.self_attn.linear_keys.bias': tensor([-0.0248, -0.0112,  0.0246,  ...,  0.0078,  0.0279, -0.0154],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.5.self_attn.linear_values.weight': tensor([[-6.2683e-02,  4.0723e-01,  3.0078e-01,  ...,  2.5342e-01,\n",
       "           3.8208e-01, -1.4209e-01],\n",
       "         [ 2.1057e-01, -4.9731e-01, -5.0195e-01,  ...,  4.3604e-01,\n",
       "          -2.5562e-01,  7.9468e-02],\n",
       "         [ 2.2192e-01, -4.8315e-01, -4.9951e-01,  ...,  2.4561e-01,\n",
       "          -1.2524e-01, -8.5876e-02],\n",
       "         ...,\n",
       "         [ 2.9297e-03,  1.6528e-01,  1.7249e-01,  ...,  2.3157e-01,\n",
       "          -8.5205e-02, -1.6647e-02],\n",
       "         [ 1.7163e-01, -5.5176e-01,  1.1542e-01,  ..., -2.9545e-03,\n",
       "           3.3618e-01,  3.1757e-04],\n",
       "         [-1.7358e-01,  1.0901e-01,  3.8971e-02,  ..., -3.2129e-01,\n",
       "          -6.4148e-02,  9.6985e-02]], dtype=torch.float16),\n",
       " 'encoder.transformer.5.self_attn.linear_values.bias': tensor([ 0.2517,  0.0800, -0.1105,  ...,  0.1067,  0.0918, -0.2163],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.5.self_attn.linear_query.weight': tensor([[-0.0835, -0.2556,  0.1780,  ..., -0.0013,  0.0685,  0.1277],\n",
       "         [-0.0474,  0.0945,  0.0859,  ..., -0.0839, -0.2522,  0.3069],\n",
       "         [-0.1220,  0.1346,  0.0013,  ..., -0.1627,  0.1812, -0.3811],\n",
       "         ...,\n",
       "         [-0.5020, -0.2435, -0.0367,  ..., -0.1248, -0.0159,  0.0864],\n",
       "         [-0.0185, -0.1132, -0.1614,  ...,  0.0128, -0.2505,  0.0236],\n",
       "         [ 0.0471, -0.0351,  0.2712,  ..., -0.0532, -0.0487,  0.1573]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.5.self_attn.linear_query.bias': tensor([-0.1298,  0.3081, -0.0649,  ...,  0.3333, -0.2089, -0.3911],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.5.self_attn.final_linear.weight': tensor([[-0.1759, -0.2527, -0.4905,  ..., -0.1455,  0.0436,  0.0316],\n",
       "         [-0.0808,  0.3103, -0.0531,  ...,  0.2908, -0.4807, -0.0984],\n",
       "         [-0.2214, -0.2119,  0.0950,  ...,  0.3025,  0.3235,  0.0836],\n",
       "         ...,\n",
       "         [-0.1042, -0.2352, -0.2393,  ..., -0.0567, -0.1390, -0.5044],\n",
       "         [ 0.1320, -0.2820,  0.1831,  ..., -0.0705,  0.1049,  0.4670],\n",
       "         [ 0.1141, -0.1040,  0.2449,  ...,  0.2822, -0.1600,  0.0797]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.5.self_attn.final_linear.bias': tensor([-0.2974,  0.1377, -0.3240,  ..., -0.3740, -0.3955, -0.0625],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.5.layer_norm.weight': tensor([0.2593, 0.2888, 0.2932,  ..., 0.2134, 0.1947, 0.1024],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.5.layer_norm.bias': tensor([ 0.0130, -0.0358,  0.0096,  ..., -0.0243, -0.0059, -0.1379],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.5.feed_forward.w_1.weight': tensor([[-0.1627,  0.4009,  0.3484,  ..., -0.4468,  0.2238,  0.0247],\n",
       "         [-0.2316, -0.2040,  0.2120,  ...,  0.0018,  0.2175, -0.0315],\n",
       "         [ 0.0982,  0.2788, -0.5264,  ...,  0.2260, -0.1183, -0.1161],\n",
       "         ...,\n",
       "         [-0.1323, -0.2255,  0.0363,  ..., -0.2161, -0.0649, -0.0839],\n",
       "         [ 0.0827,  0.3855,  0.0304,  ..., -0.1301,  0.0207,  0.0511],\n",
       "         [ 0.1125,  0.0268,  0.0286,  ..., -0.0140, -0.2603,  0.0757]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.5.feed_forward.w_1.bias': tensor([-0.3113, -0.2347, -0.2471,  ...,  0.0296, -0.0106, -0.1477],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.5.feed_forward.w_2.weight': tensor([[ 0.1118, -0.0317,  0.3906,  ...,  0.1901,  0.0196,  0.1421],\n",
       "         [ 0.2644, -0.2418, -0.0213,  ...,  0.2583, -0.1804,  0.0911],\n",
       "         [ 0.1860,  0.1185, -0.1493,  ..., -0.0756, -0.0252, -0.0974],\n",
       "         ...,\n",
       "         [-0.0878,  0.0639,  0.0248,  ...,  0.1627, -0.0352,  0.0266],\n",
       "         [-0.2057,  0.3218,  0.3306,  ...,  0.0302, -0.1030, -0.1403],\n",
       "         [-0.3589,  0.0693,  0.3821,  ...,  0.0552,  0.0178,  0.1909]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.5.feed_forward.w_2.bias': tensor([-0.1635,  0.1593,  0.2888,  ..., -0.3391, -0.1981,  0.2345],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.5.feed_forward.layer_norm.weight': tensor([1.0498, 1.0732, 1.0986,  ..., 1.0576, 0.9355, 1.1182],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.5.feed_forward.layer_norm.bias': tensor([ 0.0972, -0.2222,  0.1204,  ...,  0.2439, -0.0014,  0.7427],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.6.self_attn.linear_keys.weight': tensor([[-0.0706, -0.0345,  0.2610,  ..., -0.1284,  0.0302, -0.1357],\n",
       "         [ 0.1028,  0.0043, -0.1156,  ..., -0.1282, -0.1825, -0.1663],\n",
       "         [-0.2213, -0.1600,  0.1575,  ...,  0.2169, -0.0125,  0.1002],\n",
       "         ...,\n",
       "         [-0.0800, -0.0073, -0.0764,  ..., -0.1770, -0.0122, -0.1235],\n",
       "         [ 0.1169,  0.0127, -0.1982,  ..., -0.0235,  0.0922,  0.0699],\n",
       "         [-0.4729, -0.2825,  0.0516,  ...,  0.0944,  0.0304,  0.3552]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.6.self_attn.linear_keys.bias': tensor([-0.0212,  0.0140,  0.0011,  ...,  0.0077,  0.0287, -0.0167],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.6.self_attn.linear_values.weight': tensor([[-0.1652,  0.3330,  0.3162,  ..., -0.2389,  0.2849, -0.0800],\n",
       "         [-0.1203,  0.2966, -0.1333,  ..., -0.2632,  0.3467,  0.0620],\n",
       "         [ 0.1744, -0.0900,  0.2561,  ..., -0.0182,  0.3352, -0.1104],\n",
       "         ...,\n",
       "         [-0.5029, -0.0768,  0.3154,  ..., -0.1250, -0.3794,  0.0287],\n",
       "         [ 0.2097,  0.2666, -0.5264,  ..., -0.3022,  0.2620, -0.2422],\n",
       "         [-0.3618,  0.1602,  0.1276,  ...,  0.1013, -0.4070, -0.0555]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.6.self_attn.linear_values.bias': tensor([-0.1273,  0.0925, -0.0681,  ...,  0.1183,  0.1335,  0.1152],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.6.self_attn.linear_query.weight': tensor([[ 5.8380e-02,  1.7212e-01,  1.0687e-01,  ...,  7.8491e-02,\n",
       "           2.8854e-02,  3.4326e-01],\n",
       "         [ 3.1348e-01,  1.0724e-01, -2.2888e-03,  ..., -3.3398e-03,\n",
       "          -2.1375e-01,  2.3218e-01],\n",
       "         [-1.6968e-01, -1.7670e-02,  1.2512e-01,  ...,  4.8615e-02,\n",
       "          -2.1106e-01, -2.4805e-01],\n",
       "         ...,\n",
       "         [-4.3365e-02,  4.7638e-02,  7.3303e-02,  ..., -3.2837e-02,\n",
       "           5.4840e-02, -1.5967e-01],\n",
       "         [ 7.6599e-02,  1.3269e-01, -4.7241e-02,  ...,  1.3574e-01,\n",
       "          -1.0687e-01, -2.5854e-01],\n",
       "         [ 1.8738e-02,  1.0598e-04,  3.7262e-02,  ...,  1.2688e-02,\n",
       "           3.1647e-02, -2.9468e-01]], dtype=torch.float16),\n",
       " 'encoder.transformer.6.self_attn.linear_query.bias': tensor([-0.4375,  0.0213,  0.2203,  ..., -0.4126, -0.1324,  0.9414],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.6.self_attn.final_linear.weight': tensor([[ 0.4932, -0.2959, -0.2666,  ..., -0.0205,  0.1765, -0.2189],\n",
       "         [ 0.2407,  0.2262,  0.1125,  ..., -0.0656,  0.1926,  0.0654],\n",
       "         [ 0.2922,  0.1119,  0.3481,  ...,  0.2129,  0.0788,  0.0352],\n",
       "         ...,\n",
       "         [-0.1580,  0.1183,  0.0581,  ..., -0.2288, -0.0887, -0.0232],\n",
       "         [-0.0381,  0.1404, -0.0415,  ..., -0.2959,  0.0758, -0.3286],\n",
       "         [-0.5059,  0.4998, -0.0927,  ...,  0.3774, -0.2629, -0.5054]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.6.self_attn.final_linear.bias': tensor([-0.0626,  0.2250, -0.1521,  ..., -0.4531, -0.2175,  0.0847],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.6.layer_norm.weight': tensor([0.2781, 0.3113, 0.3044,  ..., 0.2424, 0.2245, 0.1198],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.6.layer_norm.bias': tensor([ 0.0091, -0.0322,  0.0044,  ..., -0.0353, -0.0171, -0.1396],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.6.feed_forward.w_1.weight': tensor([[ 0.5049, -0.2141,  0.1185,  ...,  0.0346, -0.0762, -0.2939],\n",
       "         [-0.1039,  0.0520, -0.2483,  ..., -0.2549,  0.0668,  0.0578],\n",
       "         [-0.1609, -0.0087,  0.0487,  ...,  0.0201, -0.0370, -0.1423],\n",
       "         ...,\n",
       "         [-0.2238,  0.2118,  0.5020,  ..., -0.2603, -0.5088, -0.1318],\n",
       "         [ 0.0547, -0.1289, -0.1384,  ..., -0.3176, -0.0857,  0.4788],\n",
       "         [-0.1636, -0.1747,  0.0696,  ...,  0.1959, -0.3271, -0.2164]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.6.feed_forward.w_1.bias': tensor([-0.2440, -0.1483, -0.1898,  ..., -0.2588, -0.2106, -0.1549],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.6.feed_forward.w_2.weight': tensor([[-0.1581, -0.0368,  0.2334,  ..., -0.3040,  0.1361,  0.1009],\n",
       "         [ 0.2061,  0.0373, -0.1328,  ...,  0.4993, -0.0421, -0.2350],\n",
       "         [ 0.0025,  0.0223, -0.1836,  ...,  0.1327, -0.1272,  0.0468],\n",
       "         ...,\n",
       "         [-0.1857,  0.1321, -0.2494,  ..., -0.3591, -0.0569,  0.1892],\n",
       "         [-0.3350, -0.0670,  0.2271,  ...,  0.2461, -0.0009, -0.2045],\n",
       "         [ 0.0830,  0.0142,  0.1000,  ..., -0.2190,  0.0870,  0.2712]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.6.feed_forward.w_2.bias': tensor([-0.2479, -0.1766,  0.3108,  ..., -0.2715,  0.1544, -0.0607],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.6.feed_forward.layer_norm.weight': tensor([1.2949, 1.2705, 1.2734,  ..., 1.2637, 1.1494, 1.0869],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.6.feed_forward.layer_norm.bias': tensor([ 0.1816, -0.1693,  0.0043,  ...,  0.1831, -0.1348,  0.7734],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.7.self_attn.linear_keys.weight': tensor([[ 0.0403,  0.2040, -0.1902,  ...,  0.1509, -0.1453, -0.1630],\n",
       "         [-0.0783, -0.0748, -0.1115,  ...,  0.1044, -0.0614, -0.2024],\n",
       "         [-0.0826, -0.0220,  0.1033,  ...,  0.1572, -0.1219,  0.0381],\n",
       "         ...,\n",
       "         [-0.1497, -0.0581, -0.2620,  ..., -0.0068, -0.0917,  0.1129],\n",
       "         [-0.3176,  0.1254,  0.0963,  ...,  0.0521, -0.0949, -0.0911],\n",
       "         [ 0.0673, -0.1017, -0.2588,  ...,  0.0936,  0.1602,  0.0665]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.7.self_attn.linear_keys.bias': tensor([ 0.0198,  0.0307,  0.0190,  ...,  0.0079, -0.0173, -0.0075],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.7.self_attn.linear_values.weight': tensor([[ 0.6250,  0.3130, -0.0906,  ...,  0.1709,  0.1100,  0.0266],\n",
       "         [-0.3528,  0.0912, -0.4282,  ...,  0.2196, -0.1589, -0.0099],\n",
       "         [-0.0517, -0.0556, -0.1302,  ...,  0.4043, -0.1920,  0.2158],\n",
       "         ...,\n",
       "         [-0.0071, -0.0645, -0.3860,  ..., -0.0900,  0.0782, -0.0291],\n",
       "         [ 0.0308,  0.2062, -0.3958,  ...,  0.0598,  0.4902, -0.1539],\n",
       "         [ 0.4363, -0.0987, -0.4602,  ...,  0.1437,  0.0404,  0.0533]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.7.self_attn.linear_values.bias': tensor([ 0.0676,  0.4995, -0.2866,  ..., -0.1379, -0.1763,  0.2477],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.7.self_attn.linear_query.weight': tensor([[ 0.0831,  0.0398,  0.1328,  ...,  0.0081, -0.0090,  0.0434],\n",
       "         [-0.1038,  0.0436, -0.0704,  ..., -0.0676,  0.1190, -0.1210],\n",
       "         [ 0.0927,  0.0960,  0.0286,  ...,  0.0106,  0.1908,  0.1411],\n",
       "         ...,\n",
       "         [ 0.0671,  0.2081,  0.0379,  ...,  0.1769,  0.0137,  0.0703],\n",
       "         [-0.0284,  0.0137,  0.2231,  ..., -0.0323,  0.2351,  0.2407],\n",
       "         [ 0.0638,  0.0581, -0.0652,  ...,  0.0352,  0.0883,  0.0115]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.7.self_attn.linear_query.bias': tensor([-0.0459,  0.5107,  0.0079,  ...,  0.2482,  0.0854,  0.0585],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.7.self_attn.final_linear.weight': tensor([[-0.3962,  0.1968,  0.2440,  ..., -0.2708,  0.0613,  0.5044],\n",
       "         [-0.1608, -0.2025,  0.0812,  ...,  0.4180,  0.1942,  0.0641],\n",
       "         [ 0.1642, -0.0959,  0.0797,  ...,  0.0237, -0.0937,  0.5010],\n",
       "         ...,\n",
       "         [-0.1365, -0.1866,  0.0842,  ..., -0.2437, -0.0931,  0.2573],\n",
       "         [-0.1290,  0.1565,  0.2512,  ..., -0.3945,  0.0866, -0.0850],\n",
       "         [-0.3518,  0.4900,  0.2866,  ...,  0.2408,  0.2727, -0.0621]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.7.self_attn.final_linear.bias': tensor([-0.1945, -0.0603, -0.1410,  ..., -0.2971, -0.3428, -0.5000],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.7.layer_norm.weight': tensor([0.2864, 0.2981, 0.3025,  ..., 0.2600, 0.2637, 0.1532],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.7.layer_norm.bias': tensor([ 0.0018, -0.0353,  0.0016,  ..., -0.0442, -0.0206, -0.0556],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.7.feed_forward.w_1.weight': tensor([[ 0.3796, -0.2915,  0.0097,  ..., -0.1077,  0.2246,  0.0941],\n",
       "         [ 0.2150,  0.0715, -0.2764,  ..., -0.3174, -0.0606, -0.1829],\n",
       "         [-0.1274,  0.3252,  0.2388,  ..., -0.0192, -0.5010,  0.2556],\n",
       "         ...,\n",
       "         [-0.1538,  0.1974,  0.0007,  ...,  0.4119,  0.2036, -0.1060],\n",
       "         [ 0.1875, -0.0032, -0.2629,  ..., -0.0845,  0.0578,  0.1394],\n",
       "         [ 0.0598, -0.2800,  0.1261,  ..., -0.0494,  0.1498, -0.3682]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.7.feed_forward.w_1.bias': tensor([-0.1786,  0.0657, -0.2452,  ..., -0.1400, -0.1942, -0.2534],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.7.feed_forward.w_2.weight': tensor([[-0.0225,  0.0469,  0.5161,  ..., -0.0067, -0.4446,  0.1309],\n",
       "         [-0.1541,  0.0912, -0.1689,  ..., -0.0529, -0.1357, -0.1484],\n",
       "         [-0.1372,  0.0805, -0.4978,  ..., -0.1472,  0.0931,  0.0875],\n",
       "         ...,\n",
       "         [ 0.3457,  0.2556,  0.1699,  ...,  0.0536,  0.0854,  0.0499],\n",
       "         [ 0.2449, -0.0298,  0.0054,  ...,  0.1077, -0.1932, -0.1685],\n",
       "         [-0.1469, -0.0487, -0.0607,  ...,  0.0502, -0.5054,  0.0828]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.7.feed_forward.w_2.bias': tensor([-0.1750, -0.2800,  0.2668,  ..., -0.0279,  0.1831,  0.1775],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.7.feed_forward.layer_norm.weight': tensor([1.4658, 1.5234, 1.4697,  ..., 1.4619, 1.3975, 1.0264],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.7.feed_forward.layer_norm.bias': tensor([ 0.2140, -0.1531, -0.1310,  ...,  0.1624, -0.3196,  0.5806],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.8.self_attn.linear_keys.weight': tensor([[-0.0688, -0.0091, -0.1277,  ..., -0.0792, -0.1315,  0.2549],\n",
       "         [ 0.0327, -0.0327,  0.1985,  ..., -0.1526, -0.1306, -0.0013],\n",
       "         [ 0.0291, -0.2593,  0.0817,  ..., -0.0741,  0.0684, -0.0975],\n",
       "         ...,\n",
       "         [ 0.1205,  0.0697, -0.0167,  ...,  0.0344, -0.0850,  0.0958],\n",
       "         [-0.0356, -0.0033,  0.0279,  ...,  0.2449,  0.3352,  0.3486],\n",
       "         [-0.0198, -0.0017, -0.0009,  ..., -0.1373,  0.0296,  0.0921]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.8.self_attn.linear_keys.bias': tensor([ 0.0190,  0.0186,  0.0075,  ..., -0.0027,  0.0239, -0.0230],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.8.self_attn.linear_values.weight': tensor([[ 2.1698e-02,  5.0391e-01, -6.3599e-02,  ...,  3.4375e-01,\n",
       "          -5.6732e-02, -2.5439e-01],\n",
       "         [-5.0098e-01,  4.8828e-01, -3.9917e-01,  ..., -6.4163e-03,\n",
       "           2.7271e-01,  5.1239e-02],\n",
       "         [ 5.0342e-01, -2.5781e-01, -9.2224e-02,  ..., -2.5854e-01,\n",
       "           3.8013e-01,  2.5391e-01],\n",
       "         ...,\n",
       "         [-2.3806e-04, -2.4878e-01, -3.6865e-01,  ...,  1.6638e-01,\n",
       "           4.3030e-02,  1.0706e-01],\n",
       "         [-7.1472e-02,  5.3406e-02,  3.3667e-01,  ..., -4.8535e-01,\n",
       "          -4.1321e-02, -2.5244e-01],\n",
       "         [ 5.5518e-01, -6.1310e-02,  4.2041e-01,  ...,  2.2180e-01,\n",
       "           1.3086e-01, -1.9629e-01]], dtype=torch.float16),\n",
       " 'encoder.transformer.8.self_attn.linear_values.bias': tensor([ 0.0462, -0.1036, -0.0646,  ..., -0.1625,  0.0988,  0.0123],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.8.self_attn.linear_query.weight': tensor([[ 0.0249, -0.0852, -0.0412,  ...,  0.1812,  0.1111,  0.2671],\n",
       "         [-0.0025,  0.0505, -0.0931,  ...,  0.1137, -0.0060, -0.0775],\n",
       "         [ 0.2181,  0.0528,  0.0793,  ...,  0.2654,  0.0399, -0.0110],\n",
       "         ...,\n",
       "         [-0.0602,  0.0107, -0.1671,  ...,  0.1301,  0.2366,  0.0963],\n",
       "         [-0.0008,  0.1672,  0.2468,  ..., -0.1220, -0.1705, -0.0371],\n",
       "         [-0.1020,  0.0135,  0.0702,  ..., -0.0245,  0.1068, -0.0165]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.8.self_attn.linear_query.bias': tensor([ 0.4907, -0.1603, -0.2605,  ...,  0.0511,  0.5312,  0.1677],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.8.self_attn.final_linear.weight': tensor([[ 0.5000, -0.0681,  0.1069,  ..., -0.0848, -0.1550,  0.4285],\n",
       "         [-0.2615,  0.5898,  0.2981,  ..., -0.0019, -0.3152, -0.5073],\n",
       "         [-0.1832,  0.0960,  0.3252,  ..., -0.1443, -0.3098,  0.2847],\n",
       "         ...,\n",
       "         [-0.0236,  0.0312,  0.1584,  ..., -0.2113, -0.5024,  0.0351],\n",
       "         [ 0.4995,  0.0998,  0.1969,  ...,  0.3953,  0.0782, -0.1827],\n",
       "         [ 0.4517, -0.3076, -0.0215,  ...,  0.5000, -0.2598,  0.3438]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.8.self_attn.final_linear.bias': tensor([ 0.1201,  0.2529, -0.2769,  ..., -0.3010,  0.0786, -0.0475],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.8.layer_norm.weight': tensor([0.3359, 0.3503, 0.3406,  ..., 0.3118, 0.3076, 0.1715],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.8.layer_norm.bias': tensor([ 0.0137, -0.0487, -0.0163,  ..., -0.0402, -0.0260, -0.1025],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.8.feed_forward.w_1.weight': tensor([[-0.3015, -0.1304,  0.0669,  ..., -0.0325, -0.2152, -0.0944],\n",
       "         [-0.2681, -0.3765,  0.2029,  ...,  0.0684, -0.3738,  0.5010],\n",
       "         [ 0.1047, -0.2003, -0.0580,  ...,  0.0778, -0.3823,  0.1543],\n",
       "         ...,\n",
       "         [ 0.2312, -0.0673, -0.2435,  ..., -0.1077,  0.1111,  0.4194],\n",
       "         [-0.0049, -0.4768,  0.1390,  ...,  0.1024,  0.1753,  0.2595],\n",
       "         [-0.2419, -0.3213, -0.1992,  ..., -0.2561, -0.0931,  0.1797]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.8.feed_forward.w_1.bias': tensor([ 0.0402, -0.1921,  0.0009,  ..., -0.1022, -0.2727, -0.0250],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.8.feed_forward.w_2.weight': tensor([[ 2.2571e-01,  5.2917e-02,  3.8025e-02,  ...,  1.0181e-01,\n",
       "          -2.2070e-01,  8.1055e-02],\n",
       "         [-2.0561e-03, -7.6782e-02,  1.0541e-01,  ...,  3.5132e-01,\n",
       "          -4.7266e-01,  1.1688e-01],\n",
       "         [ 7.1564e-03, -2.0728e-01, -2.0337e-01,  ...,  1.1371e-01,\n",
       "          -2.3010e-01,  1.0876e-01],\n",
       "         ...,\n",
       "         [-2.3773e-02,  1.3578e-04, -8.6914e-02,  ...,  7.0740e-02,\n",
       "          -2.6953e-01,  2.6047e-02],\n",
       "         [ 9.0515e-02, -1.7542e-01,  3.3740e-01,  ...,  2.4524e-01,\n",
       "           1.2427e-01, -1.3501e-01],\n",
       "         [-9.7717e-02, -6.4453e-02,  9.1858e-02,  ...,  1.1902e-02,\n",
       "          -2.8101e-01,  2.2546e-01]], dtype=torch.float16),\n",
       " 'encoder.transformer.8.feed_forward.w_2.bias': tensor([-0.2666, -0.3450,  0.4014,  ..., -0.3533,  0.4529,  0.5000],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.8.feed_forward.layer_norm.weight': tensor([1.7168, 1.6738, 1.7178,  ..., 1.6064, 1.6738, 1.0039],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.8.feed_forward.layer_norm.bias': tensor([ 0.3455, -0.1840, -0.2661,  ...,  0.2267, -0.2754,  0.4849],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.9.self_attn.linear_keys.weight': tensor([[-0.0934,  0.0262,  0.0886,  ...,  0.1398,  0.0416,  0.0706],\n",
       "         [ 0.0422, -0.0273,  0.0023,  ...,  0.0563, -0.0426, -0.0468],\n",
       "         [ 0.0401,  0.0974, -0.2075,  ...,  0.0203, -0.0175, -0.0652],\n",
       "         ...,\n",
       "         [-0.1060,  0.0012, -0.0681,  ...,  0.2020, -0.1016, -0.1328],\n",
       "         [-0.0526,  0.1748, -0.0415,  ...,  0.1202,  0.0574, -0.2179],\n",
       "         [-0.1506, -0.1725,  0.0459,  ..., -0.1196, -0.1444, -0.0065]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.9.self_attn.linear_keys.bias': tensor([ 0.0083,  0.0081,  0.0163,  ..., -0.0311,  0.0125, -0.0013],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.9.self_attn.linear_values.weight': tensor([[-0.2930, -0.3538,  0.0164,  ...,  0.4600,  0.1323, -0.1270],\n",
       "         [ 0.0593, -0.1552, -0.1039,  ..., -0.1021, -0.5010, -0.3408],\n",
       "         [ 0.5088, -0.0402, -0.3328,  ...,  0.3240,  0.4419, -0.1626],\n",
       "         ...,\n",
       "         [ 0.1453,  0.3970, -0.5234,  ..., -0.2471, -0.4275, -0.0396],\n",
       "         [ 0.2164,  0.4973, -0.1378,  ...,  0.0124, -0.0421,  0.1594],\n",
       "         [-0.1237, -0.2634, -0.2815,  ...,  0.0323,  0.0985, -0.0444]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.9.self_attn.linear_values.bias': tensor([-0.0606,  0.2057,  0.1285,  ...,  0.0414, -0.0179, -0.1126],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.9.self_attn.linear_query.weight': tensor([[ 0.0533, -0.0367, -0.0298,  ..., -0.1113, -0.1151,  0.0941],\n",
       "         [-0.1364,  0.1113, -0.1127,  ..., -0.2859,  0.1389,  0.1482],\n",
       "         [-0.0801, -0.3044, -0.0587,  ...,  0.1190,  0.0527, -0.0526],\n",
       "         ...,\n",
       "         [ 0.0027,  0.0098,  0.0071,  ..., -0.0561, -0.0421,  0.0130],\n",
       "         [-0.0845, -0.0067,  0.0457,  ..., -0.0438,  0.0351,  0.0575],\n",
       "         [ 0.0757,  0.1078,  0.1119,  ...,  0.0146, -0.0102,  0.1037]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.9.self_attn.linear_query.bias': tensor([-0.0395,  0.2544,  0.0701,  ..., -0.0873, -0.4170,  0.1514],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.9.self_attn.final_linear.weight': tensor([[ 0.1595,  0.1053,  0.3035,  ..., -0.3784, -0.0549, -0.2336],\n",
       "         [-0.2150, -0.3525,  0.2207,  ..., -0.0771, -0.3467,  0.3674],\n",
       "         [ 0.0105,  0.2457,  0.1207,  ...,  0.3616, -0.4292,  0.0353],\n",
       "         ...,\n",
       "         [ 0.5269, -0.0899, -0.2500,  ...,  0.1394, -0.0237,  0.4280],\n",
       "         [ 0.0251, -0.3645,  0.2576,  ..., -0.2942, -0.0091, -0.1046],\n",
       "         [-0.3721,  0.2549, -0.1851,  ...,  0.2076, -0.0285, -0.1213]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.9.self_attn.final_linear.bias': tensor([ 0.1194,  0.1234, -0.3315,  ..., -0.1862, -0.2507, -0.3210],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.9.layer_norm.weight': tensor([0.3367, 0.3257, 0.3416,  ..., 0.3267, 0.3186, 0.2015],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.9.layer_norm.bias': tensor([-0.0108, -0.0378, -0.0120,  ..., -0.0397, -0.0296,  0.0007],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.9.feed_forward.w_1.weight': tensor([[-0.0850, -0.0334,  0.3296,  ..., -0.1797,  0.3176, -0.1475],\n",
       "         [ 0.2062,  0.0526,  0.4924,  ...,  0.3694,  0.2479, -0.2455],\n",
       "         [-0.1028, -0.2615, -0.3381,  ...,  0.1091, -0.3706,  0.1146],\n",
       "         ...,\n",
       "         [ 0.1309, -0.0182, -0.2957,  ...,  0.1622,  0.2084,  0.1324],\n",
       "         [-0.0830, -0.2047, -0.0750,  ..., -0.1594, -0.1129, -0.0185],\n",
       "         [ 0.2568,  0.1365, -0.0729,  ...,  0.1531,  0.0869,  0.1686]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.9.feed_forward.w_1.bias': tensor([-0.0652, -0.2603, -0.2491,  ..., -0.2477, -0.1270,  0.0648],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.9.feed_forward.w_2.weight': tensor([[ 0.0651, -0.1250,  0.2908,  ..., -0.2876,  0.0086, -0.3081],\n",
       "         [-0.4285,  0.4309, -0.1884,  ..., -0.0551, -0.2028, -0.0470],\n",
       "         [ 0.1276, -0.3735, -0.4778,  ..., -0.2922,  0.0392,  0.0829],\n",
       "         ...,\n",
       "         [-0.3789,  0.1226,  0.0482,  ...,  0.4937,  0.3669, -0.0059],\n",
       "         [-0.2620,  0.1331, -0.2130,  ..., -0.1647, -0.0382,  0.0402],\n",
       "         [-0.1763, -0.3599,  0.0088,  ..., -0.2683, -0.1316,  0.0857]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.9.feed_forward.w_2.bias': tensor([-0.2073, -0.3306,  0.3516,  ..., -0.1648,  0.4053,  0.5000],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.9.feed_forward.layer_norm.weight': tensor([1.8896, 1.8975, 1.8369,  ..., 1.7158, 1.8867, 1.0020],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.9.feed_forward.layer_norm.bias': tensor([ 0.3481, -0.0704, -0.3877,  ...,  0.0132, -0.2754,  0.4976],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.10.self_attn.linear_keys.weight': tensor([[ 0.0754, -0.0750, -0.0309,  ..., -0.0340,  0.1871, -0.0750],\n",
       "         [-0.1177,  0.2014,  0.1215,  ...,  0.3188, -0.0710, -0.0375],\n",
       "         [ 0.1743, -0.0444,  0.1678,  ..., -0.1851,  0.1493,  0.0254],\n",
       "         ...,\n",
       "         [-0.1746, -0.0400,  0.0757,  ...,  0.1132, -0.1665,  0.1760],\n",
       "         [ 0.0428, -0.0995, -0.0690,  ...,  0.0299, -0.1648,  0.1179],\n",
       "         [-0.0603,  0.0314, -0.0255,  ...,  0.0553,  0.0923,  0.2301]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.10.self_attn.linear_keys.bias': tensor([ 0.0078, -0.0056, -0.0106,  ..., -0.0212, -0.0186, -0.0229],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.10.self_attn.linear_values.weight': tensor([[-0.1522,  0.2383,  0.1865,  ...,  0.2905,  0.0820,  0.0166],\n",
       "         [ 0.2666,  0.1146, -0.0781,  ...,  0.1699, -0.3335,  0.1304],\n",
       "         [-0.4468, -0.3083,  0.0360,  ..., -0.0930, -0.3240, -0.0488],\n",
       "         ...,\n",
       "         [ 0.0994, -0.2683,  0.5088,  ..., -0.0464, -0.1176,  0.3032],\n",
       "         [ 0.2908,  0.2324, -0.0009,  ..., -0.1620,  0.3689,  0.2500],\n",
       "         [ 0.0656, -0.1532, -0.0412,  ...,  0.4946, -0.2261, -0.2732]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.10.self_attn.linear_values.bias': tensor([-0.0977, -0.1240,  0.0131,  ..., -0.0121,  0.0525, -0.0734],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.10.self_attn.linear_query.weight': tensor([[-0.1930,  0.0522,  0.0082,  ...,  0.0521,  0.0998, -0.0263],\n",
       "         [-0.0575,  0.1549, -0.1658,  ..., -0.1860,  0.2107, -0.1044],\n",
       "         [-0.0764,  0.0324,  0.0944,  ...,  0.1270, -0.0540, -0.0314],\n",
       "         ...,\n",
       "         [-0.0464, -0.0764,  0.0583,  ...,  0.2264,  0.1029,  0.0735],\n",
       "         [ 0.0351, -0.0892, -0.0684,  ...,  0.1586,  0.0099, -0.1727],\n",
       "         [-0.0319, -0.0603, -0.1011,  ..., -0.3140,  0.0933, -0.0415]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.10.self_attn.linear_query.bias': tensor([-0.3320,  0.1873,  0.0091,  ...,  0.1371,  0.0784, -0.0020],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.10.self_attn.final_linear.weight': tensor([[-0.5034, -0.2983,  0.0629,  ...,  0.0390, -0.5015, -0.2040],\n",
       "         [ 0.1003,  0.1798,  0.1031,  ..., -0.3835,  0.0084,  0.5010],\n",
       "         [ 0.1758,  0.1630, -0.2917,  ...,  0.0962,  0.4963,  0.5000],\n",
       "         ...,\n",
       "         [ 0.1755,  0.3667,  0.3611,  ..., -0.2289,  0.1060, -0.2825],\n",
       "         [-0.2671, -0.4214, -0.3074,  ..., -0.2625, -0.3059,  0.0285],\n",
       "         [ 0.4990,  0.3005, -0.4709,  ...,  0.2185, -0.4902,  0.2612]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.10.self_attn.final_linear.bias': tensor([-0.0365,  0.1382, -0.2482,  ..., -0.3245, -0.1206, -0.3130],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.10.layer_norm.weight': tensor([0.3611, 0.3506, 0.3416,  ..., 0.3398, 0.3438, 0.2307],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.10.layer_norm.bias': tensor([-0.0012, -0.0452, -0.0125,  ..., -0.0408, -0.0147,  0.0466],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.10.feed_forward.w_1.weight': tensor([[ 0.1156, -0.0344,  0.1548,  ..., -0.2179,  0.3853, -0.0242],\n",
       "         [ 0.3975, -0.1139,  0.0091,  ..., -0.0827,  0.4434, -0.1761],\n",
       "         [ 0.3960,  0.0216, -0.0314,  ...,  0.0225, -0.2169, -0.0543],\n",
       "         ...,\n",
       "         [ 0.1395, -0.3721,  0.1024,  ..., -0.0687,  0.1179, -0.2325],\n",
       "         [-0.1383, -0.0200,  0.1689,  ...,  0.2786, -0.1664, -0.4736],\n",
       "         [-0.0500, -0.1879,  0.0883,  ...,  0.0870,  0.1624, -0.0334]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.10.feed_forward.w_1.bias': tensor([-0.1774, -0.2493,  0.0089,  ...,  0.0812, -0.1042, -0.0112],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.10.feed_forward.w_2.weight': tensor([[ 0.0633,  0.3196, -0.3215,  ..., -0.1116,  0.1827,  0.2673],\n",
       "         [ 0.0570, -0.2937, -0.0856,  ...,  0.1217, -0.1844,  0.2048],\n",
       "         [-0.4844, -0.1295, -0.0923,  ...,  0.0325, -0.0368, -0.0098],\n",
       "         ...,\n",
       "         [-0.0031,  0.2913, -0.1672,  ...,  0.0862,  0.0590, -0.1099],\n",
       "         [-0.2598,  0.1019,  0.2947,  ..., -0.1364, -0.1061,  0.0092],\n",
       "         [ 0.1287, -0.1024,  0.1221,  ..., -0.0401,  0.0828, -0.0964]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.10.feed_forward.w_2.bias': tensor([-0.1252, -0.2593,  0.1232,  ...,  0.0485,  0.2494,  0.5000],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.10.feed_forward.layer_norm.weight': tensor([1.7891, 1.7773, 1.8184,  ..., 1.7373, 1.7686, 1.0000],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.10.feed_forward.layer_norm.bias': tensor([ 0.3123, -0.1324, -0.2803,  ..., -0.2485, -0.1224,  0.3059],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.11.self_attn.linear_keys.weight': tensor([[ 0.0086, -0.0414, -0.3499,  ...,  0.1313,  0.1610,  0.0051],\n",
       "         [ 0.1420,  0.0161, -0.0805,  ...,  0.0367,  0.0750, -0.0147],\n",
       "         [ 0.1501,  0.0788, -0.0873,  ..., -0.0138,  0.0100, -0.0401],\n",
       "         ...,\n",
       "         [-0.2258,  0.1349, -0.0797,  ...,  0.0210, -0.0510, -0.0725],\n",
       "         [ 0.0133, -0.2283, -0.1971,  ...,  0.2808, -0.1448,  0.1022],\n",
       "         [-0.2223,  0.0643, -0.2065,  ..., -0.0892,  0.0763,  0.0452]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.11.self_attn.linear_keys.bias': tensor([-0.0247, -0.0124,  0.0158,  ...,  0.0298,  0.0184, -0.0053],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.11.self_attn.linear_values.weight': tensor([[-0.3228, -0.2213, -0.4314,  ..., -0.1243,  0.0023, -0.1805],\n",
       "         [-0.2544,  0.2683,  0.0088,  ..., -0.3044,  0.3425, -0.0644],\n",
       "         [ 0.0420,  0.0554, -0.0193,  ..., -0.2462,  0.1820, -0.2064],\n",
       "         ...,\n",
       "         [-0.3894,  0.0691, -0.2698,  ...,  0.0479,  0.0766,  0.0311],\n",
       "         [-0.5674, -0.0828, -0.1783,  ..., -0.1588,  0.3806, -0.0439],\n",
       "         [ 0.2032,  0.0678, -0.4326,  ..., -0.1093, -0.0641,  0.1697]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.11.self_attn.linear_values.bias': tensor([ 0.0423,  0.0190, -0.0619,  ..., -0.0843,  0.0345, -0.0243],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.11.self_attn.linear_query.weight': tensor([[ 0.1095, -0.1726,  0.1010,  ..., -0.0748, -0.0682, -0.0528],\n",
       "         [ 0.0183, -0.0203,  0.2111,  ...,  0.5200,  0.1798, -0.0073],\n",
       "         [ 0.1667,  0.0603, -0.1227,  ...,  0.1139,  0.0238,  0.0292],\n",
       "         ...,\n",
       "         [-0.1715,  0.1740,  0.0278,  ..., -0.0693,  0.1809, -0.0255],\n",
       "         [-0.0011, -0.0814,  0.0205,  ...,  0.0806, -0.0358,  0.1978],\n",
       "         [ 0.0307, -0.0151,  0.0194,  ..., -0.1848, -0.0357,  0.0113]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.11.self_attn.linear_query.bias': tensor([-0.0655,  0.1114, -0.0804,  ..., -0.0082,  0.4026, -0.2406],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.11.self_attn.final_linear.weight': tensor([[-0.1425, -0.1304,  0.2463,  ...,  0.2605,  0.0704,  0.5000],\n",
       "         [-0.3113,  0.3472,  0.5015,  ...,  0.2778,  0.4353,  0.2678],\n",
       "         [-0.1364, -0.0914, -0.2534,  ..., -0.2903, -0.2021,  0.0350],\n",
       "         ...,\n",
       "         [-0.3306, -0.3472,  0.4177,  ...,  0.0071, -0.5371, -0.0557],\n",
       "         [ 0.4880, -0.2247, -0.1022,  ...,  0.0623, -0.1614, -0.0088],\n",
       "         [-0.0360, -0.0154,  0.0797,  ...,  0.0604, -0.0828,  0.0997]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.11.self_attn.final_linear.bias': tensor([-0.0739, -0.0063, -0.0346,  ..., -0.2489, -0.1971, -0.2751],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.11.layer_norm.weight': tensor([0.3723, 0.3530, 0.3521,  ..., 0.3430, 0.3640, 0.5117],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.11.layer_norm.bias': tensor([ 0.0076, -0.0373, -0.0152,  ..., -0.0352,  0.0002,  0.1772],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.11.feed_forward.w_1.weight': tensor([[ 2.5732e-01, -1.6187e-01, -4.4678e-01,  ...,  5.9845e-02,\n",
       "          -3.4668e-01, -6.9824e-02],\n",
       "         [ 1.7688e-01, -1.4697e-01,  3.6548e-01,  ..., -4.4287e-01,\n",
       "          -1.0376e-01, -8.9600e-02],\n",
       "         [-2.4670e-01,  2.3022e-01,  3.3618e-01,  ..., -1.1487e-01,\n",
       "          -3.8867e-01,  1.1646e-01],\n",
       "         ...,\n",
       "         [-1.4368e-01, -6.0242e-02,  3.1592e-01,  ...,  7.5500e-02,\n",
       "           1.4355e-01,  8.1177e-02],\n",
       "         [ 4.7302e-02,  3.1885e-01,  1.1621e-01,  ..., -6.1920e-02,\n",
       "          -9.5032e-02,  5.9395e-03],\n",
       "         [-3.4888e-01, -3.6890e-01, -3.6548e-01,  ...,  2.2339e-01,\n",
       "           2.6584e-04,  1.1902e-01]], dtype=torch.float16),\n",
       " 'encoder.transformer.11.feed_forward.w_1.bias': tensor([-0.2379, -0.0254, -0.0665,  ..., -0.1392, -0.1793, -0.1711],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.11.feed_forward.w_2.weight': tensor([[-0.0247, -0.5405,  0.3447,  ...,  0.0279,  0.1028,  0.2759],\n",
       "         [ 0.1445,  0.2554,  0.0915,  ..., -0.1010, -0.0536,  0.1388],\n",
       "         [-0.0830, -0.0148, -0.2334,  ..., -0.0434, -0.1500,  0.2351],\n",
       "         ...,\n",
       "         [-0.4978,  0.0639, -0.3618,  ..., -0.0822,  0.0499,  0.0111],\n",
       "         [-0.1675, -0.2073, -0.1583,  ..., -0.1167, -0.0303,  0.3142],\n",
       "         [-0.0075,  0.0165,  0.0419,  ..., -0.0137,  0.0292, -0.0189]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.11.feed_forward.w_2.bias': tensor([-0.0811, -0.1230,  0.0258,  ...,  0.1039,  0.1157,  0.2913],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.11.feed_forward.layer_norm.weight': tensor([1.3672, 1.2451, 1.1787,  ..., 1.4297, 1.2148, 1.0000],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.11.feed_forward.layer_norm.bias': tensor([ 0.1785, -0.0873, -0.0390,  ..., -0.2549,  0.0047, -0.2499],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.layer_norm.weight': tensor([0.4553, 0.4626, 0.4673,  ..., 0.4241, 0.4702, 0.7456],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.layer_norm.bias': tensor([ 0.0014, -0.0053,  0.0018,  ..., -0.0308,  0.0012, -0.5000],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.embeddings.make_embedding.emb_luts.0.weight': tensor([[-0.0321,  0.0348,  0.0181,  ...,  0.0312, -0.0099, -0.0133],\n",
       "         [-0.0039,  0.0104, -0.0156,  ...,  0.0290, -0.0138, -0.0134],\n",
       "         [-0.0245, -0.0283, -0.0295,  ...,  0.9712, -0.0255, -0.0273],\n",
       "         ...,\n",
       "         [-0.0123, -0.0031, -0.0089,  ...,  0.0645, -0.0182, -0.0740],\n",
       "         [ 0.0085, -0.0088, -0.0091,  ...,  0.0571, -0.0035, -0.1298],\n",
       "         [-0.0076, -0.0107, -0.0051,  ...,  1.0264, -0.0338, -0.1175]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.embeddings.make_embedding.pe.pe': tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  1.0000,  1.0000]],\n",
       " \n",
       "         [[ 0.8415,  0.8317,  0.8218,  ...,  1.0000,  1.0000,  1.0000]],\n",
       " \n",
       "         [[ 0.9093,  0.9236,  0.9365,  ...,  1.0000,  1.0000,  1.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.9563,  0.5417,  0.7653,  ...,  0.8688,  0.8733,  0.8777]],\n",
       " \n",
       "         [[ 0.2705,  0.9999,  0.9649,  ...,  0.8687,  0.8733,  0.8777]],\n",
       " \n",
       "         [[-0.6639,  0.5685,  0.3343,  ...,  0.8687,  0.8732,  0.8776]]]),\n",
       " 'decoder.transformer_layers.0.self_attn.linear_keys.weight': tensor([[ 0.2527, -0.0136, -0.0460,  ...,  0.0115,  0.2996, -0.2917],\n",
       "         [-0.2238, -0.0130,  0.0161,  ..., -0.0285, -0.1180, -0.1647],\n",
       "         [-0.0419,  0.1165,  0.0529,  ..., -0.1235, -0.1420, -0.0377],\n",
       "         ...,\n",
       "         [-0.1418, -0.2262, -0.2668,  ..., -0.4019,  0.1725, -0.4397],\n",
       "         [ 0.8843,  0.7051, -0.0199,  ...,  0.0214, -0.0945, -0.0924],\n",
       "         [ 0.5151,  0.7905,  0.5708,  ..., -0.1842,  0.1663, -0.4202]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.self_attn.linear_keys.bias': tensor([ 0.0171,  0.0190, -0.0028,  ...,  0.0214,  0.0051,  0.0162],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.self_attn.linear_values.weight': tensor([[ 0.1139, -0.0122,  0.0055,  ..., -0.0195, -0.0603, -0.2316],\n",
       "         [-0.0013,  0.0484, -0.0739,  ...,  0.0040,  0.0352,  0.0255],\n",
       "         [ 0.0599, -0.0119, -0.0344,  ...,  0.0804,  0.0247, -0.1047],\n",
       "         ...,\n",
       "         [-0.0121, -0.1064, -0.1040,  ...,  0.1565, -0.1312, -0.0582],\n",
       "         [ 0.0189, -0.0357, -0.0127,  ...,  0.1639, -0.0505,  0.2944],\n",
       "         [ 0.0603,  0.0012, -0.0043,  ...,  0.0229, -0.0510, -0.0253]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.self_attn.linear_values.bias': tensor([-0.1753, -0.0671,  0.0499,  ...,  0.0657, -0.1055,  0.1422],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.self_attn.linear_query.weight': tensor([[-0.0608, -0.0219, -0.0292,  ..., -0.2030,  0.2261,  0.2169],\n",
       "         [ 0.1031,  0.2079, -0.1537,  ...,  0.4563, -0.2065, -0.2493],\n",
       "         [-0.0536, -0.1057,  0.1284,  ...,  0.4861,  0.1749,  0.3396],\n",
       "         ...,\n",
       "         [-0.5005, -0.5000, -0.5034,  ...,  0.0190,  0.0634,  0.2839],\n",
       "         [ 0.7227,  0.9077,  0.5024,  ..., -0.3882,  0.0387, -0.2537],\n",
       "         [ 0.4541,  0.6118,  0.9834,  ...,  0.1675, -0.1345,  0.0667]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.self_attn.linear_query.bias': tensor([ 0.1714, -0.2122,  0.0087,  ..., -0.0495, -0.3044, -0.4622],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.self_attn.final_linear.weight': tensor([[-0.0922, -0.0627,  0.0594,  ..., -0.0709, -0.0629,  0.0347],\n",
       "         [-0.0212,  0.0095,  0.0910,  ..., -0.0070,  0.0226,  0.1028],\n",
       "         [ 0.0100, -0.0276,  0.0167,  ...,  0.0674,  0.0024, -0.0916],\n",
       "         ...,\n",
       "         [-0.0880, -0.1554, -0.0174,  ...,  0.0215, -0.1620,  0.0854],\n",
       "         [-0.1394,  0.1870,  0.1448,  ...,  0.1100, -0.1603,  0.1107],\n",
       "         [-0.1072,  0.2668,  0.0040,  ..., -0.1271, -0.0542,  0.1467]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.self_attn.final_linear.bias': tensor([-0.1847, -0.5151, -0.2632,  ...,  0.0610,  0.0335,  0.0851],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.layer_norm_1.weight': tensor([0.2671, 1.0840, 1.7480,  ..., 0.0716, 0.1120, 0.1166],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.layer_norm_1.bias': tensor([-0.0013,  0.0142,  0.0267,  ...,  0.0082,  0.0058,  0.0043],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.context_attn.linear_keys.weight': tensor([[ 2.7686e-01,  1.1139e-01,  1.2846e-03,  ...,  1.1462e-01,\n",
       "          -3.6523e-01,  2.1698e-02],\n",
       "         [ 3.3984e-01,  5.4596e-02,  1.8262e-01,  ..., -1.6614e-01,\n",
       "           5.4962e-02,  9.5703e-02],\n",
       "         [-3.4485e-02,  3.1982e-02,  2.4927e-01,  ...,  1.3464e-01,\n",
       "          -1.5210e-01,  2.5131e-02],\n",
       "         ...,\n",
       "         [ 1.2830e-01, -9.8999e-02, -4.3060e-02,  ..., -1.1230e-01,\n",
       "           4.0527e-02,  3.7445e-02],\n",
       "         [-6.1572e-01,  3.0398e-05, -5.9692e-02,  ..., -2.1881e-02,\n",
       "           9.4528e-03,  9.6970e-03],\n",
       "         [-4.3845e-04, -4.5288e-02,  7.2693e-02,  ..., -2.0020e-02,\n",
       "          -1.5234e-01, -4.1809e-02]], dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.context_attn.linear_keys.bias': tensor([-0.0035,  0.0259,  0.0131,  ...,  0.0161, -0.0030, -0.0134],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.context_attn.linear_values.weight': tensor([[-2.2168e-01,  1.9531e-01, -2.2302e-01,  ..., -1.5430e-01,\n",
       "          -1.2484e-03, -3.5038e-03],\n",
       "         [-3.2806e-02, -1.0162e-01, -2.2998e-01,  ...,  1.2439e-01,\n",
       "           1.1713e-01,  2.1912e-02],\n",
       "         [ 8.3557e-02, -7.8430e-03,  4.4479e-03,  ..., -7.7515e-02,\n",
       "           1.2612e-04,  6.9237e-03],\n",
       "         ...,\n",
       "         [-1.2396e-01, -4.8584e-02,  6.2744e-02,  ...,  1.0675e-01,\n",
       "          -1.0156e-01, -3.1647e-02],\n",
       "         [ 1.4001e-01, -7.8613e-02, -2.6871e-02,  ..., -1.4417e-01,\n",
       "           1.5289e-02,  5.5725e-02],\n",
       "         [ 7.7400e-03,  1.3008e-02,  1.4258e-01,  ..., -1.0828e-01,\n",
       "           1.8225e-01, -2.6154e-02]], dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.context_attn.linear_values.bias': tensor([ 0.0178, -0.0514, -0.0856,  ..., -0.0892, -0.1333,  0.0561],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.context_attn.linear_query.weight': tensor([[ 0.0181,  0.1831, -0.3489,  ...,  0.0055,  0.0179,  0.3159],\n",
       "         [-0.0963,  0.0248, -0.2091,  ..., -0.0937, -0.1661,  0.0223],\n",
       "         [-0.3262,  0.1173,  0.0812,  ..., -0.0140, -0.0066,  0.2837],\n",
       "         ...,\n",
       "         [ 0.1371, -0.1003,  0.0622,  ...,  0.0039, -0.0370, -0.1047],\n",
       "         [-0.5054,  0.0831, -0.1569,  ..., -0.0121,  0.0830, -0.0902],\n",
       "         [-0.2988, -0.0426,  0.0012,  ...,  0.2725,  0.2629, -0.3853]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.context_attn.linear_query.bias': tensor([-0.0820,  0.0203,  0.0393,  ...,  0.0010, -0.1102, -0.0185],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.context_attn.final_linear.weight': tensor([[ 5.9479e-02,  5.2246e-02,  1.3879e-01,  ..., -9.1675e-02,\n",
       "          -2.8214e-02,  1.6602e-02],\n",
       "         [ 4.7424e-02,  9.0332e-02, -1.6830e-02,  ..., -4.2999e-02,\n",
       "           5.8899e-02, -5.5199e-03],\n",
       "         [ 5.9357e-02, -1.4062e-01,  1.0791e-01,  ...,  7.4707e-02,\n",
       "          -3.4363e-02, -7.2876e-02],\n",
       "         ...,\n",
       "         [-1.1938e-01, -1.7102e-01, -6.7566e-02,  ...,  5.9052e-02,\n",
       "           5.6091e-02,  7.9163e-02],\n",
       "         [ 2.0984e-01,  8.2825e-02,  1.6260e-01,  ...,  4.7180e-02,\n",
       "          -1.4214e-02,  1.7029e-01],\n",
       "         [ 4.0222e-02,  1.0779e-01, -3.9624e-01,  ...,  9.2896e-02,\n",
       "          -4.1656e-02, -9.8169e-05]], dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.context_attn.final_linear.bias': tensor([-0.2084,  0.1163, -0.0021,  ..., -0.0227,  0.0979,  0.2081],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.layer_norm_2.weight': tensor([0.1486, 0.3196, 0.3987,  ..., 0.1199, 0.0912, 0.0984],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.layer_norm_2.bias': tensor([-0.0154, -0.0182, -0.0321,  ..., -0.0105, -0.0070, -0.0171],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.feed_forward.w_1.weight': tensor([[ 0.1344,  0.7622,  0.2412,  ...,  0.2900, -0.2639, -0.3789],\n",
       "         [-0.0172,  0.1448, -0.2893,  ...,  0.1109, -0.2581, -0.0345],\n",
       "         [ 0.1434, -0.1132,  0.1077,  ..., -0.1946,  0.0080, -0.4224],\n",
       "         ...,\n",
       "         [-0.0646,  0.4250, -0.0431,  ..., -0.1595, -0.1165,  0.3054],\n",
       "         [ 0.2461, -0.0331,  0.0623,  ...,  0.1078,  0.2034, -0.1764],\n",
       "         [ 0.3833, -0.1958, -0.3770,  ...,  0.0027, -0.2030, -0.2375]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.feed_forward.w_1.bias': tensor([-0.2781, -0.0742, -0.0874,  ..., -0.0566, -0.0241, -0.2001],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.feed_forward.w_2.weight': tensor([[-0.1371, -0.1132,  0.0466,  ...,  0.2681, -0.0298, -0.1131],\n",
       "         [ 0.2164, -0.0201, -0.0924,  ...,  0.1089, -0.1300, -0.1272],\n",
       "         [ 0.0881,  0.0721, -0.0802,  ..., -0.0063,  0.0746,  0.1890],\n",
       "         ...,\n",
       "         [ 0.1101,  0.0833, -0.1298,  ..., -0.0008, -0.0935, -0.0485],\n",
       "         [-0.3896,  0.1687, -0.1404,  ...,  0.0498,  0.0757,  0.1858],\n",
       "         [-0.2925, -0.0252,  0.2664,  ...,  0.1076,  0.4316,  0.0273]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.feed_forward.w_2.bias': tensor([ 9.9304e-02,  5.1483e-02,  2.4277e-02,  ..., -4.9146e-01,\n",
       "         -1.7102e-01, -5.7220e-06], dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.feed_forward.layer_norm.weight': tensor([0.2993, 0.6899, 0.8115,  ..., 0.2859, 0.1858, 0.2041],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.feed_forward.layer_norm.bias': tensor([-0.0114, -0.0121, -0.0416,  ...,  0.0494,  0.0188,  0.0542],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.self_attn.linear_keys.weight': tensor([[ 0.1534, -0.1125, -0.1119,  ..., -0.1871,  0.2374,  0.0232],\n",
       "         [ 0.3218, -0.1428,  0.0255,  ..., -0.1315, -0.0137,  0.1196],\n",
       "         [-0.1437, -0.1470, -0.0161,  ..., -0.2886,  0.1324, -0.1108],\n",
       "         ...,\n",
       "         [-0.1249,  0.1697,  0.4082,  ..., -0.3542, -0.2754, -0.1086],\n",
       "         [ 0.4036, -0.0794,  0.0101,  ..., -0.0826,  0.5649, -0.0558],\n",
       "         [-0.0320,  0.0214, -0.0146,  ...,  0.2869, -0.0469, -0.2600]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.self_attn.linear_keys.bias': tensor([-0.0115,  0.0259, -0.0176,  ...,  0.0260,  0.0236,  0.0175],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.self_attn.linear_values.weight': tensor([[ 0.1611,  0.2081,  0.1771,  ..., -0.2059, -0.0025,  0.0425],\n",
       "         [ 0.0439, -0.1763, -0.0055,  ..., -0.1410, -0.0632, -0.1220],\n",
       "         [ 0.1693,  0.2229,  0.4146,  ...,  0.0634,  0.0104,  0.1014],\n",
       "         ...,\n",
       "         [ 0.0663, -0.0892, -0.1144,  ..., -0.2166, -0.0717,  0.0168],\n",
       "         [ 0.0919,  0.2917, -0.1696,  ..., -0.0205, -0.2695,  0.1384],\n",
       "         [-0.1870,  0.1490, -0.2499,  ..., -0.0961, -0.0006,  0.1333]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.self_attn.linear_values.bias': tensor([ 0.0769, -0.0881, -0.1488,  ..., -0.0063, -0.0072,  0.0170],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.self_attn.linear_query.weight': tensor([[-0.0667,  0.2040,  0.0103,  ..., -0.0268, -0.1711,  0.0939],\n",
       "         [-0.2396,  0.3296, -0.2190,  ...,  0.1805,  0.2666, -0.0803],\n",
       "         [ 0.0565,  0.0851, -0.0085,  ...,  0.4153,  0.1097, -0.0129],\n",
       "         ...,\n",
       "         [-0.0325, -0.0953,  0.1277,  ..., -0.4856, -0.5068,  0.2008],\n",
       "         [ 0.0311, -0.1217, -0.0499,  ...,  0.2189, -0.4949,  0.3477],\n",
       "         [-0.2493,  0.1967,  0.4160,  ..., -0.4136, -0.1923,  0.0126]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.self_attn.linear_query.bias': tensor([ 0.0458, -0.0001,  0.0027,  ..., -0.0262, -0.0651,  0.0208],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.self_attn.final_linear.weight': tensor([[-0.1395,  0.1007, -0.2021,  ..., -0.2854,  0.2642,  0.0026],\n",
       "         [-0.2791,  0.3120, -0.3350,  ...,  0.0471, -0.1516,  0.1459],\n",
       "         [ 0.2573, -0.1997,  0.0403,  ...,  0.0028,  0.0232, -0.1825],\n",
       "         ...,\n",
       "         [ 0.4160,  0.3020, -0.0767,  ...,  0.2146, -0.3484,  0.1781],\n",
       "         [-0.0596, -0.2515,  0.1478,  ...,  0.0984, -0.0452,  0.0968],\n",
       "         [-0.3391,  0.4890, -0.1302,  ...,  0.3733, -0.3181, -0.0583]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.self_attn.final_linear.bias': tensor([-0.1322, -0.5068, -0.3179,  ..., -0.4104,  0.5020,  0.0186],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.layer_norm_1.weight': tensor([0.2335, 0.3899, 0.4363,  ..., 0.1646, 0.1715, 0.1766],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.layer_norm_1.bias': tensor([ 0.0122,  0.0031,  0.0142,  ..., -0.0172,  0.0160,  0.0054],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.context_attn.linear_keys.weight': tensor([[ 3.3398e-01,  3.1158e-02,  4.3152e-02,  ..., -1.9287e-01,\n",
       "          -1.6858e-01,  4.2175e-02],\n",
       "         [ 3.4515e-02, -1.7725e-01, -7.6660e-02,  ..., -5.5206e-02,\n",
       "          -6.0150e-02, -2.6245e-01],\n",
       "         [-1.0291e-01, -2.0947e-01,  4.8615e-02,  ..., -1.3843e-01,\n",
       "           1.7810e-01, -4.2267e-02],\n",
       "         ...,\n",
       "         [ 3.3081e-02,  8.9050e-02, -9.6863e-02,  ..., -4.3579e-02,\n",
       "          -4.2603e-02,  7.5928e-02],\n",
       "         [ 1.0266e-01, -3.2788e-01, -1.7285e-01,  ..., -2.2192e-01,\n",
       "           2.8801e-04,  2.7344e-01],\n",
       "         [ 3.6304e-01, -2.3499e-01, -4.5776e-01,  ...,  1.2268e-01,\n",
       "           1.1481e-01, -2.8976e-02]], dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.context_attn.linear_keys.bias': tensor([-0.0274, -0.0099, -0.0295,  ...,  0.0261, -0.0028, -0.0228],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.context_attn.linear_values.weight': tensor([[-0.0637, -0.1584,  0.0154,  ..., -0.1114, -0.0318, -0.0013],\n",
       "         [ 0.2046,  0.1764, -0.1729,  ..., -0.3098,  0.1407, -0.0028],\n",
       "         [ 0.0202,  0.1407,  0.1124,  ...,  0.0605, -0.0785, -0.0263],\n",
       "         ...,\n",
       "         [ 0.1542,  0.1243,  0.1514,  ...,  0.1077, -0.0696,  0.0235],\n",
       "         [-0.1639, -0.0312,  0.2416,  ...,  0.0569,  0.0289,  0.0510],\n",
       "         [-0.0209,  0.0379,  0.0607,  ...,  0.0742,  0.1700, -0.0098]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.context_attn.linear_values.bias': tensor([ 0.0453, -0.0273, -0.0125,  ..., -0.0927, -0.0215,  0.1722],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.context_attn.linear_query.weight': tensor([[-0.0729, -0.0311, -0.1705,  ...,  0.1267,  0.0778, -0.0111],\n",
       "         [ 0.0575,  0.2003,  0.1246,  ...,  0.1231, -0.0115,  0.0741],\n",
       "         [ 0.1191,  0.1030, -0.2544,  ...,  0.0974,  0.0105, -0.0771],\n",
       "         ...,\n",
       "         [-0.5625, -0.1671,  0.2000,  ..., -0.2129,  0.0068,  0.2164],\n",
       "         [ 0.0772, -0.0503,  0.0390,  ..., -0.3298, -0.1742,  0.3379],\n",
       "         [ 0.0915,  0.0277, -0.4065,  ..., -0.1199,  0.4858, -0.0520]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.context_attn.linear_query.bias': tensor([-0.2184, -0.5171, -0.0284,  ..., -0.0183,  0.0548,  0.1344],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.context_attn.final_linear.weight': tensor([[-0.0327,  0.3237, -0.1772,  ...,  0.2573, -0.1348,  0.1152],\n",
       "         [ 0.0838,  0.0265, -0.2900,  ...,  0.2081,  0.1089,  0.0315],\n",
       "         [ 0.1299,  0.1479,  0.1301,  ..., -0.1229, -0.1685,  0.0691],\n",
       "         ...,\n",
       "         [-0.1422,  0.1223,  0.1954,  ..., -0.1614, -0.1797,  0.1772],\n",
       "         [ 0.0510,  0.0167, -0.0936,  ..., -0.1155, -0.2397,  0.1880],\n",
       "         [-0.3721, -0.2397,  0.0360,  ...,  0.0610, -0.0598,  0.0428]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.context_attn.final_linear.bias': tensor([-0.0711,  0.3042, -0.3445,  ..., -0.2491,  0.4126,  0.2225],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.layer_norm_2.weight': tensor([0.1310, 0.2273, 0.2188,  ..., 0.1075, 0.1068, 0.0997],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.layer_norm_2.bias': tensor([-0.0150, -0.0313, -0.0118,  ...,  0.0090,  0.0072, -0.0125],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.feed_forward.w_1.weight': tensor([[ 0.0250,  0.2991, -0.1091,  ..., -0.1277, -0.1576,  0.0674],\n",
       "         [ 0.2976, -0.1415,  0.4507,  ..., -0.1866,  0.1283,  0.0190],\n",
       "         [ 0.3267, -0.0324,  0.1790,  ..., -0.1440, -0.3320, -0.0600],\n",
       "         ...,\n",
       "         [ 0.1567,  0.0956,  0.1346,  ..., -0.3232, -0.1976, -0.3655],\n",
       "         [ 0.4209,  0.2257,  0.0028,  ..., -0.1316, -0.3711,  0.0569],\n",
       "         [ 0.1774, -0.2188,  0.1565,  ..., -0.2905,  0.0203, -0.2708]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.feed_forward.w_1.bias': tensor([-0.1600, -0.0608, -0.1709,  ..., -0.2771, -0.2356, -0.0980],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.feed_forward.w_2.weight': tensor([[-0.2366, -0.0439, -0.2294,  ...,  0.1348,  0.0668, -0.1799],\n",
       "         [ 0.1781,  0.1968,  0.0236,  ..., -0.1300, -0.2268,  0.2125],\n",
       "         [-0.0632,  0.1316,  0.0245,  ...,  0.0402,  0.0947, -0.1364],\n",
       "         ...,\n",
       "         [ 0.0862,  0.2155, -0.0999,  ..., -0.3252, -0.0651,  0.0750],\n",
       "         [ 0.1539,  0.5024,  0.3135,  ..., -0.2551,  0.0806, -0.0781],\n",
       "         [ 0.0167,  0.2075,  0.0334,  ..., -0.1337,  0.0619,  0.1277]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.feed_forward.w_2.bias': tensor([ 0.0667,  0.1331,  0.1181,  ..., -0.2947, -0.1676, -0.1841],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.feed_forward.layer_norm.weight': tensor([0.3525, 0.4556, 0.4563,  ..., 0.3020, 0.2478, 0.2815],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.feed_forward.layer_norm.bias': tensor([-0.0249, -0.0007, -0.0149,  ...,  0.0699,  0.0163,  0.0864],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.self_attn.linear_keys.weight': tensor([[-0.3308,  0.3665,  0.0386,  ...,  0.1205, -0.4648,  0.2186],\n",
       "         [ 0.0590,  0.1039,  0.0496,  ...,  0.1836, -0.3901,  0.3811],\n",
       "         [-0.4600, -0.1686,  0.0210,  ...,  0.0736,  0.1978,  0.0778],\n",
       "         ...,\n",
       "         [-0.5742, -0.1058, -0.0801,  ...,  0.3740, -0.0912, -0.0032],\n",
       "         [ 0.2705, -0.2336,  0.1692,  ..., -0.0341, -0.2332,  0.3018],\n",
       "         [ 0.0272, -0.0501,  0.1230,  ..., -0.1545, -0.0927, -0.0757]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.self_attn.linear_keys.bias': tensor([ 0.0152,  0.0089,  0.0074,  ...,  0.0129, -0.0011,  0.0167],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.self_attn.linear_values.weight': tensor([[-0.0995, -0.1671, -0.5259,  ..., -0.2869, -0.1268,  0.0750],\n",
       "         [ 0.0398, -0.0916, -0.0912,  ..., -0.1322, -0.2208,  0.1377],\n",
       "         [ 0.3433,  0.0499,  0.0323,  ...,  0.1874, -0.1367,  0.1506],\n",
       "         ...,\n",
       "         [ 0.0250,  0.0933,  0.0583,  ...,  0.0499, -0.1655, -0.2617],\n",
       "         [ 0.1227, -0.1143,  0.0930,  ...,  0.0697,  0.1770,  0.0898],\n",
       "         [ 0.0276,  0.1170, -0.1428,  ..., -0.0865,  0.0061, -0.1729]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.self_attn.linear_values.bias': tensor([-0.0004,  0.0150,  0.0520,  ...,  0.0121,  0.0303, -0.0188],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.self_attn.linear_query.weight': tensor([[-0.3401,  0.2352,  0.4128,  ...,  0.2554, -0.1636, -0.0297],\n",
       "         [ 0.1205, -0.1771, -0.1298,  ..., -0.0942,  0.0591, -0.1454],\n",
       "         [-0.0718, -0.0007,  0.0856,  ..., -0.1609,  0.2598, -0.0106],\n",
       "         ...,\n",
       "         [-0.0469, -0.1349,  0.0050,  ...,  0.1782,  0.1202, -0.1854],\n",
       "         [ 0.0954,  0.0739, -0.3987,  ...,  0.0170,  0.0279,  0.0166],\n",
       "         [-0.0461,  0.4138, -0.0368,  ..., -0.1155, -0.2113, -0.0650]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.self_attn.linear_query.bias': tensor([-0.0418, -0.0768,  0.0216,  ..., -0.0235,  0.2318, -0.0897],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.self_attn.final_linear.weight': tensor([[-1.5540e-01, -1.6052e-01,  9.6558e-02,  ...,  9.5215e-02,\n",
       "          -5.2490e-01, -1.0329e-04],\n",
       "         [-1.7261e-01,  1.2408e-01,  8.1604e-02,  ..., -2.1411e-01,\n",
       "          -1.5759e-01,  1.6846e-01],\n",
       "         [-2.4548e-01, -2.9126e-01, -7.3509e-03,  ..., -1.0956e-01,\n",
       "          -1.0693e-01, -7.5806e-02],\n",
       "         ...,\n",
       "         [ 2.4243e-01, -2.2388e-01,  3.2446e-01,  ..., -3.1104e-01,\n",
       "           1.0699e-01, -3.1201e-01],\n",
       "         [-4.9121e-01,  1.8164e-01,  1.7285e-01,  ..., -5.4102e-01,\n",
       "           1.8896e-01, -1.7456e-01],\n",
       "         [-1.4868e-01, -4.2686e-03, -1.5247e-01,  ..., -2.5244e-01,\n",
       "           2.0667e-01, -1.2164e-01]], dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.self_attn.final_linear.bias': tensor([-0.4189,  0.2380, -0.4163,  ..., -0.5039, -0.1493,  0.4475],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.layer_norm_1.weight': tensor([0.2588, 0.3323, 0.3691,  ..., 0.1847, 0.1929, 0.1981],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.layer_norm_1.bias': tensor([ 0.0080,  0.0052,  0.0182,  ..., -0.0058,  0.0125, -0.0036],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.context_attn.linear_keys.weight': tensor([[ 3.9368e-03,  1.2466e-02, -1.2500e-01,  ..., -4.4922e-01,\n",
       "           1.3086e-01, -1.2421e-01],\n",
       "         [-4.1553e-01, -2.4854e-01,  6.1249e-02,  ..., -1.4575e-01,\n",
       "           1.7358e-01, -3.6469e-02],\n",
       "         [ 2.8638e-01,  1.9629e-01, -1.6016e-01,  ...,  2.6993e-02,\n",
       "           1.7017e-01, -1.0522e-01],\n",
       "         ...,\n",
       "         [ 4.3457e-01, -1.6064e-01,  2.1777e-01,  ...,  7.3486e-02,\n",
       "          -2.4500e-01,  2.2827e-02],\n",
       "         [ 8.1360e-02,  3.6682e-02, -1.1883e-03,  ..., -8.8074e-02,\n",
       "          -5.0568e-02, -5.5603e-02],\n",
       "         [ 3.4475e-04,  4.4116e-01, -6.1188e-02,  ...,  6.7322e-02,\n",
       "           1.3672e-01, -5.3192e-02]], dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.context_attn.linear_keys.bias': tensor([ 0.0274,  0.0169, -0.0277,  ..., -0.0282, -0.0058, -0.0312],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.context_attn.linear_values.weight': tensor([[ 0.0640, -0.0147, -0.0991,  ..., -0.4907, -0.1158,  0.0133],\n",
       "         [-0.0900, -0.2211, -0.1050,  ..., -0.0369,  0.0284, -0.0867],\n",
       "         [ 0.2096, -0.0718, -0.1810,  ..., -0.0245,  0.1345,  0.0412],\n",
       "         ...,\n",
       "         [-0.1543, -0.3372, -0.2124,  ...,  0.0370, -0.2200, -0.0192],\n",
       "         [-0.0036,  0.1351, -0.0414,  ..., -0.2722, -0.1978,  0.0154],\n",
       "         [-0.0208,  0.1348, -0.2732,  ...,  0.0633,  0.1826,  0.0693]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.context_attn.linear_values.bias': tensor([-0.0737,  0.0847,  0.0364,  ...,  0.0214,  0.0005, -0.0131],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.context_attn.linear_query.weight': tensor([[-0.4275,  0.0265,  0.0817,  ..., -0.0469,  0.2341,  0.2118],\n",
       "         [ 0.3743,  0.1727,  0.0540,  ..., -0.3494,  0.1705, -0.0107],\n",
       "         [-0.1183, -0.0934,  0.1401,  ..., -0.1702,  0.0417, -0.0448],\n",
       "         ...,\n",
       "         [ 0.1122,  0.2097,  0.0602,  ...,  0.1998, -0.2957,  0.0191],\n",
       "         [ 0.2045, -0.2030, -0.1022,  ..., -0.0297, -0.2910,  0.0159],\n",
       "         [-0.4851,  0.4463,  0.0319,  ...,  0.0139, -0.0826, -0.0349]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.context_attn.linear_query.bias': tensor([-0.0117,  0.0652, -0.1204,  ...,  0.0750,  0.0415, -0.0578],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.context_attn.final_linear.weight': tensor([[-5.6915e-02,  1.0187e-01, -2.7832e-01,  ...,  2.2229e-01,\n",
       "          -3.0542e-01, -7.3204e-03],\n",
       "         [ 7.6111e-02,  1.6968e-01, -1.3794e-01,  ..., -8.0750e-02,\n",
       "          -4.1077e-02,  4.9683e-02],\n",
       "         [-4.5776e-02, -2.0325e-01, -1.5405e-01,  ..., -2.6611e-01,\n",
       "           2.3071e-01, -2.0093e-01],\n",
       "         ...,\n",
       "         [ 3.4741e-01,  2.8296e-01,  3.5187e-02,  ...,  3.4546e-02,\n",
       "           7.2754e-02,  1.6708e-02],\n",
       "         [-2.9907e-01,  3.6499e-02, -2.6807e-01,  ..., -3.5181e-01,\n",
       "          -2.1741e-01, -3.6224e-02],\n",
       "         [-2.3279e-01,  3.2013e-02,  2.1243e-04,  ..., -2.9346e-01,\n",
       "           4.3140e-01,  1.1359e-01]], dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.context_attn.final_linear.bias': tensor([-0.1670,  0.2517,  0.0768,  ..., -0.3137, -0.0340, -0.1135],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.layer_norm_2.weight': tensor([0.1266, 0.1838, 0.1929,  ..., 0.1061, 0.0986, 0.1010],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.layer_norm_2.bias': tensor([-0.0076, -0.0356, -0.0011,  ..., -0.0028, -0.0035, -0.0181],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.feed_forward.w_1.weight': tensor([[-0.1963, -0.0854,  0.0271,  ..., -0.0640, -0.1954,  0.2534],\n",
       "         [-0.3438,  0.1814, -0.1447,  ..., -0.3826, -0.0184, -0.3777],\n",
       "         [ 0.0417,  0.0775,  0.2869,  ...,  0.1899,  0.0681, -0.3806],\n",
       "         ...,\n",
       "         [-0.2186, -0.0697, -0.2954,  ..., -0.0013, -0.0532, -0.4646],\n",
       "         [-0.1605,  0.4458,  0.0753,  ...,  0.0030, -0.1979,  0.0666],\n",
       "         [ 0.1459,  0.1836,  0.0529,  ...,  0.0804, -0.0881,  0.0609]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.feed_forward.w_1.bias': tensor([-0.1088, -0.2505, -0.1223,  ..., -0.2303, -0.1230, -0.0989],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.feed_forward.w_2.weight': tensor([[-0.1610, -0.3735, -0.0555,  ...,  0.2761, -0.0305, -0.1532],\n",
       "         [-0.0477, -0.2759,  0.1680,  ...,  0.0407, -0.1489, -0.1984],\n",
       "         [-0.0375, -0.1848, -0.0933,  ...,  0.1871,  0.2727, -0.1232],\n",
       "         ...,\n",
       "         [-0.0792, -0.1058, -0.3218,  ...,  0.0746,  0.1451,  0.1779],\n",
       "         [-0.0492, -0.1247,  0.2181,  ...,  0.3079, -0.0853,  0.2299],\n",
       "         [ 0.4763,  0.1882, -0.0437,  ..., -0.0560, -0.4338, -0.0142]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.feed_forward.w_2.bias': tensor([-0.1108, -0.2070, -0.0164,  ..., -0.2815, -0.0843, -0.2876],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.feed_forward.layer_norm.weight': tensor([0.3970, 0.4524, 0.4319,  ..., 0.3435, 0.3228, 0.3274],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.feed_forward.layer_norm.bias': tensor([-0.0156,  0.0081,  0.0066,  ...,  0.0600, -0.0039,  0.0792],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.self_attn.linear_keys.weight': tensor([[-0.0989,  0.3494,  0.1242,  ..., -0.0166, -0.2554, -0.0117],\n",
       "         [-0.0774, -0.2800,  0.0182,  ..., -0.2566, -0.0605, -0.3606],\n",
       "         [ 0.1135, -0.0656,  0.0745,  ...,  0.1853, -0.1089, -0.1870],\n",
       "         ...,\n",
       "         [-0.1294, -0.1048, -0.0658,  ...,  0.0122, -0.0033, -0.1639],\n",
       "         [-0.0522,  0.1150, -0.0699,  ..., -0.0789,  0.0958,  0.0314],\n",
       "         [-0.1689,  0.0449,  0.0771,  ..., -0.0376, -0.2283, -0.1467]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.self_attn.linear_keys.bias': tensor([-0.0116, -0.0057, -0.0070,  ...,  0.0291, -0.0091, -0.0291],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.self_attn.linear_values.weight': tensor([[-0.1167, -0.2267, -0.1385,  ..., -0.1125,  0.1224, -0.4749],\n",
       "         [-0.1150, -0.0827, -0.2052,  ...,  0.1290, -0.1851,  0.3667],\n",
       "         [ 0.0052,  0.1235,  0.1295,  ..., -0.2764,  0.4204,  0.4370],\n",
       "         ...,\n",
       "         [ 0.3010, -0.2048, -0.3379,  ..., -0.2324,  0.0846,  0.2581],\n",
       "         [-0.4292, -0.1766,  0.1385,  ...,  0.1300,  0.1146,  0.2145],\n",
       "         [ 0.2388, -0.5005, -0.0915,  ..., -0.0520, -0.3730,  0.1678]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.self_attn.linear_values.bias': tensor([-0.0255,  0.0488,  0.0688,  ...,  0.0421, -0.0326,  0.0906],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.self_attn.linear_query.weight': tensor([[ 0.2737, -0.1266,  0.1499,  ...,  0.0596,  0.1345,  0.2371],\n",
       "         [-0.0415,  0.2070,  0.0330,  ..., -0.1792,  0.1012,  0.2859],\n",
       "         [-0.0612,  0.1016,  0.1646,  ..., -0.2700, -0.0338, -0.0159],\n",
       "         ...,\n",
       "         [ 0.1676, -0.0497,  0.4077,  ..., -0.0945,  0.0942, -0.0088],\n",
       "         [-0.1548, -0.1628, -0.3193,  ...,  0.2554, -0.0501,  0.0714],\n",
       "         [ 0.2166,  0.1407, -0.0704,  ...,  0.0569,  0.3152, -0.0768]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.self_attn.linear_query.bias': tensor([ 0.0457,  0.1059,  0.3831,  ...,  0.3333,  0.0374, -0.0630],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.self_attn.final_linear.weight': tensor([[-0.1353, -0.2498,  0.0072,  ...,  0.0026,  0.3059,  0.0142],\n",
       "         [ 0.0606, -0.2023,  0.1234,  ...,  0.5112, -0.1735,  0.4958],\n",
       "         [ 0.0301,  0.1292,  0.4243,  ..., -0.1786, -0.1874,  0.2764],\n",
       "         ...,\n",
       "         [ 0.1820,  0.1312,  0.3752,  ..., -0.4602,  0.6631, -0.1891],\n",
       "         [ 0.1621, -0.3711,  0.2749,  ...,  0.4641,  0.3682, -0.2791],\n",
       "         [-0.3870,  0.2444,  0.1794,  ..., -0.2311,  0.4253, -0.3052]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.self_attn.final_linear.bias': tensor([-0.3191, -0.0130, -0.3569,  ..., -0.4990, -0.2812,  0.3171],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.layer_norm_1.weight': tensor([0.2649, 0.2991, 0.3367,  ..., 0.1940, 0.2025, 0.2090],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.layer_norm_1.bias': tensor([ 0.0098,  0.0048,  0.0200,  ..., -0.0096,  0.0063, -0.0139],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.context_attn.linear_keys.weight': tensor([[-0.1865, -0.0974,  0.1832,  ..., -0.2744, -0.0507, -0.1809],\n",
       "         [ 0.3071, -0.1432,  0.2676,  ...,  0.0955, -0.1697,  0.0543],\n",
       "         [ 0.0473,  0.2034,  0.2032,  ..., -0.0693, -0.2028,  0.1306],\n",
       "         ...,\n",
       "         [-0.2595,  0.1549, -0.1284,  ..., -0.0772, -0.1320,  0.1205],\n",
       "         [-0.1119,  0.0875,  0.0643,  ...,  0.0091, -0.0217, -0.0189],\n",
       "         [ 0.0364,  0.2268,  0.0769,  ..., -0.1428,  0.0320,  0.0672]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.context_attn.linear_keys.bias': tensor([-0.0310,  0.0280,  0.0024,  ...,  0.0099, -0.0284, -0.0260],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.context_attn.linear_values.weight': tensor([[ 2.4078e-02,  2.6733e-01,  2.0782e-02,  ...,  1.9263e-01,\n",
       "           1.7105e-02,  2.2621e-03],\n",
       "         [ 1.1975e-01,  6.5063e-02, -1.6510e-02,  ..., -3.9642e-02,\n",
       "           1.5393e-01,  4.3671e-02],\n",
       "         [ 2.0248e-02,  1.1452e-02,  8.4961e-02,  ..., -2.9495e-02,\n",
       "          -1.4844e-01,  1.0025e-02],\n",
       "         ...,\n",
       "         [ 2.6025e-01,  3.3112e-02,  6.1096e-02,  ...,  1.0437e-01,\n",
       "           3.6987e-01, -4.9896e-03],\n",
       "         [-7.1680e-01, -2.0828e-02, -1.8994e-01,  ...,  4.0741e-02,\n",
       "           2.2595e-01,  1.6713e-04],\n",
       "         [ 2.0004e-02,  2.5366e-01,  2.1960e-01,  ..., -1.0065e-01,\n",
       "           6.7871e-02,  1.7929e-02]], dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.context_attn.linear_values.bias': tensor([-0.0029,  0.0220, -0.1296,  ...,  0.0403,  0.0458,  0.0118],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.context_attn.linear_query.weight': tensor([[-0.0173,  0.0173,  0.1548,  ...,  0.4448,  0.2622, -0.3184],\n",
       "         [ 0.0507,  0.5000,  0.1766,  ...,  0.2209, -0.3264, -0.1075],\n",
       "         [ 0.0124, -0.0648,  0.1494,  ..., -0.2532,  0.1809, -0.1466],\n",
       "         ...,\n",
       "         [ 0.2498, -0.0932, -0.2915,  ..., -0.2399, -0.0685,  0.0255],\n",
       "         [ 0.0212,  0.2756,  0.0865,  ...,  0.0206, -0.1403,  0.0765],\n",
       "         [ 0.0407,  0.2849, -0.2264,  ..., -0.1996,  0.0529,  0.0208]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.context_attn.linear_query.bias': tensor([-0.1206,  0.1025,  0.0757,  ..., -0.0092, -0.0883, -0.1151],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.context_attn.final_linear.weight': tensor([[ 0.0876, -0.1027,  0.1193,  ...,  0.2145,  0.0585, -0.1265],\n",
       "         [ 0.0244,  0.0413, -0.1685,  ...,  0.0888,  0.1541, -0.0012],\n",
       "         [-0.1416, -0.0668, -0.3159,  ..., -0.0815,  0.0106, -0.0503],\n",
       "         ...,\n",
       "         [ 0.0349, -0.1460, -0.0229,  ...,  0.1868, -0.0360,  0.0837],\n",
       "         [ 0.1722,  0.0825, -0.0028,  ...,  0.3721,  0.1023,  0.1249],\n",
       "         [-0.0881,  0.0850, -0.1405,  ..., -0.2732,  0.2292, -0.1927]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.context_attn.final_linear.bias': tensor([ 0.1139,  0.2313,  0.3328,  ...,  0.1598,  0.1838, -0.0592],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.layer_norm_2.weight': tensor([0.1249, 0.1664, 0.1544,  ..., 0.0951, 0.1037, 0.1024],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.layer_norm_2.bias': tensor([-0.0096, -0.0279, -0.0147,  ..., -0.0196,  0.0010, -0.0201],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.feed_forward.w_1.weight': tensor([[ 0.2854, -0.1564,  0.2632,  ...,  0.3538,  0.1858, -0.2240],\n",
       "         [ 0.1320, -0.0382,  0.1373,  ...,  0.0072, -0.1225,  0.2688],\n",
       "         [-0.0849, -0.1731,  0.4026,  ..., -0.0120,  0.2983, -0.0650],\n",
       "         ...,\n",
       "         [-0.2664, -0.1324,  0.2439,  ...,  0.1598,  0.0373,  0.1060],\n",
       "         [-0.0652,  0.0218,  0.0721,  ..., -0.0628, -0.5562, -0.1991],\n",
       "         [ 0.0721,  0.0939, -0.2037,  ..., -0.0305, -0.1075, -0.0715]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.feed_forward.w_1.bias': tensor([-0.1403, -0.0063, -0.0494,  ...,  0.0057, -0.2385, -0.1196],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.feed_forward.w_2.weight': tensor([[-3.0322e-01, -6.6833e-02,  1.0114e-01,  ..., -5.2452e-04,\n",
       "           4.5197e-02,  2.4658e-02],\n",
       "         [ 7.6111e-02, -1.4819e-01,  2.0898e-01,  ..., -1.3293e-01,\n",
       "           1.6663e-02,  5.0812e-02],\n",
       "         [ 1.4941e-01, -1.0266e-01, -1.2354e-01,  ..., -6.5327e-04,\n",
       "          -8.2458e-02, -1.0616e-04],\n",
       "         ...,\n",
       "         [ 8.2153e-02,  1.2646e-01, -1.3855e-01,  ...,  7.0679e-02,\n",
       "          -1.0352e-01, -8.3435e-02],\n",
       "         [-2.7199e-03,  3.4009e-01, -2.1057e-01,  ..., -4.7485e-01,\n",
       "           7.7454e-02, -3.9380e-01],\n",
       "         [ 2.1704e-01,  3.9139e-03,  8.0872e-02,  ..., -1.1017e-01,\n",
       "          -1.6699e-01, -2.3010e-02]], dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.feed_forward.w_2.bias': tensor([-0.1926, -0.1788,  0.1004,  ..., -0.1218,  0.0177, -0.1982],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.feed_forward.layer_norm.weight': tensor([0.4517, 0.4846, 0.4919,  ..., 0.3994, 0.3762, 0.3960],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.feed_forward.layer_norm.bias': tensor([0.0141, 0.0581, 0.0347,  ..., 0.0001, 0.0165, 0.0414],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.self_attn.linear_keys.weight': tensor([[ 0.2207,  0.1823, -0.0239,  ..., -0.0892, -0.2250, -0.0438],\n",
       "         [-0.1675,  0.0461, -0.2908,  ..., -0.0573,  0.2861, -0.1185],\n",
       "         [-0.2162,  0.2639,  0.0948,  ...,  0.0665,  0.1389,  0.2520],\n",
       "         ...,\n",
       "         [-0.0280, -0.0919,  0.0092,  ...,  0.2295, -0.2079,  0.0559],\n",
       "         [ 0.1720,  0.1978, -0.1338,  ..., -0.3010,  0.5542,  0.1371],\n",
       "         [ 0.1345,  0.1317, -0.1661,  ...,  0.0614, -0.0829, -0.2491]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.self_attn.linear_keys.bias': tensor([ 0.0078,  0.0124, -0.0312,  ..., -0.0085, -0.0288, -0.0269],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.self_attn.linear_values.weight': tensor([[-0.2673,  0.0440,  0.2004,  ...,  0.0255, -0.0587,  0.0551],\n",
       "         [ 0.0173, -0.0415, -0.1914,  ...,  0.2194, -0.2268,  0.0632],\n",
       "         [-0.3674, -0.1038,  0.1384,  ...,  0.1515,  0.1569, -0.2007],\n",
       "         ...,\n",
       "         [ 0.0292,  0.3396, -0.3240,  ...,  0.2029,  0.3286,  0.3315],\n",
       "         [ 0.1886, -0.2532,  0.0946,  ..., -0.0008,  0.2737,  0.4102],\n",
       "         [ 0.4709, -0.1427, -0.1335,  ..., -0.3120, -0.1081,  0.0392]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.self_attn.linear_values.bias': tensor([ 0.0049,  0.0786,  0.0156,  ..., -0.0225, -0.0117, -0.1213],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.self_attn.linear_query.weight': tensor([[-0.1050, -0.0892, -0.0248,  ..., -0.2117,  0.2351, -0.1338],\n",
       "         [-0.0142, -0.0568, -0.2524,  ...,  0.1010, -0.2367,  0.1089],\n",
       "         [ 0.0411, -0.0777,  0.0359,  ..., -0.0947,  0.0923,  0.1312],\n",
       "         ...,\n",
       "         [-0.0166,  0.0511, -0.0418,  ...,  0.2347,  0.0373,  0.0219],\n",
       "         [-0.3071, -0.1300,  0.0977,  ..., -0.3276, -0.2328, -0.2103],\n",
       "         [-0.0814, -0.2233,  0.1093,  ..., -0.1879, -0.1793, -0.1509]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.self_attn.linear_query.bias': tensor([-0.1097, -0.2244,  0.1980,  ..., -0.0424,  0.0051, -0.0490],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.self_attn.final_linear.weight': tensor([[-0.2305, -0.0163,  0.1232,  ...,  0.3679,  0.2944, -0.0201],\n",
       "         [ 0.2825, -0.3196,  0.0299,  ...,  0.3411,  0.2332, -0.2466],\n",
       "         [ 0.1153, -0.1301, -0.1711,  ...,  0.0217, -0.2856, -0.2861],\n",
       "         ...,\n",
       "         [-0.2355,  0.1714,  0.0613,  ..., -0.0398, -0.2347,  0.0804],\n",
       "         [ 0.1771, -0.1948,  0.1324,  ...,  0.2856, -0.0511, -0.0304],\n",
       "         [ 0.0372,  0.3025, -0.4363,  ...,  0.2578, -0.0452, -0.1451]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.self_attn.final_linear.bias': tensor([-0.2571, -0.0980, -0.3325,  ..., -0.5020, -0.4917,  0.4910],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.layer_norm_1.weight': tensor([0.2859, 0.3113, 0.3257,  ..., 0.2101, 0.2292, 0.2230],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.layer_norm_1.bias': tensor([ 0.0132,  0.0039,  0.0213,  ..., -0.0128,  0.0103, -0.0153],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.context_attn.linear_keys.weight': tensor([[-0.3931,  0.0401,  0.1293,  ..., -0.1195, -0.0222,  0.0602],\n",
       "         [ 0.2490, -0.2781, -0.0809,  ..., -0.0480,  0.1019, -0.0571],\n",
       "         [-0.1403,  0.0654, -0.0033,  ...,  0.0448,  0.0354,  0.1317],\n",
       "         ...,\n",
       "         [-0.1971, -0.1921, -0.2170,  ...,  0.2629,  0.1705, -0.0448],\n",
       "         [ 0.0312, -0.1581,  0.1361,  ..., -0.0624, -0.0419,  0.0023],\n",
       "         [-0.0662,  0.0148, -0.1705,  ..., -0.0999, -0.0140, -0.0345]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.context_attn.linear_keys.bias': tensor([ 0.0095,  0.0270,  0.0019,  ...,  0.0180, -0.0116,  0.0144],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.context_attn.linear_values.weight': tensor([[ 0.0465,  0.1771, -0.3411,  ..., -0.0360, -0.1864,  0.0047],\n",
       "         [ 0.0253, -0.0935,  0.0439,  ...,  0.2808,  0.1083, -0.0020],\n",
       "         [ 0.0543, -0.3330, -0.0420,  ..., -0.2228,  0.1534,  0.0454],\n",
       "         ...,\n",
       "         [-0.1678, -0.2131, -0.0310,  ..., -0.1007,  0.1016, -0.0550],\n",
       "         [-0.1109,  0.0102,  0.1556,  ...,  0.1377, -0.0242,  0.0895],\n",
       "         [ 0.0603,  0.3479,  0.2795,  ...,  0.0861, -0.2308, -0.0095]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.context_attn.linear_values.bias': tensor([-0.0964, -0.0339,  0.0177,  ..., -0.0941, -0.0497, -0.0724],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.context_attn.linear_query.weight': tensor([[-0.1450, -0.3315, -0.1952,  ...,  0.0352,  0.0111,  0.0752],\n",
       "         [-0.0774, -0.0184, -0.0053,  ...,  0.1453,  0.0826,  0.3909],\n",
       "         [-0.2399, -0.0374, -0.0986,  ..., -0.2700,  0.3972,  0.3582],\n",
       "         ...,\n",
       "         [-0.0827, -0.0493, -0.0366,  ...,  0.0909, -0.2639,  0.0032],\n",
       "         [ 0.1049,  0.1342,  0.3726,  ..., -0.0033,  0.0166,  0.1332],\n",
       "         [-0.2563, -0.3057, -0.1438,  ..., -0.1454, -0.0888, -0.0361]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.context_attn.linear_query.bias': tensor([ 0.0404,  0.1339, -0.0085,  ...,  0.1522,  0.0819,  0.0841],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.context_attn.final_linear.weight': tensor([[ 0.0031, -0.0746,  0.2087,  ..., -0.1304, -0.2308, -0.2993],\n",
       "         [-0.0189, -0.2008, -0.2449,  ..., -0.1711, -0.0311,  0.1272],\n",
       "         [-0.2607,  0.1437, -0.4104,  ...,  0.0939, -0.0751,  0.2810],\n",
       "         ...,\n",
       "         [ 0.0663, -0.4783,  0.0988,  ..., -0.0980,  0.1279, -0.1688],\n",
       "         [-0.1671, -0.1387,  0.1354,  ..., -0.2666, -0.3020, -0.0603],\n",
       "         [ 0.0801, -0.0495,  0.4460,  ...,  0.1710, -0.0508, -0.0991]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.context_attn.final_linear.bias': tensor([ 0.1298,  0.2185,  0.3086,  ...,  0.0321, -0.1702, -0.3853],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.layer_norm_2.weight': tensor([0.1193, 0.1481, 0.1348,  ..., 0.0941, 0.1001, 0.0947],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.layer_norm_2.bias': tensor([-0.0073, -0.0116, -0.0107,  ..., -0.0408, -0.0026, -0.0193],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.feed_forward.w_1.weight': tensor([[-0.4097, -0.2700,  0.0737,  ...,  0.0936,  0.0958,  0.2720],\n",
       "         [ 0.0781, -0.1074,  0.0117,  ...,  0.2820,  0.2788, -0.1580],\n",
       "         [-0.2754, -0.2556,  0.0128,  ...,  0.2820,  0.0628,  0.0275],\n",
       "         ...,\n",
       "         [ 0.1747,  0.2019,  0.3281,  ...,  0.3142,  0.0674,  0.1089],\n",
       "         [-0.1899, -0.1687,  0.3840,  ..., -0.1477, -0.3643,  0.2465],\n",
       "         [ 0.4243,  0.2018, -0.3101,  ...,  0.2864, -0.1592,  0.0157]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.feed_forward.w_1.bias': tensor([-0.2307,  0.0841, -0.3652,  ..., -0.2961, -0.1600, -0.1461],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.feed_forward.w_2.weight': tensor([[-0.0831, -0.1288,  0.0689,  ..., -0.2583, -0.0450,  0.1154],\n",
       "         [-0.3525, -0.0558, -0.0667,  ..., -0.2041,  0.1604, -0.0671],\n",
       "         [-0.1544, -0.0462, -0.3406,  ...,  0.0112,  0.2053,  0.0386],\n",
       "         ...,\n",
       "         [ 0.2124, -0.1754, -0.0749,  ...,  0.2812,  0.3555,  0.1595],\n",
       "         [-0.0262, -0.1567,  0.1155,  ...,  0.2015, -0.0365,  0.1342],\n",
       "         [ 0.1613,  0.0448,  0.0573,  ...,  0.3638, -0.1046,  0.0436]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.feed_forward.w_2.bias': tensor([-0.1475, -0.1466,  0.2227,  ..., -0.0585,  0.1493,  0.0556],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.feed_forward.layer_norm.weight': tensor([0.5186, 0.5400, 0.5186,  ..., 0.4551, 0.4565, 0.4644],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.feed_forward.layer_norm.bias': tensor([ 0.0312,  0.1107,  0.0245,  ..., -0.1113, -0.0250,  0.0216],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.self_attn.linear_keys.weight': tensor([[-6.5857e-02,  2.0251e-01, -5.7465e-02,  ...,  4.4647e-02,\n",
       "          -1.0632e-01,  7.2449e-02],\n",
       "         [ 1.3733e-02, -1.0150e-01, -2.9434e-02,  ...,  1.1426e-01,\n",
       "           1.4832e-01,  8.0338e-03],\n",
       "         [ 1.1176e-01, -4.4739e-02, -8.9355e-02,  ..., -5.9605e-06,\n",
       "          -1.0858e-01, -2.6672e-02],\n",
       "         ...,\n",
       "         [-2.2217e-01, -1.2128e-01, -5.0537e-02,  ...,  1.6589e-01,\n",
       "          -1.8567e-01, -5.2338e-02],\n",
       "         [ 5.4565e-02,  7.6027e-03, -1.1421e-02,  ..., -1.1682e-01,\n",
       "           5.4169e-02,  2.6636e-01],\n",
       "         [-1.3696e-01, -3.6450e-01,  2.5366e-01,  ..., -3.2410e-02,\n",
       "          -1.6187e-01,  9.6558e-02]], dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.self_attn.linear_keys.bias': tensor([-0.0155,  0.0081, -0.0183,  ..., -0.0101, -0.0080,  0.0053],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.self_attn.linear_values.weight': tensor([[ 0.3101, -0.0659, -0.4985,  ..., -0.0261,  0.1904, -0.0820],\n",
       "         [ 0.1340, -0.1665, -0.0553,  ..., -0.0634,  0.3303, -0.4929],\n",
       "         [-0.5005, -0.4053,  0.0614,  ...,  0.1809, -0.4783,  0.2988],\n",
       "         ...,\n",
       "         [-0.0119, -0.3416, -0.1237,  ...,  0.0163, -0.1481, -0.0235],\n",
       "         [-0.2915,  0.0356, -0.1187,  ..., -0.1050, -0.2200,  0.3716],\n",
       "         [-0.2551,  0.0391,  0.1863,  ..., -0.0909, -0.0381,  0.0756]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.self_attn.linear_values.bias': tensor([-0.0251, -0.0740,  0.1063,  ..., -0.0436,  0.0135,  0.0956],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.self_attn.linear_query.weight': tensor([[-0.0420, -0.3079, -0.0490,  ...,  0.1382,  0.0266,  0.1229],\n",
       "         [-0.0640,  0.0458,  0.1144,  ...,  0.1064, -0.1210, -0.0217],\n",
       "         [ 0.1219, -0.0167,  0.0353,  ..., -0.0523, -0.1027,  0.1458],\n",
       "         ...,\n",
       "         [ 0.3435, -0.0014,  0.2062,  ...,  0.0873, -0.1273, -0.0268],\n",
       "         [-0.1941,  0.1359, -0.2434,  ...,  0.2717,  0.1084,  0.2108],\n",
       "         [-0.1007,  0.1511, -0.1353,  ...,  0.1998, -0.2651, -0.3638]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.self_attn.linear_query.bias': tensor([-0.0583, -0.0863,  0.0507,  ..., -0.0739,  0.1298,  0.1917],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.self_attn.final_linear.weight': tensor([[ 0.2006, -0.1216,  0.1897,  ...,  0.0042,  0.0221, -0.0220],\n",
       "         [-0.2788,  0.2749,  0.3147,  ..., -0.3538, -0.1953,  0.1487],\n",
       "         [-0.1451,  0.3015, -0.4993,  ..., -0.1499,  0.0264, -0.0385],\n",
       "         ...,\n",
       "         [-0.3020,  0.2366, -0.0396,  ..., -0.0085, -0.3369, -0.0049],\n",
       "         [ 0.2009, -0.1084, -0.2537,  ..., -0.3242, -0.2581,  0.1196],\n",
       "         [ 0.4978, -0.2510,  0.3047,  ..., -0.0637,  0.0760, -0.1024]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.self_attn.final_linear.bias': tensor([-0.2795,  0.4678, -0.2693,  ..., -0.4160, -0.0284,  0.2915],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.layer_norm_1.weight': tensor([0.2969, 0.3320, 0.3101,  ..., 0.2297, 0.2537, 0.2499],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.layer_norm_1.bias': tensor([ 0.0145,  0.0063,  0.0268,  ..., -0.0158,  0.0065, -0.0284],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.context_attn.linear_keys.weight': tensor([[ 0.0375, -0.0067,  0.0670,  ..., -0.3262, -0.0666,  0.0624],\n",
       "         [-0.0308,  0.0286, -0.1774,  ..., -0.1131, -0.0634,  0.1097],\n",
       "         [-0.0727,  0.0724, -0.2861,  ...,  0.2018,  0.0775, -0.0070],\n",
       "         ...,\n",
       "         [-0.1735, -0.0476, -0.1813,  ..., -0.2632, -0.2131, -0.0457],\n",
       "         [ 0.2214, -0.1711,  0.0067,  ..., -0.3740,  0.2871, -0.0119],\n",
       "         [-0.2520, -0.1650,  0.0536,  ..., -0.2666, -0.4377,  0.1949]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.context_attn.linear_keys.bias': tensor([ 0.0144, -0.0133,  0.0258,  ...,  0.0226, -0.0057, -0.0240],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.context_attn.linear_values.weight': tensor([[ 0.1630, -0.0317,  0.1660,  ..., -0.4785, -0.2400, -0.0245],\n",
       "         [ 0.0836, -0.0155,  0.2700,  ...,  0.0586, -0.1124,  0.0049],\n",
       "         [-0.0585,  0.0667, -0.0399,  ..., -0.2175, -0.0422, -0.0768],\n",
       "         ...,\n",
       "         [ 0.2810,  0.2720, -0.0113,  ..., -0.2407, -0.2625,  0.0165],\n",
       "         [ 0.1631,  0.1704,  0.0071,  ..., -0.2157, -0.0864,  0.0110],\n",
       "         [ 0.0294,  0.1296,  0.1190,  ...,  0.1326, -0.0536,  0.0583]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.context_attn.linear_values.bias': tensor([ 0.0158, -0.1880,  0.0825,  ..., -0.0160, -0.0419, -0.0264],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.context_attn.linear_query.weight': tensor([[ 0.0022,  0.1277, -0.0131,  ...,  0.0833, -0.0323, -0.1841],\n",
       "         [ 0.1743,  0.0327, -0.2566,  ..., -0.0372, -0.0149,  0.1061],\n",
       "         [ 0.2786,  0.0632,  0.1578,  ...,  0.1000,  0.0400,  0.3135],\n",
       "         ...,\n",
       "         [-0.1490, -0.1283, -0.1852,  ...,  0.0140, -0.1982,  0.0712],\n",
       "         [-0.0038,  0.1048, -0.1748,  ...,  0.1429,  0.2014, -0.0426],\n",
       "         [ 0.2123,  0.0413, -0.0383,  ...,  0.0410, -0.1842,  0.3259]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.context_attn.linear_query.bias': tensor([-0.0406, -0.0256,  0.0312,  ..., -0.0875,  0.0524,  0.0250],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.context_attn.final_linear.weight': tensor([[ 0.0500, -0.2297, -0.2686,  ...,  0.1371, -0.0784,  0.0501],\n",
       "         [ 0.3943,  0.1156,  0.1155,  ...,  0.1086, -0.0457,  0.1461],\n",
       "         [-0.0682, -0.1912, -0.1614,  ...,  0.2607,  0.1146, -0.2332],\n",
       "         ...,\n",
       "         [-0.2773, -0.2771,  0.3865,  ..., -0.0358,  0.0470, -0.2032],\n",
       "         [-0.0219,  0.0157, -0.1818,  ..., -0.1792, -0.1313,  0.1989],\n",
       "         [ 0.0125,  0.2400, -0.2211,  ..., -0.0269, -0.1218, -0.4712]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.context_attn.final_linear.bias': tensor([ 0.1956,  0.0657,  0.3936,  ...,  0.0127, -0.3669, -0.3601],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.layer_norm_2.weight': tensor([0.1075, 0.1220, 0.1219,  ..., 0.0916, 0.0882, 0.0904],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.layer_norm_2.bias': tensor([-0.0060, -0.0189, -0.0099,  ..., -0.0571, -0.0138, -0.0278],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.feed_forward.w_1.weight': tensor([[ 0.0246, -0.1316,  0.0693,  ...,  0.0710,  0.0610, -0.1390],\n",
       "         [-0.0105,  0.1313, -0.1672,  ...,  0.0849,  0.3064, -0.0118],\n",
       "         [-0.0682,  0.0808,  0.1024,  ..., -0.0484,  0.1665,  0.0039],\n",
       "         ...,\n",
       "         [ 0.3459,  0.1885,  0.3340,  ...,  0.1039, -0.0782, -0.0128],\n",
       "         [ 0.0859, -0.0403,  0.0482,  ..., -0.0431,  0.1514,  0.0355],\n",
       "         [ 0.1876,  0.1115, -0.5215,  ..., -0.3174, -0.3076,  0.0912]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.feed_forward.w_1.bias': tensor([-0.1182, -0.0240, -0.1111,  ..., -0.0903,  0.0151, -0.2751],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.feed_forward.w_2.weight': tensor([[-0.0988, -0.2712,  0.0759,  ..., -0.0606,  0.0951,  0.0548],\n",
       "         [-0.0896,  0.0380, -0.1500,  ...,  0.1359,  0.1032, -0.2303],\n",
       "         [-0.2039,  0.2030, -0.0993,  ...,  0.0145, -0.0025,  0.0596],\n",
       "         ...,\n",
       "         [-0.2715,  0.0426,  0.1118,  ..., -0.2710,  0.0401,  0.3145],\n",
       "         [ 0.3484, -0.2305, -0.2507,  ...,  0.0446, -0.0930,  0.0582],\n",
       "         [ 0.0522,  0.2581,  0.1652,  ...,  0.1074, -0.0864,  0.0522]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.feed_forward.w_2.bias': tensor([-0.0361,  0.0535,  0.2190,  ..., -0.0262,  0.0795, -0.0141],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.feed_forward.layer_norm.weight': tensor([0.5527, 0.5947, 0.5669,  ..., 0.5171, 0.5190, 0.5498],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.feed_forward.layer_norm.bias': tensor([-0.0238,  0.1709,  0.0458,  ..., -0.1510, -0.0305, -0.0568],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.self_attn.linear_keys.weight': tensor([[ 0.0374,  0.2539,  0.0807,  ..., -0.2534, -0.1730,  0.1367],\n",
       "         [ 0.1406,  0.1671, -0.1053,  ..., -0.0083, -0.1061,  0.0356],\n",
       "         [ 0.1313,  0.2104,  0.0596,  ..., -0.2021,  0.1768,  0.1305],\n",
       "         ...,\n",
       "         [-0.0445,  0.3679,  0.3291,  ..., -0.1658,  0.0309,  0.0033],\n",
       "         [ 0.1257,  0.0351,  0.0739,  ..., -0.0812, -0.0751,  0.0815],\n",
       "         [-0.3774, -0.2363, -0.0306,  ...,  0.2869, -0.1887,  0.2236]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.self_attn.linear_keys.bias': tensor([-0.0245,  0.0265,  0.0033,  ...,  0.0219, -0.0003, -0.0004],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.self_attn.linear_values.weight': tensor([[ 0.1777,  0.1213, -0.0913,  ..., -0.0241, -0.2084,  0.0174],\n",
       "         [ 0.5625,  0.0407,  0.4060,  ...,  0.0525, -0.2460, -0.0887],\n",
       "         [-0.0046, -0.1355,  0.1956,  ..., -0.2605,  0.1372, -0.1455],\n",
       "         ...,\n",
       "         [ 0.3633, -0.1598,  0.4990,  ...,  0.0370, -0.4541,  0.3074],\n",
       "         [-0.6509,  0.4268,  0.5879,  ...,  0.2170, -0.2910,  0.5000],\n",
       "         [ 0.0174, -0.2291,  0.1272,  ...,  0.3635,  0.0128, -0.2561]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.self_attn.linear_values.bias': tensor([-0.0603,  0.0184,  0.1882,  ..., -0.0049,  0.0327, -0.1523],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.self_attn.linear_query.weight': tensor([[-0.0482, -0.3528, -0.1902,  ...,  0.0988,  0.0460,  0.0452],\n",
       "         [-0.0535, -0.1464,  0.1217,  ..., -0.1593, -0.1014,  0.0892],\n",
       "         [-0.0216, -0.0822,  0.0185,  ..., -0.0079,  0.3145,  0.1541],\n",
       "         ...,\n",
       "         [-0.1473,  0.0253, -0.0071,  ..., -0.0023, -0.0278, -0.0009],\n",
       "         [ 0.3037, -0.0680,  0.0512,  ..., -0.0131, -0.1829, -0.1847],\n",
       "         [-0.0150, -0.1372, -0.0367,  ...,  0.0043, -0.1564,  0.1874]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.self_attn.linear_query.bias': tensor([-0.1130, -0.0684, -0.0753,  ..., -0.3828, -0.1595,  0.0504],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.self_attn.final_linear.weight': tensor([[ 0.1996,  0.2776,  0.0723,  ...,  0.6182, -0.4133, -0.1593],\n",
       "         [-0.1759,  0.1315,  0.0181,  ...,  0.3464,  0.4951, -0.2612],\n",
       "         [-0.2367,  0.1771,  0.1803,  ..., -0.5269, -0.2527, -0.2202],\n",
       "         ...,\n",
       "         [ 0.0767, -0.0532, -0.1245,  ...,  0.1116,  0.5005, -0.0312],\n",
       "         [-0.0836,  0.0947,  0.0076,  ...,  0.2340, -0.0242, -0.0476],\n",
       "         [ 0.0997,  0.3879, -0.1774,  ...,  0.1447,  0.4998,  0.0815]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.self_attn.final_linear.bias': tensor([-0.2966,  0.2942, -0.3508,  ..., -0.3872,  0.0909,  0.3867],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.layer_norm_1.weight': tensor([0.3369, 0.3384, 0.3252,  ..., 0.2742, 0.3064, 0.2991],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.layer_norm_1.bias': tensor([ 0.0210,  0.0098,  0.0302,  ..., -0.0051,  0.0017, -0.0262],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.context_attn.linear_keys.weight': tensor([[ 0.3401,  0.1907,  0.0227,  ...,  0.1218, -0.2205, -0.1613],\n",
       "         [-0.1348,  0.0866,  0.0611,  ...,  0.0736,  0.0831, -0.0015],\n",
       "         [-0.2825,  0.1018,  0.0981,  ..., -0.0231,  0.2522, -0.1433],\n",
       "         ...,\n",
       "         [-0.0963,  0.3123, -0.3767,  ...,  0.2382, -0.0230,  0.0461],\n",
       "         [ 0.3325, -0.4280, -0.1127,  ..., -0.1572,  0.1757,  0.0476],\n",
       "         [ 0.1704,  0.1230, -0.1737,  ..., -0.1663, -0.2683, -0.0545]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.context_attn.linear_keys.bias': tensor([-0.0267, -0.0164,  0.0116,  ...,  0.0257, -0.0025,  0.0022],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.context_attn.linear_values.weight': tensor([[ 1.6235e-01, -8.6121e-02, -4.5074e-02,  ...,  1.9666e-01,\n",
       "          -3.8672e-01,  8.2092e-03],\n",
       "         [-2.8613e-01,  3.4271e-02, -4.3677e-01,  ..., -5.0964e-02,\n",
       "          -6.2012e-02,  7.8857e-02],\n",
       "         [-2.7466e-01, -2.5659e-01,  4.3921e-01,  ..., -4.0820e-01,\n",
       "          -3.2642e-01,  3.2349e-02],\n",
       "         ...,\n",
       "         [-4.6753e-02,  2.1582e-01,  1.4502e-01,  ...,  6.9336e-02,\n",
       "           6.7322e-02, -1.3863e-02],\n",
       "         [-7.0251e-02, -2.0645e-02, -1.1169e-01,  ...,  1.6626e-01,\n",
       "           8.9355e-02,  2.6276e-02],\n",
       "         [ 5.1416e-01, -5.3525e-05,  3.3600e-02,  ...,  2.0093e-01,\n",
       "           2.9614e-01,  3.6316e-02]], dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.context_attn.linear_values.bias': tensor([-0.0952, -0.1188, -0.1442,  ..., -0.0037,  0.0580,  0.0503],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.context_attn.linear_query.weight': tensor([[ 0.2246, -0.0369, -0.0668,  ..., -0.2939, -0.1271,  0.0820],\n",
       "         [-0.2778, -0.0588,  0.1204,  ..., -0.2460,  0.1610, -0.0326],\n",
       "         [ 0.1885,  0.0735,  0.1874,  ..., -0.1624,  0.1464, -0.0825],\n",
       "         ...,\n",
       "         [-0.1677,  0.0193, -0.3943,  ...,  0.0600, -0.1323, -0.0068],\n",
       "         [-0.2080, -0.0361,  0.1622,  ..., -0.1429,  0.1573, -0.1770],\n",
       "         [-0.0502,  0.0901,  0.1974,  ...,  0.0373,  0.0597,  0.1471]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.context_attn.linear_query.bias': tensor([-0.0816, -0.0087, -0.1379,  ...,  0.0420,  0.0195, -0.1461],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.context_attn.final_linear.weight': tensor([[-0.0215, -0.2637, -0.1755,  ..., -0.4727, -0.0643,  0.0902],\n",
       "         [ 0.0960,  0.2106,  0.0665,  ...,  0.1514, -0.2832, -0.0386],\n",
       "         [ 0.2603, -0.0852,  0.4165,  ..., -0.0095, -0.2157, -0.1486],\n",
       "         ...,\n",
       "         [-0.0221,  0.1790, -0.2588,  ..., -0.1587, -0.1099, -0.0893],\n",
       "         [-0.1489,  0.0752,  0.2411,  ...,  0.0696, -0.1183, -0.0610],\n",
       "         [-0.1570, -0.2546, -0.0327,  ...,  0.0521, -0.1205,  0.4062]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.context_attn.final_linear.bias': tensor([ 0.4001,  0.2559,  0.4946,  ...,  0.2712, -0.3901, -0.4368],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.layer_norm_2.weight': tensor([0.1088, 0.1266, 0.1223,  ..., 0.0982, 0.0980, 0.0988],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.layer_norm_2.bias': tensor([-0.0053, -0.0124, -0.0187,  ..., -0.0555, -0.0078, -0.0295],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.feed_forward.w_1.weight': tensor([[-0.0851,  0.0754,  0.1345,  ...,  0.3159,  0.1464,  0.0422],\n",
       "         [ 0.2727,  0.1659, -0.0242,  ..., -0.1598, -0.0930,  0.0657],\n",
       "         [ 0.1584, -0.0405, -0.1112,  ..., -0.0997, -0.1469, -0.2573],\n",
       "         ...,\n",
       "         [ 0.2394, -0.2903, -0.0901,  ...,  0.3789,  0.2290, -0.3430],\n",
       "         [ 0.0126,  0.0057, -0.0182,  ...,  0.0438, -0.3665, -0.0600],\n",
       "         [-0.3196,  0.0820,  0.0243,  ...,  0.1395, -0.0446, -0.4631]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.feed_forward.w_1.bias': tensor([-0.1229, -0.1085, -0.0858,  ..., -0.3970, -0.1575,  0.0030],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.feed_forward.w_2.weight': tensor([[-0.1260, -0.0411, -0.0211,  ..., -0.1797,  0.1743,  0.2720],\n",
       "         [-0.2678,  0.2886,  0.2220,  ...,  0.0752,  0.0397,  0.1187],\n",
       "         [-0.2213, -0.2236, -0.0878,  ..., -0.0323, -0.0224, -0.0335],\n",
       "         ...,\n",
       "         [-0.1696,  0.0558,  0.2617,  ..., -0.2012, -0.1853, -0.0159],\n",
       "         [-0.1678, -0.0100, -0.1174,  ..., -0.1779,  0.0478,  0.1150],\n",
       "         [ 0.1125, -0.0822,  0.0068,  ..., -0.2668, -0.2499,  0.3943]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.feed_forward.w_2.bias': tensor([ 0.0576, -0.2017,  0.3735,  ..., -0.0267,  0.5044,  0.0363],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.feed_forward.layer_norm.weight': tensor([0.6333, 0.6772, 0.6577,  ..., 0.6431, 0.6191, 0.6387],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.feed_forward.layer_norm.bias': tensor([-0.0312,  0.2100,  0.0613,  ..., -0.2198, -0.0563,  0.0056],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.self_attn.linear_keys.weight': tensor([[-0.2045,  0.0935, -0.1895,  ..., -0.0121, -0.1221, -0.0599],\n",
       "         [-0.0090, -0.3213, -0.0044,  ..., -0.0103, -0.0870,  0.2140],\n",
       "         [-0.0508, -0.1552, -0.1685,  ..., -0.0257, -0.1137, -0.3550],\n",
       "         ...,\n",
       "         [ 0.0738,  0.0740, -0.0399,  ...,  0.0630,  0.0160, -0.0910],\n",
       "         [ 0.0360, -0.0048, -0.0846,  ..., -0.0117,  0.0858, -0.0630],\n",
       "         [-0.0284,  0.0037, -0.2744,  ..., -0.1581, -0.0492, -0.3699]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.self_attn.linear_keys.bias': tensor([ 0.0037,  0.0206,  0.0255,  ..., -0.0012, -0.0041, -0.0044],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.self_attn.linear_values.weight': tensor([[-0.0089,  0.4580,  0.1262,  ...,  0.0078,  0.3901, -0.1656],\n",
       "         [ 0.1880, -0.0881,  0.2271,  ..., -0.1149, -0.0731, -0.2908],\n",
       "         [-0.1429, -0.2030, -0.2288,  ..., -0.1512,  0.1527, -0.1395],\n",
       "         ...,\n",
       "         [ 0.1218,  0.2458, -0.0825,  ...,  0.2137,  0.2061, -0.0891],\n",
       "         [ 0.1573,  0.2622, -0.0113,  ...,  0.0221, -0.0740,  0.0135],\n",
       "         [-0.1108, -0.0303,  0.2396,  ..., -0.0332, -0.0999,  0.1989]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.self_attn.linear_values.bias': tensor([-0.0156,  0.0625,  0.0502,  ..., -0.3411,  0.2294,  0.1083],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.self_attn.linear_query.weight': tensor([[-0.1903, -0.0818,  0.0698,  ..., -0.1096, -0.1135, -0.2006],\n",
       "         [-0.1033,  0.3027,  0.1921,  ...,  0.1516, -0.0990,  0.0273],\n",
       "         [ 0.1155,  0.1646, -0.0541,  ..., -0.2522,  0.0373, -0.0517],\n",
       "         ...,\n",
       "         [-0.1730, -0.1442,  0.3608,  ...,  0.0540, -0.0877,  0.0861],\n",
       "         [ 0.1680,  0.1840, -0.1689,  ...,  0.0338,  0.0041,  0.0251],\n",
       "         [ 0.0446,  0.1721, -0.0281,  ...,  0.0358,  0.1898,  0.2507]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.self_attn.linear_query.bias': tensor([ 0.0382, -0.0410, -0.0156,  ..., -0.2959, -0.1064, -0.0396],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.self_attn.final_linear.weight': tensor([[-0.1278, -0.2954, -0.0012,  ...,  0.0865, -0.0617,  0.0590],\n",
       "         [-0.0119,  0.1688,  0.1304,  ...,  0.3594,  0.0177, -0.3987],\n",
       "         [-0.4302,  0.1376,  0.2698,  ...,  0.2532,  0.1486,  0.0579],\n",
       "         ...,\n",
       "         [ 0.1769,  0.1301,  0.2328,  ...,  0.0317, -0.1260, -0.1396],\n",
       "         [ 0.1765,  0.1703,  0.3884,  ...,  0.1753, -0.0198,  0.1588],\n",
       "         [-0.3267, -0.1447, -0.5879,  ..., -0.1171,  0.1121,  0.1244]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.self_attn.final_linear.bias': tensor([-0.1833,  0.3335, -0.2705,  ..., -0.1660, -0.0508,  0.5049],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.layer_norm_1.weight': tensor([0.3381, 0.3489, 0.3391,  ..., 0.2952, 0.3115, 0.3103],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.layer_norm_1.bias': tensor([ 0.0134,  0.0132,  0.0375,  ..., -0.0148,  0.0051, -0.0371],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.context_attn.linear_keys.weight': tensor([[-0.3711, -0.0614,  0.0996,  ..., -0.1182,  0.0873, -0.0322],\n",
       "         [-0.2235,  0.3223,  0.1847,  ..., -0.1969, -0.2405,  0.1166],\n",
       "         [-0.2136,  0.0717, -0.1069,  ...,  0.2681,  0.1412,  0.0034],\n",
       "         ...,\n",
       "         [-0.2046, -0.1971, -0.0922,  ..., -0.1748,  0.0704, -0.0338],\n",
       "         [-0.2839,  0.0338, -0.0923,  ..., -0.1671, -0.1193, -0.1566],\n",
       "         [ 0.1528,  0.1070,  0.4490,  ..., -0.1133,  0.0535, -0.0446]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.context_attn.linear_keys.bias': tensor([-0.0200, -0.0157, -0.0066,  ..., -0.0196,  0.0060, -0.0199],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.context_attn.linear_values.weight': tensor([[ 0.1058, -0.1161, -0.4517,  ..., -0.2382,  0.5146,  0.0323],\n",
       "         [-0.3267, -0.0266, -0.2542,  ...,  0.1765,  0.0455,  0.0431],\n",
       "         [-0.0636, -0.2844, -0.0081,  ...,  0.1559,  0.4092, -0.1288],\n",
       "         ...,\n",
       "         [ 0.1447,  0.1050,  0.2108,  ..., -0.6055,  0.3354, -0.0129],\n",
       "         [ 0.3230,  0.1869,  0.1417,  ...,  0.0695, -0.3398, -0.0172],\n",
       "         [ 0.1238, -0.2754,  0.4731,  ..., -0.1932, -0.1099, -0.0350]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.context_attn.linear_values.bias': tensor([-0.1930, -0.0421,  0.1481,  ..., -0.1722, -0.0246,  0.0468],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.context_attn.linear_query.weight': tensor([[-0.4705, -0.0850, -0.0084,  ..., -0.3408,  0.0473,  0.0865],\n",
       "         [ 0.1898,  0.0959,  0.1753,  ...,  0.1469, -0.1388,  0.1073],\n",
       "         [ 0.1635,  0.0573, -0.0794,  ..., -0.0566, -0.0975, -0.1176],\n",
       "         ...,\n",
       "         [ 0.0769, -0.1356, -0.0036,  ..., -0.0539,  0.0759, -0.1608],\n",
       "         [ 0.1654, -0.2303,  0.0085,  ..., -0.2002, -0.1268, -0.1071],\n",
       "         [-0.1133, -0.1595, -0.2695,  ..., -0.3528,  0.2029,  0.0522]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.context_attn.linear_query.bias': tensor([-0.0659,  0.3047,  0.0176,  ..., -0.0298, -0.1838,  0.0766],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.context_attn.final_linear.weight': tensor([[ 0.1005, -0.2866,  0.0150,  ...,  0.0687,  0.0201,  0.1119],\n",
       "         [-0.3127,  0.0046, -0.4910,  ...,  0.1282, -0.5044, -0.1580],\n",
       "         [-0.0880,  0.1805, -0.3584,  ...,  0.2240, -0.0374, -0.4968],\n",
       "         ...,\n",
       "         [-0.4656,  0.1459, -0.1335,  ..., -0.4026, -0.0428, -0.1377],\n",
       "         [ 0.1560,  0.2075,  0.0927,  ..., -0.0337,  0.0627,  0.0725],\n",
       "         [-0.0769,  0.4978,  0.0244,  ...,  0.6108, -0.0981,  0.1642]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.context_attn.final_linear.bias': tensor([ 0.1737,  0.0965,  0.4023,  ..., -0.2581, -0.4963, -0.3105],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.layer_norm_2.weight': tensor([0.1185, 0.1268, 0.1243,  ..., 0.0994, 0.1035, 0.1086],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.layer_norm_2.bias': tensor([-0.0175, -0.0222, -0.0108,  ..., -0.0543, -0.0084, -0.0273],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.feed_forward.w_1.weight': tensor([[ 0.0210, -0.0683,  0.2157,  ...,  0.0250,  0.3140,  0.4192],\n",
       "         [ 0.2238,  0.2529,  0.1068,  ...,  0.2678,  0.0637,  0.1896],\n",
       "         [ 0.4622,  0.0760,  0.2435,  ...,  0.1342,  0.2158, -0.1066],\n",
       "         ...,\n",
       "         [-0.0657,  0.0519, -0.1526,  ...,  0.2031,  0.1222, -0.0950],\n",
       "         [ 0.3167,  0.2002,  0.4006,  ...,  0.2532,  0.0899,  0.1620],\n",
       "         [ 0.0201, -0.0225, -0.0485,  ...,  0.0050, -0.2136, -0.0174]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.feed_forward.w_1.bias': tensor([-0.1454,  0.0207, -0.2444,  ..., -0.2520, -0.2686, -0.0136],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.feed_forward.w_2.weight': tensor([[ 0.0240,  0.0366, -0.3574,  ..., -0.0224, -0.1803, -0.0250],\n",
       "         [-0.1412, -0.0702, -0.0832,  ...,  0.1190,  0.1544, -0.0342],\n",
       "         [ 0.1305, -0.0353, -0.0725,  ..., -0.1155,  0.4028, -0.3269],\n",
       "         ...,\n",
       "         [-0.0705, -0.2150,  0.0446,  ...,  0.0066,  0.0831,  0.1426],\n",
       "         [-0.1654,  0.1814, -0.2573,  ..., -0.2487,  0.2815, -0.0829],\n",
       "         [-0.1708,  0.1843,  0.0834,  ...,  0.1592,  0.1464, -0.4170]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.feed_forward.w_2.bias': tensor([ 0.3928, -0.1920,  0.4954,  ..., -0.3877,  0.5059,  0.1106],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.feed_forward.layer_norm.weight': tensor([0.7974, 0.8071, 0.7432,  ..., 0.8247, 0.7603, 0.7881],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.feed_forward.layer_norm.bias': tensor([-0.1459,  0.1334,  0.0034,  ..., -0.1862, -0.0635, -0.1029],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.self_attn.linear_keys.weight': tensor([[-0.1423,  0.0493, -0.0364,  ..., -0.2412, -0.0038, -0.1906],\n",
       "         [-0.1034,  0.0400, -0.0563,  ...,  0.0095,  0.1847,  0.1011],\n",
       "         [ 0.0593, -0.0185, -0.1671,  ...,  0.0114, -0.2832, -0.0166],\n",
       "         ...,\n",
       "         [-0.0690, -0.1224,  0.2441,  ..., -0.0893,  0.0546,  0.2991],\n",
       "         [-0.0170, -0.0854,  0.1444,  ..., -0.0768,  0.1268,  0.1306],\n",
       "         [ 0.1249,  0.1434,  0.1725,  ..., -0.2191, -0.0253, -0.0129]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.self_attn.linear_keys.bias': tensor([-0.0060,  0.0152,  0.0233,  ..., -0.0166,  0.0221, -0.0134],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.self_attn.linear_values.weight': tensor([[ 0.1410,  0.1727, -0.0191,  ..., -0.1436,  0.0425,  0.3655],\n",
       "         [ 0.2812, -0.0403,  0.1802,  ..., -0.1816, -0.2754, -0.1415],\n",
       "         [ 0.1836,  0.2639, -0.3894,  ..., -0.3167,  0.1504, -0.4910],\n",
       "         ...,\n",
       "         [ 0.2883, -0.0306, -0.3403,  ...,  0.0630, -0.2426, -0.0954],\n",
       "         [-0.0223,  0.1888, -0.0462,  ..., -0.0820,  0.1877, -0.0966],\n",
       "         [-0.1565, -0.0103,  0.1691,  ..., -0.1262, -0.2935,  0.1974]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.self_attn.linear_values.bias': tensor([ 0.0117,  0.1648,  0.0165,  ..., -0.2168,  0.2529, -0.0457],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.self_attn.linear_query.weight': tensor([[ 0.0427, -0.1158, -0.1973,  ..., -0.0760,  0.0096,  0.0612],\n",
       "         [ 0.0319, -0.3293,  0.0804,  ...,  0.0157,  0.0489,  0.0158],\n",
       "         [ 0.1538,  0.0602,  0.1025,  ...,  0.0074, -0.0712, -0.3633],\n",
       "         ...,\n",
       "         [-0.0150, -0.0189,  0.0870,  ...,  0.0556,  0.0825,  0.0517],\n",
       "         [ 0.0912, -0.2279, -0.0597,  ...,  0.1247, -0.0776, -0.0311],\n",
       "         [ 0.0519,  0.0293, -0.0393,  ...,  0.1355,  0.0712, -0.2561]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.self_attn.linear_query.bias': tensor([-0.1656, -0.0432, -0.0004,  ...,  0.0256, -0.1768, -0.0460],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.self_attn.final_linear.weight': tensor([[ 0.4148,  0.2620,  0.1461,  ..., -0.2103, -0.0971, -0.0137],\n",
       "         [ 0.0315,  0.1788, -0.1793,  ...,  0.2517, -0.2411, -0.2437],\n",
       "         [ 0.2203, -0.0828, -0.3955,  ...,  0.0433, -0.3066, -0.2430],\n",
       "         ...,\n",
       "         [ 0.2292,  0.0508, -0.0446,  ..., -0.0126,  0.1786, -0.0538],\n",
       "         [-0.1774, -0.1196, -0.0755,  ...,  0.2372, -0.1290,  0.0710],\n",
       "         [-0.3435,  0.0744,  0.2435,  ...,  0.1345, -0.1583, -0.0632]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.self_attn.final_linear.bias': tensor([ 0.1243,  0.2539, -0.2976,  ...,  0.2218,  0.1137,  0.5005],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.layer_norm_1.weight': tensor([0.4102, 0.3804, 0.3806,  ..., 0.3447, 0.3757, 0.3708],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.layer_norm_1.bias': tensor([ 0.0182,  0.0095,  0.0290,  ..., -0.0136,  0.0106, -0.0280],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.context_attn.linear_keys.weight': tensor([[-0.1570, -0.1323,  0.1772,  ...,  0.0469, -0.0352,  0.0019],\n",
       "         [ 0.1349, -0.2822,  0.0867,  ...,  0.0389,  0.1478, -0.0174],\n",
       "         [ 0.3291,  0.3396, -0.0086,  ...,  0.1729, -0.1600, -0.0617],\n",
       "         ...,\n",
       "         [ 0.0868, -0.1453,  0.1772,  ...,  0.0400,  0.0132,  0.0280],\n",
       "         [ 0.2019, -0.1984, -0.2167,  ..., -0.0485,  0.0724,  0.0322],\n",
       "         [-0.0590, -0.1819,  0.2715,  ..., -0.0208,  0.1816,  0.0951]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.context_attn.linear_keys.bias': tensor([-0.0289,  0.0023, -0.0247,  ..., -0.0080, -0.0143, -0.0205],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.context_attn.linear_values.weight': tensor([[ 0.1135,  0.0676,  0.0016,  ...,  0.1982,  0.0425, -0.0825],\n",
       "         [ 0.2925, -0.0800, -0.0367,  ...,  0.3589,  0.2947,  0.0084],\n",
       "         [ 0.2712, -0.0820,  0.3889,  ..., -0.0808, -0.3374, -0.0158],\n",
       "         ...,\n",
       "         [ 0.2910,  0.1021, -0.0634,  ..., -0.1815,  0.0561,  0.0302],\n",
       "         [-0.0091,  0.0112,  0.0781,  ...,  0.1381, -0.3035, -0.0421],\n",
       "         [ 0.1682, -0.0823, -0.3062,  ..., -0.4304,  0.1691, -0.1249]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.context_attn.linear_values.bias': tensor([ 0.0173, -0.0172,  0.1008,  ..., -0.0360,  0.2008,  0.3875],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.context_attn.linear_query.weight': tensor([[-0.1891, -0.1931, -0.1881,  ..., -0.2788, -0.0663, -0.0147],\n",
       "         [ 0.1151, -0.2732, -0.0383,  ..., -0.3081, -0.1140,  0.0378],\n",
       "         [ 0.0862,  0.1174,  0.1058,  ...,  0.0028,  0.0516,  0.1337],\n",
       "         ...,\n",
       "         [ 0.1978, -0.2303,  0.2014,  ...,  0.0764, -0.0551, -0.0450],\n",
       "         [-0.0210,  0.0884,  0.1086,  ...,  0.0174, -0.0796,  0.1521],\n",
       "         [ 0.0110, -0.0436, -0.0723,  ..., -0.0287, -0.0842, -0.0739]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.context_attn.linear_query.bias': tensor([ 0.0989,  0.0967, -0.0564,  ..., -0.0964, -0.0172,  0.1020],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.context_attn.final_linear.weight': tensor([[-0.1542,  0.2229, -0.0826,  ..., -0.1195, -0.4167,  0.3042],\n",
       "         [ 0.1630, -0.0679,  0.4854,  ..., -0.0095,  0.1709,  0.2983],\n",
       "         [ 0.0172,  0.0249, -0.0252,  ..., -0.3215,  0.0625,  0.4380],\n",
       "         ...,\n",
       "         [-0.0400, -0.0425,  0.1798,  ...,  0.2079,  0.4265, -0.1339],\n",
       "         [-0.0088,  0.0418, -0.2290,  ..., -0.1038, -0.1912,  0.1637],\n",
       "         [-0.0147,  0.0756,  0.0246,  ..., -0.1266, -0.1960,  0.1256]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.context_attn.final_linear.bias': tensor([-0.3616,  0.0895, -0.0173,  ...,  0.2443, -0.3513, -0.4993],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.layer_norm_2.weight': tensor([0.1331, 0.1266, 0.1376,  ..., 0.1132, 0.1064, 0.1123],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.layer_norm_2.bias': tensor([-0.0070, -0.0088, -0.0046,  ..., -0.0548, -0.0182, -0.0306],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.feed_forward.w_1.weight': tensor([[-0.4417, -0.2146,  0.1429,  ..., -0.1768, -0.0359,  0.2349],\n",
       "         [-0.0397, -0.4207,  0.0406,  ...,  0.1848, -0.0008, -0.0076],\n",
       "         [ 0.1375, -0.1194,  0.0920,  ..., -0.1770,  0.4021, -0.0606],\n",
       "         ...,\n",
       "         [-0.0806, -0.1650,  0.2084,  ...,  0.1735,  0.2954, -0.1643],\n",
       "         [ 0.5210, -0.0171,  0.2417,  ...,  0.1959, -0.0807, -0.0129],\n",
       "         [-0.1121,  0.0619, -0.0764,  ..., -0.1229, -0.0679, -0.0245]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.feed_forward.w_1.bias': tensor([-0.1028, -0.3286, -0.2461,  ..., -0.3433, -0.4426, -0.0970],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.feed_forward.w_2.weight': tensor([[ 0.0093,  0.0911, -0.0565,  ..., -0.0317, -0.2344, -0.0305],\n",
       "         [-0.1876,  0.0094,  0.1968,  ..., -0.0558, -0.0459,  0.0405],\n",
       "         [ 0.0945, -0.0292,  0.0138,  ..., -0.2668,  0.0869,  0.0090],\n",
       "         ...,\n",
       "         [-0.2488,  0.2688,  0.2932,  ...,  0.2343, -0.2141, -0.0137],\n",
       "         [ 0.0183,  0.1749, -0.2096,  ...,  0.0723,  0.1492,  0.2289],\n",
       "         [ 0.0099, -0.0686,  0.1202,  ...,  0.0895,  0.4971,  0.1537]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.feed_forward.w_2.bias': tensor([ 0.5020, -0.3313,  0.4246,  ..., -0.5049,  0.5127, -0.2952],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.feed_forward.layer_norm.weight': tensor([1.0195, 1.0068, 0.9004,  ..., 1.0664, 0.9517, 1.0547],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.feed_forward.layer_norm.bias': tensor([-0.0638,  0.1104, -0.0249,  ..., -0.1152, -0.0183, -0.0844],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.self_attn.linear_keys.weight': tensor([[ 0.0412,  0.0670, -0.1750,  ...,  0.0121,  0.0238, -0.1610],\n",
       "         [-0.0029, -0.0763, -0.0480,  ..., -0.0039, -0.0032, -0.0043],\n",
       "         [-0.1422, -0.0167, -0.0263,  ..., -0.0453,  0.0587,  0.0488],\n",
       "         ...,\n",
       "         [-0.0135, -0.2300, -0.1558,  ...,  0.0684, -0.0237, -0.0024],\n",
       "         [ 0.0916, -0.0360,  0.0775,  ...,  0.0323,  0.3794,  0.0118],\n",
       "         [ 0.0227, -0.2444, -0.0505,  ...,  0.1031,  0.0501, -0.0388]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.self_attn.linear_keys.bias': tensor([ 0.0125, -0.0178, -0.0150,  ...,  0.0234, -0.0055, -0.0001],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.self_attn.linear_values.weight': tensor([[ 0.0773, -0.0530,  0.0605,  ..., -0.1372,  0.0030,  0.0672],\n",
       "         [ 0.2700,  0.0829, -0.3567,  ...,  0.2375, -0.0533,  0.0267],\n",
       "         [ 0.0301,  0.2338,  0.0894,  ..., -0.2131, -0.3374,  0.2045],\n",
       "         ...,\n",
       "         [-0.2666, -0.2839, -0.1636,  ...,  0.3601,  0.0040, -0.3984],\n",
       "         [-0.0587,  0.0031, -0.1780,  ...,  0.2126,  0.2250,  0.1445],\n",
       "         [-0.2861,  0.4861,  0.2120,  ..., -0.1197,  0.0020, -0.3445]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.self_attn.linear_values.bias': tensor([ 0.2822,  0.0126,  0.1081,  ..., -0.1305, -0.0354, -0.1240],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.self_attn.linear_query.weight': tensor([[ 0.0190,  0.2964, -0.2443,  ..., -0.0494,  0.1162, -0.1262],\n",
       "         [-0.0831,  0.1069, -0.2057,  ...,  0.0015, -0.0763, -0.0235],\n",
       "         [-0.0543, -0.1862, -0.0384,  ...,  0.0033,  0.2153,  0.0687],\n",
       "         ...,\n",
       "         [ 0.1157,  0.0988,  0.0569,  ...,  0.2520,  0.2612, -0.1421],\n",
       "         [ 0.1771, -0.0110, -0.0386,  ...,  0.0368, -0.1282,  0.1573],\n",
       "         [ 0.0934, -0.0471,  0.1450,  ...,  0.0489, -0.0100, -0.0621]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.self_attn.linear_query.bias': tensor([ 0.0312, -0.0041,  0.0637,  ...,  0.0500,  0.0525, -0.0266],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.self_attn.final_linear.weight': tensor([[ 0.0059,  0.2668,  0.2118,  ..., -0.2654, -0.0499,  0.2559],\n",
       "         [-0.2656,  0.0468, -0.2856,  ...,  0.0208, -0.1278, -0.0139],\n",
       "         [ 0.3335, -0.2891,  0.0821,  ...,  0.1823, -0.1914,  0.5107],\n",
       "         ...,\n",
       "         [ 0.3220, -0.3181,  0.3914,  ..., -0.2502, -0.1149, -0.1750],\n",
       "         [ 0.0428,  0.4807, -0.2045,  ..., -0.2837,  0.4756,  0.1137],\n",
       "         [ 0.2433,  0.1946,  0.1001,  ..., -0.3577,  0.1799,  0.0641]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.self_attn.final_linear.bias': tensor([ 0.1238, -0.4033, -0.2258,  ...,  0.2472,  0.3831,  0.5000],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.layer_norm_1.weight': tensor([0.4631, 0.4277, 0.4272,  ..., 0.4153, 0.4421, 0.4478],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.layer_norm_1.bias': tensor([ 0.0149,  0.0303,  0.0506,  ..., -0.0093,  0.0063, -0.0349],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.context_attn.linear_keys.weight': tensor([[ 0.0770,  0.0722,  0.1160,  ...,  0.3164,  0.0057, -0.0553],\n",
       "         [ 0.3330, -0.0047,  0.2705,  ..., -0.0791,  0.2482,  0.0441],\n",
       "         [-0.3704, -0.2062, -0.1801,  ..., -0.1881, -0.1003,  0.1001],\n",
       "         ...,\n",
       "         [-0.1610, -0.0007,  0.0804,  ..., -0.1913,  0.0624, -0.0886],\n",
       "         [ 0.3018,  0.2052, -0.0257,  ..., -0.0631,  0.1400,  0.0320],\n",
       "         [ 0.1862,  0.1296,  0.1224,  ..., -0.0197, -0.2944, -0.0590]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.context_attn.linear_keys.bias': tensor([-0.0226, -0.0145,  0.0011,  ...,  0.0308, -0.0099,  0.0002],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.context_attn.linear_values.weight': tensor([[-0.3394,  0.0433, -0.1200,  ...,  0.0173, -0.2808, -0.0782],\n",
       "         [-0.0045,  0.4124,  0.1859,  ..., -0.4805,  0.1549,  0.0018],\n",
       "         [-0.4233,  0.1075,  0.4949,  ...,  0.1077,  0.1019, -0.0264],\n",
       "         ...,\n",
       "         [ 0.2230, -0.0882, -0.2074,  ...,  0.0633,  0.1359, -0.0247],\n",
       "         [-0.5117,  0.3386,  0.1121,  ..., -0.0284, -0.0029, -0.0371],\n",
       "         [ 0.0726,  0.0780,  0.4983,  ...,  0.0255, -0.1683, -0.0468]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.context_attn.linear_values.bias': tensor([ 0.3779, -0.3152,  0.2307,  ...,  0.1636,  0.0676,  0.1646],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.context_attn.linear_query.weight': tensor([[ 0.2583,  0.1307,  0.1001,  ..., -0.1404,  0.1581, -0.2004],\n",
       "         [-0.3770,  0.1218, -0.2306,  ..., -0.0723, -0.1956, -0.0639],\n",
       "         [-0.0841, -0.5142, -0.1659,  ..., -0.0734, -0.1177,  0.3169],\n",
       "         ...,\n",
       "         [-0.2815,  0.1479,  0.0992,  ..., -0.1552, -0.0829, -0.0184],\n",
       "         [-0.2330, -0.3279,  0.1219,  ..., -0.0037,  0.1787, -0.1523],\n",
       "         [-0.0017,  0.0161,  0.3259,  ..., -0.0775,  0.1696,  0.1904]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.context_attn.linear_query.bias': tensor([ 0.0332,  0.0112, -0.0296,  ...,  0.0381, -0.1301,  0.1093],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.context_attn.final_linear.weight': tensor([[ 0.1061, -0.4470, -0.2705,  ...,  0.2384,  0.1384, -0.0640],\n",
       "         [-0.0952,  0.2932,  0.1537,  ..., -0.0050,  0.5049,  0.0322],\n",
       "         [-0.0566,  0.2688,  0.1437,  ...,  0.0014, -0.3093, -0.1046],\n",
       "         ...,\n",
       "         [ 0.1476, -0.1459,  0.0959,  ...,  0.0439,  0.0358,  0.0531],\n",
       "         [-0.2729,  0.0743, -0.1349,  ..., -0.3132, -0.3823,  0.0961],\n",
       "         [-0.0887,  0.0021,  0.0411,  ...,  0.0129,  0.2395, -0.0094]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.context_attn.final_linear.bias': tensor([-0.2170, -0.0152,  0.1914,  ...,  0.2715, -0.2610,  0.1967],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.layer_norm_2.weight': tensor([0.1378, 0.1429, 0.1512,  ..., 0.1249, 0.1181, 0.1212],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.layer_norm_2.bias': tensor([-0.0044, -0.0116, -0.0141,  ..., -0.0624, -0.0088, -0.0331],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.feed_forward.w_1.weight': tensor([[-0.1227, -0.1321,  0.0471,  ..., -0.0699,  0.3000,  0.0210],\n",
       "         [ 0.0005, -0.0152,  0.2771,  ...,  0.0279,  0.3499,  0.0677],\n",
       "         [ 0.1163, -0.1272,  0.0693,  ..., -0.0362, -0.1906, -0.1945],\n",
       "         ...,\n",
       "         [ 0.0235,  0.0792, -0.1301,  ..., -0.2754,  0.1683,  0.1143],\n",
       "         [ 0.2629,  0.1120,  0.1351,  ..., -0.4358,  0.3750,  0.1439],\n",
       "         [-0.0416, -0.0441, -0.0393,  ...,  0.2874, -0.1884, -0.1794]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.feed_forward.w_1.bias': tensor([-0.1962,  0.0445, -0.2059,  ..., -0.0576, -0.1960, -0.2871],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.feed_forward.w_2.weight': tensor([[ 8.3008e-02, -5.6122e-02, -7.4219e-02,  ...,  1.0742e-01,\n",
       "           1.8079e-01,  3.5797e-02],\n",
       "         [ 6.8481e-02, -3.7988e-01,  6.8176e-02,  ..., -1.2708e-01,\n",
       "          -2.2437e-01, -1.4900e-02],\n",
       "         [ 6.8115e-02,  1.5976e-02, -5.3986e-02,  ...,  9.3079e-03,\n",
       "          -7.8064e-02, -2.0206e-04],\n",
       "         ...,\n",
       "         [ 1.0767e-01, -2.2559e-01,  6.6956e-02,  ...,  1.7554e-01,\n",
       "          -1.1255e-01, -1.7712e-01],\n",
       "         [ 1.7139e-01, -2.7878e-02,  2.8882e-01,  ..., -3.6530e-02,\n",
       "           3.8892e-01, -3.6774e-02],\n",
       "         [ 1.1041e-01,  1.1151e-01,  4.4946e-01,  ...,  2.5220e-01,\n",
       "          -1.1469e-01,  2.3083e-01]], dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.feed_forward.w_2.bias': tensor([ 0.4521, -0.3462,  0.2539,  ..., -0.4795,  0.3608, -0.4741],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.feed_forward.layer_norm.weight': tensor([1.1299, 1.1377, 1.1543,  ..., 1.1816, 1.1719, 1.1152],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.feed_forward.layer_norm.bias': tensor([-0.0729,  0.0707,  0.0654,  ..., -0.0680, -0.0355, -0.0500],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.self_attn.linear_keys.weight': tensor([[-0.0400,  0.3137,  0.2144,  ..., -0.0206, -0.0992,  0.0430],\n",
       "         [ 0.0554, -0.1588, -0.0986,  ...,  0.0203,  0.0802,  0.0438],\n",
       "         [ 0.0055,  0.3245,  0.2040,  ...,  0.0994,  0.1707, -0.0129],\n",
       "         ...,\n",
       "         [ 0.2585,  0.1141, -0.0263,  ...,  0.3691, -0.1616,  0.1261],\n",
       "         [-0.1812, -0.1230, -0.0220,  ...,  0.2556,  0.0544, -0.1011],\n",
       "         [ 0.0005,  0.0013, -0.0595,  ...,  0.1917,  0.1915, -0.1076]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.self_attn.linear_keys.bias': tensor([-0.0079, -0.0251, -0.0022,  ...,  0.0141, -0.0074, -0.0267],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.self_attn.linear_values.weight': tensor([[ 0.0881, -0.0816, -0.3635,  ..., -0.1008,  0.2979, -0.1902],\n",
       "         [-0.4851,  0.2356, -0.1570,  ...,  0.4988,  0.2627, -0.2037],\n",
       "         [-0.4224,  0.0797, -0.1847,  ...,  0.1333,  0.2605,  0.1112],\n",
       "         ...,\n",
       "         [-0.1384,  0.1107,  0.3318,  ...,  0.0625, -0.3311,  0.2041],\n",
       "         [ 0.2361, -0.0741,  0.0787,  ...,  0.1681,  0.2666,  0.1646],\n",
       "         [ 0.0750,  0.3904,  0.2969,  ..., -0.0723,  0.3203,  0.2676]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.self_attn.linear_values.bias': tensor([ 0.0285,  0.0306, -0.0096,  ...,  0.1098, -0.0996,  0.0311],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.self_attn.linear_query.weight': tensor([[-0.0460, -0.0241, -0.1788,  ...,  0.0292,  0.1790,  0.0556],\n",
       "         [ 0.1548,  0.5137,  0.0047,  ...,  0.0356, -0.0297, -0.0916],\n",
       "         [ 0.2031,  0.1603,  0.0542,  ...,  0.1442, -0.1305, -0.3298],\n",
       "         ...,\n",
       "         [ 0.1368, -0.1372,  0.1624,  ..., -0.0352,  0.0616,  0.0765],\n",
       "         [-0.0724,  0.2190,  0.0684,  ...,  0.4541,  0.0286,  0.0825],\n",
       "         [ 0.0134,  0.2773, -0.0466,  ...,  0.0743, -0.0175,  0.0654]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.self_attn.linear_query.bias': tensor([-0.1167,  0.1196,  0.5557,  ...,  0.0797,  0.0830, -0.0103],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.self_attn.final_linear.weight': tensor([[ 0.1730, -0.1925,  0.2759,  ..., -0.1399,  0.3088, -0.1022],\n",
       "         [ 0.3936,  0.5000,  0.0141,  ..., -0.1937, -0.4775,  0.0814],\n",
       "         [ 0.2512, -0.1377, -0.1252,  ..., -0.0349, -0.4585, -0.1528],\n",
       "         ...,\n",
       "         [ 0.0133,  0.1696,  0.0926,  ..., -0.1819,  0.3171, -0.1846],\n",
       "         [ 0.3127,  0.3918,  0.3259,  ...,  0.1276,  0.2471, -0.1536],\n",
       "         [-0.2988, -0.4556,  0.1615,  ...,  0.1554,  0.1527,  0.1873]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.self_attn.final_linear.bias': tensor([-0.0384, -0.5000, -0.2482,  ...,  0.2445,  0.2729,  0.2881],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.layer_norm_1.weight': tensor([0.4563, 0.4343, 0.4404,  ..., 0.4568, 0.4783, 0.4636],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.layer_norm_1.bias': tensor([ 0.0194,  0.0422,  0.0615,  ..., -0.0120,  0.0118, -0.0328],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.context_attn.linear_keys.weight': tensor([[ 3.4149e-02, -5.1575e-02, -6.8474e-03,  ..., -2.6270e-01,\n",
       "          -1.0559e-01,  3.8544e-02],\n",
       "         [-1.1383e-02,  1.3562e-01,  5.2887e-02,  ..., -3.1143e-02,\n",
       "           4.0924e-02, -1.0577e-01],\n",
       "         [ 1.4014e-01, -5.2338e-02,  1.0590e-01,  ...,  8.5938e-02,\n",
       "          -1.3757e-01, -5.6519e-02],\n",
       "         ...,\n",
       "         [ 8.4595e-02,  1.6101e-01,  5.9387e-02,  ...,  2.5620e-02,\n",
       "           1.3062e-01,  8.1665e-02],\n",
       "         [-1.1390e-04,  1.9577e-02, -4.6783e-02,  ...,  6.4331e-02,\n",
       "           2.7808e-01,  7.2205e-02],\n",
       "         [ 1.6769e-02,  1.3708e-01,  1.8848e-01,  ...,  1.6357e-01,\n",
       "          -3.4241e-02,  4.7729e-02]], dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.context_attn.linear_keys.bias': tensor([-0.0041, -0.0047, -0.0187,  ..., -0.0097,  0.0097, -0.0026],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.context_attn.linear_values.weight': tensor([[-0.3379, -0.1038, -0.1940,  ...,  0.1746, -0.0856,  0.0266],\n",
       "         [-0.2671,  0.0333, -0.0767,  ...,  0.4155, -0.5005, -0.0095],\n",
       "         [ 0.3232, -0.4922, -0.1841,  ..., -0.3206, -0.0970, -0.0087],\n",
       "         ...,\n",
       "         [ 0.1046,  0.0024, -0.0066,  ...,  0.4270, -0.3845, -0.0042],\n",
       "         [-0.1265, -0.1398,  0.5059,  ..., -0.2610, -0.1061, -0.1057],\n",
       "         [-0.2502,  0.1376, -0.0500,  ...,  0.3198, -0.2014, -0.0424]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.context_attn.linear_values.bias': tensor([-0.1736, -0.1248, -0.0724,  ..., -0.0193,  0.0529, -0.0103],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.context_attn.linear_query.weight': tensor([[ 0.2465,  0.3572, -0.1578,  ...,  0.1051,  0.1056,  0.2705],\n",
       "         [ 0.1396,  0.0517,  0.1371,  ..., -0.1715,  0.2111,  0.0310],\n",
       "         [ 0.2795, -0.0452,  0.0254,  ...,  0.0745, -0.2004, -0.0875],\n",
       "         ...,\n",
       "         [ 0.2090,  0.1552,  0.0294,  ...,  0.0163,  0.1880, -0.0748],\n",
       "         [-0.0975,  0.2720, -0.0205,  ..., -0.2959,  0.0607, -0.1752],\n",
       "         [-0.0417,  0.0686,  0.0341,  ..., -0.0052, -0.0607,  0.1924]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.context_attn.linear_query.bias': tensor([-0.0300, -0.0188,  0.1372,  ...,  0.0365,  0.1879, -0.0723],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.context_attn.final_linear.weight': tensor([[-0.3167,  0.0122,  0.5063,  ..., -0.1279, -0.4751, -0.0692],\n",
       "         [ 0.3191, -0.3286, -0.1993,  ...,  0.2517, -0.3118,  0.1114],\n",
       "         [-0.2600, -0.3374,  0.1562,  ..., -0.0804, -0.0756,  0.4536],\n",
       "         ...,\n",
       "         [-0.4307,  0.0817, -0.2385,  ...,  0.0475, -0.0667, -0.0591],\n",
       "         [-0.3049, -0.0431, -0.3372,  ...,  0.0893, -0.2561, -0.6758],\n",
       "         [ 0.0142, -0.3330, -0.1852,  ...,  0.0263,  0.1252,  0.2230]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.context_attn.final_linear.bias': tensor([-0.4207,  0.3188, -0.1364,  ...,  0.4988,  0.1904,  0.2290],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.layer_norm_2.weight': tensor([0.1527, 0.1567, 0.1797,  ..., 0.1515, 0.1356, 0.1373],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.layer_norm_2.bias': tensor([ 0.0061, -0.0064, -0.0070,  ..., -0.0638, -0.0081, -0.0396],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.feed_forward.w_1.weight': tensor([[ 0.1241, -0.1385, -0.1334,  ..., -0.0159,  0.2927,  0.0763],\n",
       "         [ 0.0085,  0.0656,  0.0775,  ..., -0.0494, -0.1176,  0.4116],\n",
       "         [-0.4534,  0.1737,  0.0739,  ...,  0.1304,  0.3660, -0.1552],\n",
       "         ...,\n",
       "         [ 0.4180,  0.0042, -0.1140,  ..., -0.1505, -0.3511, -0.1108],\n",
       "         [-0.1115,  0.1649,  0.0151,  ...,  0.1686,  0.1980, -0.2250],\n",
       "         [ 0.4780,  0.0598, -0.0205,  ...,  0.0529,  0.2556, -0.0136]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.feed_forward.w_1.bias': tensor([ 0.0577, -0.0452, -0.0366,  ..., -0.1164, -0.1792, -0.0677],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.feed_forward.w_2.weight': tensor([[-0.0407, -0.1617,  0.1550,  ...,  0.1205, -0.5635,  0.1567],\n",
       "         [-0.4338,  0.2764, -0.1478,  ..., -0.0103, -0.0054,  0.1014],\n",
       "         [-0.3440, -0.4995, -0.1874,  ...,  0.0639, -0.1431, -0.2632],\n",
       "         ...,\n",
       "         [-0.0983,  0.1627,  0.1613,  ..., -0.0656, -0.2231,  0.2571],\n",
       "         [ 0.2744, -0.3892,  0.2253,  ..., -0.0503,  0.2094,  0.0518],\n",
       "         [ 0.1449, -0.2666,  0.1943,  ..., -0.1539, -0.0191, -0.0632]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.feed_forward.w_2.bias': tensor([ 0.2487, -0.1239, -0.1879,  ..., -0.2219, -0.0636, -0.2351],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.feed_forward.layer_norm.weight': tensor([1.2422, 1.2334, 1.1982,  ..., 1.0586, 1.2021, 1.0361],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.feed_forward.layer_norm.bias': tensor([-0.0576,  0.0120,  0.0236,  ..., -0.0124,  0.0219, -0.0676],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.self_attn.linear_keys.weight': tensor([[ 0.2886,  0.1121, -0.0573,  ...,  0.0407, -0.0726, -0.2524],\n",
       "         [-0.0529, -0.0389,  0.0386,  ..., -0.1946, -0.0904,  0.0975],\n",
       "         [ 0.0696, -0.2507,  0.1560,  ..., -0.0846, -0.1680,  0.0520],\n",
       "         ...,\n",
       "         [-0.1136,  0.1085,  0.0013,  ...,  0.1771,  0.0459,  0.0127],\n",
       "         [-0.0032, -0.1432,  0.0987,  ...,  0.0447, -0.1344, -0.1840],\n",
       "         [ 0.0046, -0.1639, -0.0983,  ...,  0.0025, -0.1478,  0.0221]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.self_attn.linear_keys.bias': tensor([-0.0265, -0.0275, -0.0131,  ..., -0.0024,  0.0249,  0.0030],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.self_attn.linear_values.weight': tensor([[-0.0421,  0.3135,  0.3291,  ...,  0.0999,  0.2000, -0.1260],\n",
       "         [ 0.4084, -0.0457,  0.0492,  ..., -0.3721,  0.1077, -0.1754],\n",
       "         [ 0.2480, -0.3030, -0.0977,  ..., -0.2354,  0.1797,  0.2527],\n",
       "         ...,\n",
       "         [-0.5186, -0.2668,  0.1226,  ..., -0.4153, -0.1384, -0.2952],\n",
       "         [ 0.1610, -0.0661,  0.2795,  ..., -0.1015,  0.3125,  0.0173],\n",
       "         [ 0.3323, -0.0244, -0.1360,  ...,  0.2456, -0.2400,  0.1328]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.self_attn.linear_values.bias': tensor([-0.1528,  0.0600,  0.0855,  ..., -0.1096,  0.1088,  0.2185],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.self_attn.linear_query.weight': tensor([[ 0.0587,  0.0670, -0.1146,  ..., -0.2434, -0.0790, -0.1637],\n",
       "         [ 0.0241,  0.0916,  0.1638,  ...,  0.0948,  0.0159, -0.2111],\n",
       "         [ 0.2725,  0.0554, -0.0481,  ...,  0.0247, -0.1354, -0.2389],\n",
       "         ...,\n",
       "         [-0.1547,  0.1963,  0.0953,  ...,  0.1492,  0.0555, -0.2155],\n",
       "         [-0.1432, -0.2502, -0.0406,  ...,  0.1307, -0.1010,  0.1482],\n",
       "         [ 0.0346,  0.0297,  0.2104,  ...,  0.0958,  0.0427,  0.0003]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.self_attn.linear_query.bias': tensor([ 0.0385, -0.0245,  0.0283,  ..., -0.2791, -0.0375,  0.2209],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.self_attn.final_linear.weight': tensor([[-0.0576,  0.1973,  0.3860,  ...,  0.0765, -0.0016, -0.0147],\n",
       "         [ 0.2068,  0.3779, -0.3418,  ..., -0.0335, -0.2578, -0.1578],\n",
       "         [ 0.1189,  0.2546, -0.0859,  ..., -0.3020, -0.1877, -0.3271],\n",
       "         ...,\n",
       "         [ 0.0778,  0.1593, -0.0295,  ...,  0.2583,  0.0644, -0.0288],\n",
       "         [ 0.1534,  0.1674,  0.3032,  ..., -0.2220, -0.2607, -0.3271],\n",
       "         [-0.0537,  0.2104,  0.3301,  ...,  0.2390, -0.0082,  0.1754]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.self_attn.final_linear.bias': tensor([ 0.1097, -0.2463, -0.2507,  ...,  0.0800,  0.3486, -0.2583],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.layer_norm_1.weight': tensor([0.4829, 0.4480, 0.4556,  ..., 0.5693, 0.4761, 0.5493],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.layer_norm_1.bias': tensor([ 0.0308,  0.0459,  0.0575,  ...,  0.0363,  0.0300, -0.0277],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.context_attn.linear_keys.weight': tensor([[ 0.0150, -0.1483,  0.0195,  ..., -0.0609,  0.2163, -0.0399],\n",
       "         [ 0.0737,  0.1042,  0.1449,  ..., -0.0804,  0.0560,  0.0888],\n",
       "         [ 0.0198, -0.0088, -0.1322,  ...,  0.1057, -0.1063,  0.0030],\n",
       "         ...,\n",
       "         [ 0.2419, -0.2389, -0.0453,  ...,  0.0304,  0.2489, -0.0426],\n",
       "         [-0.1217, -0.0621, -0.0611,  ..., -0.0334, -0.2382,  0.0494],\n",
       "         [ 0.1508,  0.0376, -0.2196,  ...,  0.0021, -0.0352,  0.0072]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.context_attn.linear_keys.bias': tensor([ 0.0108, -0.0240, -0.0222,  ..., -0.0309,  0.0018,  0.0179],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.context_attn.linear_values.weight': tensor([[ 0.0915, -0.2383,  0.2522,  ..., -0.1445,  0.1812, -0.0104],\n",
       "         [-0.0807, -0.5171,  0.2520,  ...,  0.1231, -0.4302, -0.0272],\n",
       "         [-0.3323, -0.1627, -0.3203,  ...,  0.1217,  0.1506, -0.0217],\n",
       "         ...,\n",
       "         [ 0.2469,  0.3311,  0.0141,  ...,  0.2886, -0.0831,  0.0104],\n",
       "         [-0.2668,  0.0880, -0.2100,  ...,  0.3708,  0.2345,  0.0215],\n",
       "         [-0.4988,  0.3843, -0.3296,  ..., -0.2578,  0.1772,  0.0237]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.context_attn.linear_values.bias': tensor([-0.0620, -0.0496, -0.1109,  ..., -0.0281, -0.0598, -0.0185],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.context_attn.linear_query.weight': tensor([[ 0.0280,  0.3601, -0.0068,  ..., -0.0675, -0.1942, -0.1337],\n",
       "         [ 0.0245, -0.2771,  0.0118,  ...,  0.1318,  0.0584, -0.0584],\n",
       "         [ 0.2629,  0.1715,  0.2456,  ..., -0.2177,  0.0575, -0.1232],\n",
       "         ...,\n",
       "         [ 0.1587,  0.0914, -0.1600,  ..., -0.0190,  0.1428,  0.0207],\n",
       "         [ 0.1343,  0.1017, -0.2053,  ..., -0.0431, -0.0784,  0.0717],\n",
       "         [ 0.4167,  0.2212,  0.1477,  ...,  0.2322,  0.1138,  0.1298]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.context_attn.linear_query.bias': tensor([-0.0859,  0.0497, -0.0948,  ..., -0.1085,  0.3074,  0.0983],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.context_attn.final_linear.weight': tensor([[ 0.1483, -0.3293,  0.1842,  ...,  0.1281,  0.3018,  0.0837],\n",
       "         [-0.2546, -0.1025,  0.0375,  ..., -0.1661,  0.2649, -0.0537],\n",
       "         [ 0.5557, -0.2404,  0.2537,  ...,  0.2489, -0.4336, -0.2517],\n",
       "         ...,\n",
       "         [ 0.1147, -0.0643,  0.2102,  ...,  0.0982,  0.2448,  0.1059],\n",
       "         [ 0.1241, -0.3018, -0.0366,  ...,  0.0471, -0.0987, -0.0247],\n",
       "         [-0.1238,  0.0363,  0.0179,  ..., -0.1735,  0.0165,  0.0449]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.context_attn.final_linear.bias': tensor([-0.4980, -0.1772, -0.2944,  ...,  0.3176,  0.3970, -0.3376],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.layer_norm_2.weight': tensor([0.2180, 0.2142, 0.2432,  ..., 0.2505, 0.1937, 0.5459],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.layer_norm_2.bias': tensor([ 0.0131, -0.0211, -0.0297,  ..., -0.0921, -0.0276, -0.1627],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.feed_forward.w_1.weight': tensor([[ 0.2133,  0.1367,  0.1583,  ...,  0.1154,  0.1157,  0.2507],\n",
       "         [ 0.5381,  0.2512,  0.0615,  ..., -0.1866,  0.0807, -0.4976],\n",
       "         [ 0.1731, -0.0155,  0.1274,  ..., -0.0911, -0.0814,  0.0748],\n",
       "         ...,\n",
       "         [-0.4023,  0.0764,  0.1492,  ...,  0.0858, -0.1229, -0.1460],\n",
       "         [ 0.3167, -0.4993, -0.1406,  ..., -0.0420, -0.0975, -0.1920],\n",
       "         [ 0.0118, -0.0053,  0.3694,  ..., -0.1689,  0.1758,  0.3420]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.feed_forward.w_1.bias': tensor([-0.0565, -0.1241,  0.1722,  ..., -0.1564, -0.1884, -0.1355],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.feed_forward.w_2.weight': tensor([[-0.2155,  0.0026, -0.0821,  ...,  0.2438,  0.4438, -0.1379],\n",
       "         [-0.0099,  0.0284,  0.1877,  ..., -0.3665, -0.3655, -0.4993],\n",
       "         [ 0.2686, -0.4487, -0.1062,  ..., -0.1671,  0.3542,  0.2520],\n",
       "         ...,\n",
       "         [-0.0320,  0.1042,  0.0439,  ...,  0.0439, -0.1384,  0.1189],\n",
       "         [-0.0164, -0.0094, -0.0580,  ..., -0.0873,  0.1727,  0.1772],\n",
       "         [-0.0598, -0.1796, -0.1490,  ...,  0.0716, -0.3953,  0.0061]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.feed_forward.w_2.bias': tensor([ 0.1031,  0.1510, -0.1302,  ...,  0.2443, -0.0541, -0.2505],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.feed_forward.layer_norm.weight': tensor([1.0381, 1.0693, 1.0654,  ..., 1.0059, 1.0098, 1.0000],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.feed_forward.layer_norm.bias': tensor([ 0.1394,  0.1179,  0.1831,  ..., -0.1677,  0.1306, -0.0412],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.layer_norm.weight': tensor([0.6274, 1.0703, 1.1914,  ..., 0.8398, 0.4060, 0.4321],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.layer_norm.bias': tensor([ 0.0264,  0.0236,  0.0949,  ...,  0.3755, -0.0079, -0.0789],\n",
       "        dtype=torch.float16)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['src', 'tgt', 'data_task', 'decoder_start_token'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['vocab'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weight': tensor([[-0.0321,  0.0348,  0.0181,  ...,  0.0312, -0.0099, -0.0133],\n",
       "         [-0.0039,  0.0104, -0.0156,  ...,  0.0290, -0.0138, -0.0134],\n",
       "         [-0.0245, -0.0283, -0.0295,  ...,  0.9712, -0.0255, -0.0273],\n",
       "         ...,\n",
       "         [-0.0123, -0.0031, -0.0089,  ...,  0.0645, -0.0182, -0.0740],\n",
       "         [ 0.0085, -0.0088, -0.0091,  ...,  0.0571, -0.0035, -0.1298],\n",
       "         [-0.0076, -0.0107, -0.0051,  ...,  1.0264, -0.0338, -0.1175]],\n",
       "        dtype=torch.float16),\n",
       " 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['generator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(config='', save_config=None, data={}, skip_empty_level='silent', save_data='', overwrite=False, n_sample=0, dump_transforms=False, src_vocab='', tgt_vocab='', share_vocab=True, src_feats_vocab=None, src_vocab_size=256206, tgt_vocab_size=256206, vocab_size_multiple=1, src_words_min_frequency=1, tgt_words_min_frequency=1, src_seq_length_trunc=None, tgt_seq_length_trunc=None, both_embeddings=None, src_embeddings=None, tgt_embeddings=None, embeddings_type=None, switchout_temperature=1.0, tokendrop_temperature=1.0, tokenmask_temperature=1.0, reversible_tokenization='joiner', prior_tokenization=False, src_subword_model='', tgt_subword_model='', src_subword_nbest=1, tgt_subword_nbest=1, src_subword_alpha=0.0, tgt_subword_alpha=0.0, src_subword_vocab='', tgt_subword_vocab='', src_vocab_threshold=0, tgt_vocab_threshold=0, src_subword_type='none', tgt_subword_type='none', src_onmttok_kwargs=\"{'mode': 'none'}\", tgt_onmttok_kwargs=\"{'mode': 'none'}\", src_seq_length=150, tgt_seq_length=150, src_prefix='', tgt_prefix='', permute_sent_ratio=0.0, rotate_ratio=0.0, insert_ratio=0.0, random_ratio=0.0, mask_ratio=0.0, mask_length='subword', poisson_lambda=3.0, replace_length=-1, src_word_vec_size=1024, tgt_word_vec_size=1024, word_vec_size=1024, share_decoder_embeddings=True, share_embeddings=True, position_encoding=True, position_encoding_type='SinusoidalConcat', update_vocab=False, feat_merge='concat', feat_vec_size=-1, feat_vec_exponent=0.7, model_task='seq2seq', model_type='text', model_dtype='fp16', encoder_type='transformer', decoder_type='transformer', freeze_encoder=False, freeze_decoder=False, layers=-1, enc_layers=12, dec_layers=12, hidden_size=1024, enc_hid_size=1024, dec_hid_size=1024, cnn_kernel_width=3, pos_ffn_activation_fn='relu', input_feed=1, bridge=False, rnn_type='LSTM', context_gate=None, bridge_extra_node=True, bidir_edges=True, state_dim=512, n_edge_types=2, n_node=2, n_steps=2, src_ggnn_size=0, global_attention='general', global_attention_function='softmax', self_attn_type='scaled-dot', max_relative_positions=0, heads=16, transformer_ff=4096, aan_useffn=False, add_qkvbias=True, lambda_align=0.0, alignment_layer=-3, alignment_heads=0, full_context_alignment=False, copy_attn=False, copy_attn_type='general', generator_function='softmax', copy_attn_force=False, reuse_copy_attn=False, copy_loss_by_seqlength=False, coverage_attn=False, lambda_coverage=0.0, lm_prior_model=None, lm_prior_lambda=0.0, lm_prior_tau=1.0, loss_scale=0, apex_opt_level='', data_type='text', save_model='nllb', save_checkpoint_steps=5000, keep_checkpoint=50, gpu_ranks=[0], world_size=1, gpu_backend='nccl', gpu_verbose_level=0, master_ip='localhost', master_port=10000, seed=1234, param_init=0.0, param_init_glorot=True, train_from='', reset_optim='none', pre_word_vecs_enc=None, pre_word_vecs_dec=None, freeze_word_vecs_enc=False, freeze_word_vecs_dec=False, num_workers=4, batch_size=8192, batch_size_multiple=1, batch_type='tokens', normalization='tokens', accum_count=[4], accum_steps=[0], valid_steps=5000, valid_batch_size=4096, train_steps=100000, single_pass=False, early_stopping=0, early_stopping_criteria=None, optim='', adagrad_accumulator_init=0, max_grad_norm=0.0, dropout=[0.1], attention_dropout=[0.1], dropout_steps=[0], truncated_decoder=0, adam_beta1=0.9, adam_beta2=0.98, label_smoothing=0.1, average_decay=0.0, average_every=1, learning_rate=5e-05, learning_rate_decay=0.5, start_decay_steps=50000, decay_steps=10000, decay_method='none', warmup_steps=4000, log_file='', log_file_level='0', verbose=False, train_eval_steps=200, train_metrics=[], valid_metrics=[], scoring_debug=False, dump_preds=None, report_every=100, exp_host='', exp='', tensorboard=False, tensorboard_log_dir='runs/onmt', bucket_size=262144, bucket_size_init=-1, bucket_size_increment=0, prefetch_factor=400, brnn=False, data_task='seq2seq', decoder_start_token='</s>', _all_transform={'filtertoolong'})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['opt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the large TSV file from CC-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>en</th>\n",
       "      <th>zh</th>\n",
       "      <th>extra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.249919</td>\n",
       "      <td>(b) The discussion of cumulative impacts shall...</td>\n",
       "      <td>（轻）（殊）（还）（兀）（自）（在）（做）（心）（理）（斗）（争）（，）（听）（得）（他）（...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.249479</td>\n",
       "      <td>And [the Lord] became impatient over the miser...</td>\n",
       "      <td>因他違背了耶和華的約，又因他在以色列中，行了愚妄的事。</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.249218</td>\n",
       "      <td>We saved the world, or at least stopped two wo...</td>\n",
       "      <td>我们拯救了世界,或者至少停止两个世界碰撞。</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.249163</td>\n",
       "      <td>To me, [differences] in their backgrounds, in ...</td>\n",
       "      <td>（轻）（殊）（还）（兀）（自）（在）（做）（心）（理）（斗）（争）（，）（听）（得）（他）（...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.248880</td>\n",
       "      <td>A. In having a foretaste in grace here of what...</td>\n",
       "      <td>（轻）（殊）（还）（兀）（自）（在）（做）（心）（理）（斗）（争）（，）（听）（得）（他）（...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.248351</td>\n",
       "      <td>You will then invade Rome, and Allah will enab...</td>\n",
       "      <td>然后，您将入侵罗马，真主将使您能够征服它。</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.247962</td>\n",
       "      <td>Maybe tonight or tomorrow [Wednesday] I will s...</td>\n",
       "      <td>也许今晚或明天[星期三]我会发言。</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.247731</td>\n",
       "      <td>They use what is before them, give thanks for it,</td>\n",
       "      <td>前者是“承蒙赏赐”，后者是“多谢款待”的意思。</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.247479</td>\n",
       "      <td>God made men and women so they can bear childr...</td>\n",
       "      <td>上帝创造了男女，所以他们可以生孩子，”穆加贝说。</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.247374</td>\n",
       "      <td>'They make things happen on the stage; they in...</td>\n",
       "      <td>”他们让事情发生在舞台上;他们发明;他们创造。</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.247193</td>\n",
       "      <td>(4) the provision(s) in the agreement claimed ...</td>\n",
       "      <td>（轻）（殊）（还）（兀）（自）（在）（做）（心）（理）（斗）（争）（，）（听）（得）（他）（...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.247132</td>\n",
       "      <td>If God’s Son sets you free, you are free indeed.</td>\n",
       "      <td>神的儿子使你有自由，你就真正得自由。</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.246990</td>\n",
       "      <td>'But why should one servant return?'</td>\n",
       "      <td>但是，为什么有一个仆人要回来呢？</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.246914</td>\n",
       "      <td>And mayest thou see thy children’s children; p...</td>\n",
       "      <td>願你看見你兒女的兒女！願平安歸於以色列！（《詩》128:6）</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.246892</td>\n",
       "      <td>The only thing these men were guilty of was wa...</td>\n",
       "      <td>“这些男人唯一有罪的就是想要保护你和我。</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.246682</td>\n",
       "      <td>But Mariam could not hear comfort in God's words.</td>\n",
       "      <td>但是瑪黎安無法從真主的話語裡聽見安慰。</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.246461</td>\n",
       "      <td>“One of the ways of doing that [was] through t...</td>\n",
       "      <td>而這麼做的途徑之一[就是]通過食物來體現。\\n1.2460833\\tCREATE TABLE...</td>\n",
       "      <td>我们看到旁边有一些尸体，还有一些人被救起。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.245138</td>\n",
       "      <td>(b) contain any other information the EPA may ...</td>\n",
       "      <td>（轻）（殊）（还）（兀）（自）（在）（做）（心）（理）（斗）（争）（，）（听）（得）（他）（...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                                 en  \\\n",
       "0   1.249919  (b) The discussion of cumulative impacts shall...   \n",
       "1   1.249479  And [the Lord] became impatient over the miser...   \n",
       "2   1.249218  We saved the world, or at least stopped two wo...   \n",
       "3   1.249163  To me, [differences] in their backgrounds, in ...   \n",
       "4   1.248880  A. In having a foretaste in grace here of what...   \n",
       "5   1.248351  You will then invade Rome, and Allah will enab...   \n",
       "6   1.247962  Maybe tonight or tomorrow [Wednesday] I will s...   \n",
       "7   1.247731  They use what is before them, give thanks for it,   \n",
       "8   1.247479  God made men and women so they can bear childr...   \n",
       "9   1.247374  'They make things happen on the stage; they in...   \n",
       "10  1.247193  (4) the provision(s) in the agreement claimed ...   \n",
       "11  1.247132   If God’s Son sets you free, you are free indeed.   \n",
       "12  1.246990               'But why should one servant return?'   \n",
       "13  1.246914  And mayest thou see thy children’s children; p...   \n",
       "14  1.246892  The only thing these men were guilty of was wa...   \n",
       "15  1.246682  But Mariam could not hear comfort in God's words.   \n",
       "16  1.246461  “One of the ways of doing that [was] through t...   \n",
       "17  1.245138  (b) contain any other information the EPA may ...   \n",
       "\n",
       "                                                   zh                  extra  \n",
       "0   （轻）（殊）（还）（兀）（自）（在）（做）（心）（理）（斗）（争）（，）（听）（得）（他）（...                    NaN  \n",
       "1                         因他違背了耶和華的約，又因他在以色列中，行了愚妄的事。                    NaN  \n",
       "2                               我们拯救了世界,或者至少停止两个世界碰撞。                    NaN  \n",
       "3   （轻）（殊）（还）（兀）（自）（在）（做）（心）（理）（斗）（争）（，）（听）（得）（他）（...                    NaN  \n",
       "4   （轻）（殊）（还）（兀）（自）（在）（做）（心）（理）（斗）（争）（，）（听）（得）（他）（...                    NaN  \n",
       "5                               然后，您将入侵罗马，真主将使您能够征服它。                    NaN  \n",
       "6                                   也许今晚或明天[星期三]我会发言。                    NaN  \n",
       "7                             前者是“承蒙赏赐”，后者是“多谢款待”的意思。                    NaN  \n",
       "8                            上帝创造了男女，所以他们可以生孩子，”穆加贝说。                    NaN  \n",
       "9                             ”他们让事情发生在舞台上;他们发明;他们创造。                    NaN  \n",
       "10  （轻）（殊）（还）（兀）（自）（在）（做）（心）（理）（斗）（争）（，）（听）（得）（他）（...                    NaN  \n",
       "11                                 神的儿子使你有自由，你就真正得自由。                    NaN  \n",
       "12                                   但是，为什么有一个仆人要回来呢？                    NaN  \n",
       "13                     願你看見你兒女的兒女！願平安歸於以色列！（《詩》128:6）                    NaN  \n",
       "14                               “这些男人唯一有罪的就是想要保护你和我。                    NaN  \n",
       "15                                但是瑪黎安無法從真主的話語裡聽見安慰。                    NaN  \n",
       "16  而這麼做的途徑之一[就是]通過食物來體現。\\n1.2460833\\tCREATE TABLE...  我们看到旁边有一些尸体，还有一些人被救起。  \n",
       "17  （轻）（殊）（还）（兀）（自）（在）（做）（心）（理）（斗）（争）（，）（听）（得）（他）（...                    NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"en-zh.bitextf.tsv\",sep = '\\t', header=None, nrows=18, names =['id','en','zh', 'extra'])\n",
    "df\n",
    "# pd.read_table(\"en-zh.bitextf.tsv\", encoding='utf-8') \n",
    "# 'extra1','extra2','extra3','extra4','extra5','extra6','extra7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "（轻）（殊）（还）（兀）（自）（在）（做）（心）（理）（斗）（争）（，）（听）（得）（他）（问）（，）（愣）（了）（愣）（才）（反）（应）（过）（来）（，）（忙）（道）（：）（郁）（瓷）（说）（四）（界）（考）（核）（的）（名）（册）（上）（有）（我）（的）（名）（字）（，）（想）（问）（问）（…）（…）（是）（不）（是）（师）（父）（替）（我）（报）（的）（名）（？）（见）（她）（一）（本）（正）（经）（的）（拉）（着）（自）（己）（的）（袖）（子）（反）（复）（嗅）（闻）（，）（扶）（渊）（露）（出）（浅）（笑）（，）（是）（吗）（？）（好）（一）（个）（甩）（手）（掌）（柜）（，）（将）（这）（麻）（烦）（问）（题）（丢）（给）（她）（，）（自）（己）（洗）（脱）（地）（一）（干）（二）（净）（，）（天）（帝）（就）（是）（天）（帝）（，）（轻）（殊）（腹）（诽）（。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"en-zh.bitextf.tsv\",\"r\", encoding='utf-8') as f:\n",
    "    for i in range(21):\n",
    "        line = f.readline()\n",
    "        if (i > 18) & (len(line.split('\\t'))==3):\n",
    "            random_char = line.split('\\t')[2]\n",
    "print(random_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing the rewriting script\n",
    "# new_file = open('new_test.txt',\"a\", encoding='utf-8')\n",
    "\n",
    "# with open(\"en-zh.bitextf.tsv\",\"r\", encoding='utf-8') as f:\n",
    "    \n",
    "#     for i in range(21):\n",
    "#         line = f.readline()\n",
    "#         if (i > 10) & (len(line.split('\\t'))==3) & (random_char not in line):\n",
    "#             new_file.write(line)\n",
    "\n",
    "# new_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the rewriting script\n",
    "new_en_file = open('en-zh.bitextf-en-edited.en',\"a\", encoding='utf-8')\n",
    "new_zh_file = open('en-zh.bitextf-zh-edited.zh',\"a\", encoding='utf-8')\n",
    "\n",
    "with open(\"en-zh.bitextf.tsv\",\"r\", encoding='utf-8') as f:\n",
    "    while line:= f.readline():\n",
    "        if (len(line.split('\\t'))==3) & (random_char not in line):\n",
    "            new_en_file.write(line.split('\\t')[1])\n",
    "            new_zh_file.write(line.split('\\t')[2])\n",
    "\n",
    "new_en_file.close()\n",
    "new_zh_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"en-zh.bitextf-en-edited.en\", encoding='utf-8') as f:\n",
    "    en_str = f.read()\n",
    "with open(\"en-zh.bitextf-zh-edited.zh\", encoding='utf-8') as f:\n",
    "    zh_str = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_str.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_df = pd.series(en_str)\n",
    "zh_df = pd.series(zh_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using News Data  (600K data points)\n",
    "Pandas does not separate the lines properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"news.en\", encoding='utf-8') as f:\n",
    "    en_str = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"news.translatedto.zh\", encoding='utf-8') as f:\n",
    "    zh_str = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           52.76 - Geoff Boycott, England (16 innings, 68...\n",
       "1           In the immediate term, though, that lack of pr...\n",
       "2           What this group does is to take down various d...\n",
       "3           'i live 90% of my life in my head ': What Davi...\n",
       "4           The products highlight the city's heritage and...\n",
       "                                  ...                        \n",
       "19763863    Think of new asphalt on streets; these objects...\n",
       "19763864    Ottawa recalled G Matt O'Connor from the AHL t...\n",
       "19763865    Alvarez, who ruled as de facto president from ...\n",
       "19763866        I'm definitely headed in the right direction.\n",
       "19763867                                                     \n",
       "Length: 19763868, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_df = pd.Series(en_str.split('\\n'))\n",
    "en_df\n",
    "# 19763867 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                            -英格兰的 Geoff Boycott （16英寸，686公里）\n",
       "1           不过，在近期内，对于德鲁 Brees 来说，压力的缺乏可能是个问题，他的传球达到了1，065...\n",
       "2                            这个团体所做的就是在它认为是犯罪和导致恐怖行为的不同网站上进行。\n",
       "3                                           我生活中的90%生活在我的头脑中。\n",
       "4                            这些产品突出了城市的遗产和文化，它设计的是爱和好的旧式爱国主义。\n",
       "                                  ...                        \n",
       "19763863                    想想街道上的新沥青，这些东西看上去像木炭，或者在某些情况下更黑暗。\n",
       "19763864                     渥太华点了 G 马特·奥康纳从 AHL 到支持迈克·康顿的比赛。\n",
       "19763865    拉雷维21报告说，自从1973年政变以来，阿尔瓦雷斯曾是来自1981-85年的总统，但因19...\n",
       "19763866                                            我的方向是正确的。\n",
       "19763867                                                     \n",
       "Length: 19763868, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_df = pd.Series(zh_str.split('\\n'))\n",
    "zh_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"饱|畅|湍|滩|岭|诩|阔|荫|鸽|勋|鸡|鹰|裙|艳|哦|毋庸|稻|蔗|熔|亥|裤|氢|《|》\"\n",
    "\n",
    "relevant_index = zh_df.str.contains(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131         And rarely do Republicans talk about how they ...\n",
       "178         It might strengthen the case for tweaking eggs...\n",
       "196         Once at the Sunday Times, Evans pushed his tal...\n",
       "197         Then, he took her into a company stockroom whe...\n",
       "224         It's what Newton was on about when he wrote \"H...\n",
       "                                  ...                        \n",
       "19763729    Israeli Prime Minister Benjamin Netanyahu inst...\n",
       "19763750    He expects to be interviewed for the position ...\n",
       "19763779    28 (UPI) -- Scarlett Johansson is the top-gros...\n",
       "19763814    The Seattle Seahawks and Pro Bowl defensive en...\n",
       "19763837    Zadran and his four cousins claimed a small ro...\n",
       "Length: 681491, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_en = en_df[relevant_index]\n",
    "output_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131                而且，共和党人很少谈论他们将如何为一个保护《平价医疗法》所取得的保险利益的制度买单。\n",
       "178             这可能会加强以其他方式调整鸡蛋的理由，创造出“完美”的婴儿以头发或眼睛颜色排列的“完美”。\n",
       "196              在周日的《星期日泰晤士报》，埃文斯向他的才华记者们提出了更多关于数千例死亡和死胎的信息。\n",
       "197           然后，他把她带到一间公司的一间储藏室，在那里，他把裤子拉下来，并且在对她的性侵犯前暴露了自己。\n",
       "224                        牛顿在他的第二版《假说》中写道：“我不会假设，”他说，“我只知道。”\n",
       "                                  ...                        \n",
       "19763729    以色列总理本雅明·内塔尼亚胡指示该委员会推迟投票，以明显的努力缓和与美国的关系，《华盛顿邮报...\n",
       "19763750                 他希望在《法案》正式面试候选人以接替 Ryan 担任主教练的职位上面试。\n",
       "19763779                       据《福布斯》报道，斯嘉丽·约翰逊是2016年最吸引人的演员。\n",
       "19763814    据上周五的多项报道，西雅图海鹰和“ Pro BAT ”卫冕冠军迈克尔·贝内特（ Michae...\n",
       "19763837    撒督和他的四个表兄弟们在仓库隔壁的一栋楼里找了一间小屋子，12月初的一个晚上，他们在一个大盆...\n",
       "Length: 681491, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_zh = zh_df[relevant_index]\n",
    "output_zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3950327                                                     氢\n",
       "14071985                                                    鸡\n",
       "15433006                                                    鸡\n",
       "1959741                                                     氢\n",
       "1167074                                                     鸡\n",
       "                                  ...                        \n",
       "16590114    自2014年5月以来，自诩为“埃塞克斯女孩”的 Dockerill 女士曾担任圣凯瑟琳市& ...\n",
       "16274096    自2014年5月以来，自诩为“埃塞克斯女孩”的 Dockerill 女士曾担任圣凯瑟琳市& ...\n",
       "997379      当然，海滩度假是以它的方式起飞的另一个原因& mdash ;& mdash ;&& mdas...\n",
       "1778781     《 Def 电影》制作人创造了一个点，但它的制片人， Russell Simmons & m...\n",
       "4249432     《柳叶刀》的写作《柳叶刀》( The Oxford Martin Schools of Fo...\n",
       "Length: 681491, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_zh.sort_values(key = lambda x: x.str.len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>And rarely do Republicans talk about how they ...</td>\n",
       "      <td>而且，共和党人很少谈论他们将如何为一个保护《平价医疗法》所取得的保险利益的制度买单。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It might strengthen the case for tweaking eggs...</td>\n",
       "      <td>这可能会加强以其他方式调整鸡蛋的理由，创造出“完美”的婴儿以头发或眼睛颜色排列的“完美”。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Once at the Sunday Times, Evans pushed his tal...</td>\n",
       "      <td>在周日的《星期日泰晤士报》，埃文斯向他的才华记者们提出了更多关于数千例死亡和死胎的信息。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Then, he took her into a company stockroom whe...</td>\n",
       "      <td>然后，他把她带到一间公司的一间储藏室，在那里，他把裤子拉下来，并且在对她的性侵犯前暴露了自己。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It's what Newton was on about when he wrote \"H...</td>\n",
       "      <td>牛顿在他的第二版《假说》中写道：“我不会假设，”他说，“我只知道。”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681485</th>\n",
       "      <td>The spokesperson said Akdeniz feels a personal...</td>\n",
       "      <td>这位发言人说， Akdeniz 觉得与《星球大战》电影和他们的已故明星有个人联系。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681486</th>\n",
       "      <td>Israeli Prime Minister Benjamin Netanyahu inst...</td>\n",
       "      <td>以色列总理本雅明·内塔尼亚胡指示该委员会推迟投票，以明显的努力缓和与美国的关系，《华盛顿邮报...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681488</th>\n",
       "      <td>28 (UPI) -- Scarlett Johansson is the top-gros...</td>\n",
       "      <td>据《福布斯》报道，斯嘉丽·约翰逊是2016年最吸引人的演员。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681489</th>\n",
       "      <td>The Seattle Seahawks and Pro Bowl defensive en...</td>\n",
       "      <td>据上周五的多项报道，西雅图海鹰和“ Pro BAT ”卫冕冠军迈克尔·贝内特（ Michae...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681490</th>\n",
       "      <td>Zadran and his four cousins claimed a small ro...</td>\n",
       "      <td>撒督和他的四个表兄弟们在仓库隔壁的一栋楼里找了一间小屋子，12月初的一个晚上，他们在一个大盆...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>601460 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        0  \\\n",
       "0       And rarely do Republicans talk about how they ...   \n",
       "1       It might strengthen the case for tweaking eggs...   \n",
       "2       Once at the Sunday Times, Evans pushed his tal...   \n",
       "3       Then, he took her into a company stockroom whe...   \n",
       "4       It's what Newton was on about when he wrote \"H...   \n",
       "...                                                   ...   \n",
       "681485  The spokesperson said Akdeniz feels a personal...   \n",
       "681486  Israeli Prime Minister Benjamin Netanyahu inst...   \n",
       "681488  28 (UPI) -- Scarlett Johansson is the top-gros...   \n",
       "681489  The Seattle Seahawks and Pro Bowl defensive en...   \n",
       "681490  Zadran and his four cousins claimed a small ro...   \n",
       "\n",
       "                                                        1  \n",
       "0              而且，共和党人很少谈论他们将如何为一个保护《平价医疗法》所取得的保险利益的制度买单。  \n",
       "1           这可能会加强以其他方式调整鸡蛋的理由，创造出“完美”的婴儿以头发或眼睛颜色排列的“完美”。  \n",
       "2            在周日的《星期日泰晤士报》，埃文斯向他的才华记者们提出了更多关于数千例死亡和死胎的信息。  \n",
       "3         然后，他把她带到一间公司的一间储藏室，在那里，他把裤子拉下来，并且在对她的性侵犯前暴露了自己。  \n",
       "4                      牛顿在他的第二版《假说》中写道：“我不会假设，”他说，“我只知道。”  \n",
       "...                                                   ...  \n",
       "681485          这位发言人说， Akdeniz 觉得与《星球大战》电影和他们的已故明星有个人联系。  \n",
       "681486  以色列总理本雅明·内塔尼亚胡指示该委员会推迟投票，以明显的努力缓和与美国的关系，《华盛顿邮报...  \n",
       "681488                     据《福布斯》报道，斯嘉丽·约翰逊是2016年最吸引人的演员。  \n",
       "681489  据上周五的多项报道，西雅图海鹰和“ Pro BAT ”卫冕冠军迈克尔·贝内特（ Michae...  \n",
       "681490  撒督和他的四个表兄弟们在仓库隔壁的一栋楼里找了一间小屋子，12月初的一个晚上，他们在一个大盆...  \n",
       "\n",
       "[601460 rows x 2 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop duplicates\n",
    "combined_df = pd.concat([output_en, output_zh], axis=1).reset_index(drop=True).drop_duplicates()\n",
    "combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         And rarely do Republicans talk about how they ...\n",
       "1         It might strengthen the case for tweaking eggs...\n",
       "2         Once at the Sunday Times, Evans pushed his tal...\n",
       "3         Then, he took her into a company stockroom whe...\n",
       "4         It's what Newton was on about when he wrote \"H...\n",
       "                                ...                        \n",
       "681485    The spokesperson said Akdeniz feels a personal...\n",
       "681486    Israeli Prime Minister Benjamin Netanyahu inst...\n",
       "681488    28 (UPI) -- Scarlett Johansson is the top-gros...\n",
       "681489    The Seattle Seahawks and Pro Bowl defensive en...\n",
       "681490    Zadran and his four cousins claimed a small ro...\n",
       "Name: 0, Length: 601460, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                而且，共和党人很少谈论他们将如何为一个保护《平价医疗法》所取得的保险利益的制度买单。\n",
       "1             这可能会加强以其他方式调整鸡蛋的理由，创造出“完美”的婴儿以头发或眼睛颜色排列的“完美”。\n",
       "2              在周日的《星期日泰晤士报》，埃文斯向他的才华记者们提出了更多关于数千例死亡和死胎的信息。\n",
       "3           然后，他把她带到一间公司的一间储藏室，在那里，他把裤子拉下来，并且在对她的性侵犯前暴露了自己。\n",
       "4                        牛顿在他的第二版《假说》中写道：“我不会假设，”他说，“我只知道。”\n",
       "                                ...                        \n",
       "681485            这位发言人说， Akdeniz 觉得与《星球大战》电影和他们的已故明星有个人联系。\n",
       "681486    以色列总理本雅明·内塔尼亚胡指示该委员会推迟投票，以明显的努力缓和与美国的关系，《华盛顿邮报...\n",
       "681488                       据《福布斯》报道，斯嘉丽·约翰逊是2016年最吸引人的演员。\n",
       "681489    据上周五的多项报道，西雅图海鹰和“ Pro BAT ”卫冕冠军迈克尔·贝内特（ Michae...\n",
       "681490    撒督和他的四个表兄弟们在仓库隔壁的一栋楼里找了一间小屋子，12月初的一个晚上，他们在一个大盆...\n",
       "Name: 1, Length: 601460, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.iloc[:,0].to_csv(r'cc-matrix-enzh-0to30M.en', header=None, index=None, sep='\\n', mode='a')\n",
    "combined_df.iloc[:,1].to_csv(r'cc-matrix-enzh-0to30M.zh', header=None, index=None, sep='\\n', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (3381747708.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[44], line 6\u001b[1;36m\u001b[0m\n\u001b[1;33m    with open(\"cc-matrix-enzh-300K.zh\", encoding='utf-8',\"a\") as f2:\u001b[0m\n\u001b[1;37m                                                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "with open(\"cc-matrix-enzh-0to30M.zh\", encoding='utf-8') as f:\n",
    "    # zh_str = f.read()\n",
    "    lines_number = 300000\n",
    "    head = [f.readline() for _ in range(lines_number)]\n",
    "\n",
    "    with open(\"cc-matrix-enzh-300K.zh\", encoding='utf-8',\"a\") as f2:\n",
    "        f2.write(\"\".join(head))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['而且，共和党人很少谈论他们将如何为一个保护《平价医疗法》所取得的保险利益的制度买单。\\n',\n",
       " '这可能会加强以其他方式调整鸡蛋的理由，创造出“完美”的婴儿以头发或眼睛颜色排列的“完美”。\\n',\n",
       " '在周日的《星期日泰晤士报》，埃文斯向他的才华记者们提出了更多关于数千例死亡和死胎的信息。\\n',\n",
       " '然后，他把她带到一间公司的一间储藏室，在那里，他把裤子拉下来，并且在对她的性侵犯前暴露了自己。\\n',\n",
       " '牛顿在他的第二版《假说》中写道：“我不会假设，”他说，“我只知道。”\\n',\n",
       " 'Facebook 、 Google 、微软、推特和雅虎已就《调查权力法案》草案向英国议会表示关注。\\n',\n",
       " '《消费者报告》审查睡眠辅助器具和补救办法\\n',\n",
       " '《十诫》是近年来最大的电视成功故事之一。\\n',\n",
       " '在接受《纽约时报》采访时，特朗普说他报复是因为他“生气是因为他们起诉”。\\n',\n",
       " '在 Burbank 中被禁止的：伯父的《大时间 Burleskyun .1940》风格的演出展示了异国情调的舞者、音乐和喜剧。\\n',\n",
       " '《消费者报告》调查睡觉辅助器具的效果，补救措施\\n',\n",
       " '据《 Maariv 报》报道，希腊机组人员并不首先了解以色列乘客的要求，并继续准备飞机飞行。\\n',\n",
       " '只有审慎行事的人才知道，在《金钱邮报》联系的时候，它的平均利率是25%。\\n',\n",
       " '《星球大战》：在中国第二大电影市场的第一天，神枪手觉醒了，他的门票收入估计为3.33亿美元，迪斯尼周六说。\\n',\n",
       " '《邮报》说，根据《邮报》第3章，王位的第三人不会感到更兴奋，因为他走路到了诺福克的蒙特梭利学校。\\n',\n",
       " '与此同时，国会和立法部门的俄亥俄州民主党人也在向国家环境保护署询问，几个月前，居民们发现了他们受铅污染的自来水。\\n',\n",
       " '如果他们曾经生活过《权利法案》，他们就会感到震惊和惊叹不已。\\n',\n",
       " '”20岁的 Kristlyn Whitlock 说：“我一定会来这里的，”20岁的 Kristlyn Whitlock 说，他穿着四层裤子和五层上衣，呆在这里取暖。\\n',\n",
       " \"该恐怖团体使用加密的信息服务电报向其他战斗人员提供咨询，说普拉卡什是殉道者，使用` shahada '来描述一名被杀的战士，据《先驱太阳报》报道。\\n\",\n",
       " '更广阔的世界是在意识到人们在孩子的出版中所知道的是什么——那是质量和产出正在得到更好的一年。\\n',\n",
       " 'Porsha Williams ----《亚特兰大的真正主妇》的模特、歌手和明星\\n',\n",
       " '伦敦政治电视连续剧《 Borgen 》( Borgen )和著名的丹麦电影( A - Hi - Jing )也是由 Tobias Lindholm 共同撰写的。\\n',\n",
       " '去年12月，这位容光焕发的瑜珈爱好者向《每日邮报》报道了她的个人与抑郁的私情，揭示了社交媒体如何拯救了她的生命，并激励她帮助他人。\\n',\n",
       " '据《南华早报》报道，65岁的李也是该公司的主要股东之一。\\n',\n",
       " '他是《华盛顿邮报》领导部分的一名客座撰稿人。\\n',\n",
       " '而这反过来又让人们看到很多 Instagram 帐户，然后走，“哦[...]我的生活看上去不像这样。\\n',\n",
       " '哦，除了他们是女人。\\n',\n",
       " '任何人如不提供身份证明文件，将被退回，而不遵守《守则》的人则会被罚款。\\n',\n",
       " '《推特》( Twitter )上周五在全球发生的一切事情中决定，这是最重要的“时刻”。\\n',\n",
       " 'Capternal 公司 AsraZninga 在英国雇用了6，700人，告诉《星期日泰晤士报》它在英国没有支付公司税，因为在2014年没有纳税利润。\\n',\n",
       " '在今年的第一个主要比赛开始时，一家名为《 Buzfuter 》和 BBC 的调查报告显示，这场比赛在网球中流行，在一次可疑的比赛中，他与大满贯中的单打冠军有牵连。\\n',\n",
       " '这位流行歌手后来在《星期六晚上》( LevideLive Live )的说唱音乐中回击，《周六夜》( LightyLive )上写的“击打着如此生病的说唱歌手”。\\n',\n",
       " '他在1970年代对《花花公子》杂志说他是双性恋者。\\n',\n",
       " '《与你的南侧》，由理查德?泰安撰写和执导，在 Sunde舞蹈电影节上首次亮相，这是与未来的妻子米歇尔·鲁滨逊首次约会的事实和虚构。\\n',\n",
       " '奥地利在帕施切尔霍夫斯特球场的壮观胜利开启了这个时代，在这个时代，“ BBC2”狂热的大卫·维和他挑选的色彩艳丽的滑雪礼服的评选主要由“ BBC2”的主导。\\n',\n",
       " '这项研究发表在美国心脏协会的《日刊》上，分析了1388名随机选择的参与者的咖啡和巧克力消费，其中不包括那些具有持久性的心跳加速的参与者。\\n',\n",
       " '这位游泳者据说是60年代的老人，当救援人员把他拖到一块木板上划船把他带到沙滩时，他似乎是无意识的。\\n',\n",
       " '当波特曼转过身来露出黑色的内裤时，这位女演员的面颊上皱起了眉毛，“黑天鹅”这件衣服是她所穿的。\\n',\n",
       " '1993年国际原子能机构(原子能机构)指控北朝鲜违反了《不扩散条约》，并要求视察员进入两个核废料储存地。\\n',\n",
       " '《圣经》中的《圣经》。\\n',\n",
       " '这本书的作者在《哈利·波特》系列中引用了坏人——“食死徒”——以他们“纯血”的热情而闻名。\\n',\n",
       " '魅力女孩：在海滩明星梅根.麦凯纳和前好莱坞女星斯蒂芬妮·戴维斯（ Stephanie Mckenn ）之前，她一直对这一事件产生了严重的吸引力。他去年被从肥皂中被解雇了\\n',\n",
       " '在由白人警察对黑人进行的一系列致命的警察枪击事件中，《黑人生命物质运动》已成为在美国根深蒂固的种族紧张关系的一个强有力的象征。\\n',\n",
       " '《 CCS 》的本·伍德预测说：“对于那些无法穿戴的科技，有一个非常男性的偏见，但是你会看到我所称的这些东西的珠宝首饰。”\\n',\n",
       " '根据《信息自由法》释放的《卫报》的数字，45名警察中的31人的比例高于黑人和少数族裔背景。\\n',\n",
       " '德克萨斯大学的科学家们给了四种不同饮食中的老鼠群体，他们发现，在6个月大的时候，吃蔗糖丰富的饮食的老鼠的一半以上已经患上了乳腺癌。\\n',\n",
       " '12月底，“黑寡妇”歌手在佛罗里达迈阿密度海滩度假，在一系列性感的泳衣上展示了她的曲线。\\n',\n",
       " '我们会在民意调查中看到：“哦，妈的，我们和女人在一起！”\\n',\n",
       " '在2011年，博士的东汉西罗博士在悉尼的家中吃了一个柠檬和蜂蜜茶，读读《圣经》。\\n',\n",
       " '“哦，天哪，我不知道她在这里，我感到很荣幸能在她面前打比赛，”威廉斯说。\\n',\n",
       " '这家剧院的租售人卡伯特告诉《纽约每日新闻》，房东费希尔兄弟昨天告诉他们，他们的租约不会再延长了。\\n',\n",
       " '《阿德莱德》中罕见的“紧急”30分邮票，价值几千枚\\n',\n",
       " '去年的电视颁奖典礼，《如何摆脱谋杀》和“透明父母”获得了更多的奖项。\\n',\n",
       " '她被丢在没有裤子的地上了，但还是活下来了。\\n',\n",
       " '雷伊，英国女演员黛丝·里德利在《星球大战》中的主角：“觉醒的力量”上周末在全球票房收入了10亿英镑\\n',\n",
       " '星期一，当他返回家乡时，他发现火焰熔化了他的船和车库，但房子却奇迹般地完好无损。\\n',\n",
       " '当奥斯卡提名在这个月出现时，人们很快就会抱怨《电影艺术和科学学院》没有为四种行为类别中的任何一个群体指定任何少数群体。\\n',\n",
       " '美国的运营商于今年8月同意为2016年700000美元的为期7700美元的捕鱼支付7700美元，这在所有船舶中间分开，这是《南太平洋金枪鱼条约》规定的临时协议的一部分，这是17个国家之间的一项27年的协议。\\n',\n",
       " '几个月前，热土把她的金发刮掉了，出演惊悚电影《疯人院》，2014年在影院上映。\\n',\n",
       " '野蛮劳工2，200万美元的《布兰德顿公寓计划》\\n',\n",
       " '副官们在客厅里聚集在海滩上集合，同其他地方的人没有区别。\\n',\n",
       " '不过，普遍的看法是，酋长应该在休斯顿得到支持，而在辛辛那提的钢铁公司，明尼苏达州的海鹰队，以及在华盛顿的打道者。\\n',\n",
       " '星期一在伊斯兰堡举行了一次重要聚会，有四个国家——阿富汗、巴基斯坦、中国和美国——希望为饱受战争摧残的阿富汗国家制定和平路线图。\\n',\n",
       " '《国土报》首次报道了这一事件，周四教育部的一份声明证实了这一事件。\\n',\n",
       " '甚至在电影《木兰》中，就有一位年轻的中国女孩把她的父亲从死亡中拯救出来，而男性人物则有76%的对话。\\n',\n",
       " '男演员 Rene Russo 在10月在林肯广场剧院参加了《夜车》纽约首演。\\n',\n",
       " '这就是为什么我们呼吁修改《法案》，以加强委员会的作用，并使它负责编制官方预测。\\n',\n",
       " '哦，人类生命的变化无常！\\n',\n",
       " '图像版权：迪士尼/卢卡斯电影图像字幕英国组合约翰博加和戴西·里德利在《星球大战》中扮演主角：“神力觉醒”\\n',\n",
       " '根据《公司法》第7211( c )条，严格禁止董事出席会议或投票。\\n',\n",
       " '由于激烈的讨论，戈德伯格对那些认为本剧《碧雷》的人做出了回应。\\n',\n",
       " '据《 Buzzpower 》报道，一名球员涉嫌参与了这场被指控的“匹配”球拍，它使世界网球失去了一系列的比赛，而这一比赛的几率估计不到7，500美元。\\n',\n",
       " '富含蛋白质、关键营养素，包括锌以及20种其他的维他命和矿物质，鸡蛋很容易被纳入任何一餐，并在几分钟内做好准备。\\n',\n",
       " '在袭击中，圣战分子在讽刺的《查理·黑本》杂志上杀害了12人，4名人质被杀在一个犹太超市和一名女警察。\\n',\n",
       " '“这家伙做得很好，所以我想记录下，”诺里斯克告诉《每日新闻》。\\n',\n",
       " '当晚最响亮的掌声是海伦?米伦，他以《微波奖》的罪名，介绍了她的 Trumbo 共同出演的布赖恩·克朗顿。\\n',\n",
       " '在白天的戏剧中被提名为顶级演员是 Susan Flannery 和希瑟·汤姆，《我们的生活和年轻的米歇尔·斯塔福德》。\\n',\n",
       " '《今日道德》杂志编辑饮料?考迪( Beverin Cuddy )告诉利物浦回声：狗主人需要帮助他们确保他们“更好地了解狗”，并建议他们参加处理会议。\\n',\n",
       " '你正在梦想赢得一个《魔戒》的经典，也许是金色的 Slipper 。\\n',\n",
       " '在《查理·黑布袭击》的10个月之后，“巴黎”再次成为袭击目标\\n',\n",
       " '该研究小组在《科学》杂志上报告了新研究结果。\\n',\n",
       " '这些彩色的照片显示，在科罗拉多（ a ）和科罗拉多（ b ）东部的恐龙恐龙岭的邓肯公路上的达科塔山岩中，有来自达科达石的痕迹。\\n',\n",
       " '5名股票经纪人（包括一个名叫 Libor 勋爵的股票经纪人）已经清理了计划，以调整银行利率，以赚取几百万英镑的利润。\\n',\n",
       " 'UFC 主席达娜·怀特在接受《泰晤士报》采访时说：“美国女性减肥冠军霍莉·霍尔德（ Hollar Holm ）已经签署了一项多年合同延续合同，其中包括“终极战斗冠军”。\\n',\n",
       " '他还选择了世界上最伟大的 DVD 作为他的奢侈品，并选择了史蒂文·皮克斯的《更好的天使》作为他的《我们的自然》。\\n',\n",
       " '哦，罗科。\\n',\n",
       " '韩国政治家李齐宇说，他被国家情报机关介绍说，爆炸的“可能”可能会导致氢气爆炸。\\n',\n",
       " '科多创造了内衣、袜子、袜子、短袜、裤子和裤子的特殊折叠技术，以及关于在衣柜里挂衣服的命令的特殊说明。\\n',\n",
       " '有些人很受欢迎，所以需要等待名单和彩票，而这些名单都是用来填饱肚子的。\\n',\n",
       " '哦，我的胸膛嗡嗡响，耳朵在燃烧，我正经历着一种充满自信的幻想。\\n',\n",
       " '“感觉就像我是肉店的时候，”她说，“然后她不假思索地说，‘圣诞节时，我就会把火鸡脖子上的火鸡背在笼子里。”\\n',\n",
       " '他说，新设施及其广阔的方案规划是他确保云门的蓝图的一部分。\\n',\n",
       " '\"在2013年， Vardalos 写了一本书《即时妈妈》，内容是一对夫妇在24小时内的通知中成长为父母，并\"\"传播收养信息，并努力让一些孩子通过\"\"。\"\\n',\n",
       " '当地的 Idris Assoumana 一天在海滩上与辛普森-肯顿交谈，他说他对在一个豪华的天台派对上的新的一年表示欢迎。\\n',\n",
       " '至少默多克并没有抨击保守党，正如《时代》所做的那样，谷歌前宣传部瑞秋·怀特斯通是凯尔特的长子的教母。\\n',\n",
       " '我从来没有意识到这一点，而不是我在写我的最新小说《魔戒》的时候。\\n',\n",
       " '如果她在《卖火柴的女孩》里着火了，你必须把自己绑在桅杆上，以抵抗她那阶段的疯狂。\\n',\n",
       " '1月29日发行了我的《小甜甜》，价格为6.88英镑加运费。\\n',\n",
       " '《承诺101》。\\n',\n",
       " '这位神秘人的地下洞穴中透不过气来打断他，他认为他是臭名昭著的琼斯先生，他是一位隐居的艺术家，他的形象是用贪婪的华丽的稻草人给随机的公民提供。\\n',\n",
       " '英国女杂志记者苏哈代，63岁，是英国许多游客在海滩时的戏剧开始展开。\\n',\n",
       " '墨西哥位于墨西哥坎昆的白色海滩和清澈的水域中，以42.1%闻名。\\n',\n",
       " '2016年《金球奖》：全球冠军得主\\n',\n",
       " '这家餐馆将供应著名的陀螺三明治和鸡和饭。\\n',\n",
       " '粉丝们倾向于认为鸭子是家禽世界的牛排，因为它的味道比鸡肉的味道要强烈得多。\\n',\n",
       " '在2015年的电影节上，杜兰·杜兰演出了最伟大的热门单曲，以及《化学兄弟》的电影《 Missy Elliott 》的回归。\\n',\n",
       " '在接受《每日邮报》采访时， Fury 将同性婚姻比作恋童癖，此后英国广播公司给他的荣誉，使他加入了体育年运动的性格的短名单。\\n',\n",
       " '《劳工领袖法案》（ Bill Shorten ）公布了一套新政策措施，旨在保护澳大利亚工人不被那些傲慢的老板解雇。\\n',\n",
       " \"2004年10月，剑桥公爵夫人凯瑟琳出席了在 L ' Anima 《秋夜狂欢》晚会上的表演。\\n\",\n",
       " '去年，他的美国出版商《新方向》（ New DHS ）在全国6个城市的巡回演讲中，将他带在全国各地。\\n',\n",
       " '但与在中国开展业务规模较大的美国帐蓬电影不同，新的“星球大战”显然不会影响这部电影的美国非同寻常的赛事，预计这个周末的《星球大战》将超过8亿美元。\\n',\n",
       " 'Filali 的死亡引发了街头示威，最终导致摩洛哥议会一致投票，废除了《刑法》第475条。\\n',\n",
       " '在《 JJ Abrams 》的这部影片成为全美最畅销电影的新闻之后，这一壮举得以实现。\\n',\n",
       " '5.在日本，请看《卡罗尔》的《》\\n',\n",
       " '《网络安全挑战》团队一直在追踪调查悉尼大学，直到最后五分钟的比赛中正确的答案逐步淘汰了悉尼大学的竞争者。\\n',\n",
       " '为了确定这些基因，他们在两个物种中的杂交鸡（图片），然后在新的环境中测量它们的焦虑程度\\n',\n",
       " '妇女被建议不穿制服，穿迷你裙无袖衣服，穿短或紧身的公共服装。\\n',\n",
       " '水系统，哦，上帝，我的意思是你知道，让我们不要进入弗林特，密歇根州和那里发生的恐怖。\\n',\n",
       " '诺兰曾担任斯奈德的第一部超人电影《钢铁侠》（2013年）的共同作家和执行制片人，并继续以“咨询能力”参加即将到来的超级英雄的 mashup 。\\n',\n",
       " '他更接近电影《涅磐》，在美国和中国这两个最大的市场上生产真正的全球电影。\\n',\n",
       " '约翰·道格拉斯·汤普森在《沃尔多夫语》中的“ Satchmo ”： Long Whars 剧院和莎士比亚戏剧公司生产公司\\n',\n",
       " '布鲁皮对《福布斯》说，“怀疑是准确的，特别是与黑莓的联系”。\\n',\n",
       " '达伦·海滩写道：“ Ryman 联赛总理司可以自豪地认为，它有两个靠泊，有些人没有被提升、降级，也没有被强迫迁移到几十年。\\n',\n",
       " '\"苏格兰政府将对《土地改革法案》进行49次修订，因为这要求更多的\"\"激进\"\"。\"\\n',\n",
       " '乔治·奥威尔，《九十四年》\\n',\n",
       " '使用彩色铅笔和遮荫技术，他然后填补了空白，放慢了他的幻觉的生活。\\n',\n",
       " '《英国国民保健服务计划》每年都有8个病例，其中许多人患有心脏病或中风。\\n',\n",
       " '她后来与她在苏格兰的家庭定居下来，她的第四部小说《恩密密语》开始了。\\n',\n",
       " '这位美国女人游客——被我跟她谈话的一位职员描述为‘非常漂亮’——她和丈夫一起沿着海滩散步了。\\n',\n",
       " '两年前，我说了一些关于《教程》非常白人男性的事，这是现实，而我被媒体用粉碎了。\\n',\n",
       " 'Topeeka Capital Journal 报告称，米歇尔·霍尔姆斯11号行为准则中的一项规定，“低领的领口和迷你裙”对女性来说是不合适的。\\n',\n",
       " '更多的人在论坛上阅读枪支文化，而不是阅读《纽约时报》。\\n',\n",
       " '《华尔街日报》财经编辑丹尼斯·贝尔曼参加了“ CBS 今晨”，讨论中国的经济担忧和对美国市场的担忧。\\n',\n",
       " '由蒂姆·兰登《每日邮报》\\n',\n",
       " '纽波特海滩\\n',\n",
       " '专家认为，朝鲜可能刚刚试验了一种混合动力混合装置，它将氢同位素混合在正常的原子裂变材料中。\\n',\n",
       " '据《印度时报》引用的一份内部政府报告称，印度边境安全部队(边境安全部队)本周承认，“电子监控设备在国际边界和电子监控设备上出现故障”。\\n',\n",
       " '没有一个球队想要承认对方的优势，所以，毫不奇怪，维京人和海鹰队一直低调行事。\\n',\n",
       " '贾斯汀·丘采尔的《麦克白》的电影展现了一种对人类现状的不光彩的真实愿景，这与当时在人文学科的学术研究中的任何内容都不一样。\\n',\n",
       " '这位81岁的澳大利亚人对《电台时报》说：“因为里面没有黑人。”\\n',\n",
       " '卡梅伦在《星期日泰晤士报》中写道，“这里的使命并不短些，那就是社会的转变。”\\n',\n",
       " '《古兰经》——伊斯兰教的圣书——禁止穆斯林吃猪肉，而猪也被用来嘲讽或冒犯穆斯林。\\n',\n",
       " '但在发生了67点的伤病之后，只有4899人参加了针对鹰队的第18轮主场比赛。\\n',\n",
       " '不过，尽管这个想法可以让世界上一半的创意大厨们在愤怒中咬牙切齿，但在《标准》的阿尔玛可能是一个比阿尔玛那更好的餐厅：更加稳定，节奏更平衡，更有趣。\\n',\n",
       " '同时，在高温下放到平底锅里煮3到4分钟，然后把这些鸡肉切成薄片。\\n',\n",
       " '李丽雅在《斯德哥尔摩》的《巴巴派对体验》上还说：“ SOS ”，她在《斯德哥尔摩》杂志上的演讲中写道：“ SOS ”\\n',\n",
       " '唐纳德·特朗普的儿子们在爱荷华州的核心小组中捕食野鸡：“它们是坚硬的。”\\n',\n",
       " '苏珊的女儿伊娃·阿穆里，右，也选择了裸照，尽管她胸部稍小一点，胸部也穿着一件廉价的蓝色裙子\\n',\n",
       " '《哈利波特书》的插图是作者亲自画的。\\n',\n",
       " '美国感到不安的是，南非将其视为对美国鸡进口的不公平限制，而不是美国对禽流感和沙门氏菌的担心。\\n',\n",
       " '科尔特将伊鹰扣留到了212码和13次，建造了44-3，领先3个季度。\\n',\n",
       " '罗莉·马戈利斯：《建筑文摘》中的 Rick Stil / Congal 的摄影\\n',\n",
       " '迄今为止，在冲突中没有使用氢弹，但世界核武库中大部分是此类武器。\\n',\n",
       " '12月，英国《国家贸易标准》称，自2015年10月以来，该机构所检查气垫板的88%可能爆炸或起火。\\n',\n",
       " '她用浅粉色的毛衣和黑色的牛仔裤，以及简单的黑色芭蕾舞鞋配合看\\n',\n",
       " '然而，拉玛的《蝴蝶甲》获得了更多的提名——比其他任何艺术家都更多。\\n',\n",
       " '《 Trains Dean Dead 》一书的作者麦克·迈克尔·威廉斯说：“有一种神奇的东西，吃着吃过多少小桌的小桌布。”\\n',\n",
       " '在他们苏联的修正主义和大量的突击步枪上，你有相当多的鸡尾酒。\\n',\n",
       " '《爱尔兰新闻报》报告说， AA 告知他的手下，但他们没有理会他，但没有及时采取行动，也没有允许袭击行动。\\n',\n",
       " '据《纽约时报》上周六报道，迈克尔·布隆伯格( Michael Bloomberg )已经告诉他的助手，他的助手们将为美国总统大选制定一项独立竞选计划。\\n',\n",
       " '\"《福利上限》限制了工作年龄的失业人员每周5500英镑的福利，其依据是，它向\"\"无工作家庭\"\"发出了一个强有力的信息，即他们必须更努力地获得工作。\"\\n',\n",
       " '《维也纳公约》不应涵盖从事犯罪活动的特使。\\n',\n",
       " '根据1973年《公共记录法》第9款密封的旧唱片，用打字机的打字机打出墨迹，用墨迹涂写。\\n',\n",
       " '在去年2013年的《神怪圣经》中，日本成为第一个从日本赢得年奖的《世界基督教年》，这让人印象深刻。\\n',\n",
       " '他的公司《铜锣湾图书》将在他上台之前出版一本关于中国国家习近平所谓的爱情事务的书。\\n',\n",
       " '当原子弹仅仅使用核裂变（原子的分裂）时，氢弹使用裂变作为先引爆来引爆核聚变反应（原子的组合），产生难以置信的能量。\\n',\n",
       " '由于《新闻自由法》的《新闻自由法》，国务院副新闻记者贾森·利奥波德提出的诉讼要求国务院在1月底前完成约3万名克林顿电子邮件的出版工作。\\n',\n",
       " '更衣室里的同事们不经意的给詹姆斯出了错误的短裤，詹姆斯在首季的时候穿着不同于他的队友。\\n',\n",
       " '据《卫报》所知，最初没有父母或老师提出证据。\\n',\n",
       " '《电视连续剧》中的最佳男演员： Mad Rami Malek ， Robot Wagner Moura 先生， nternos Bob Odenikik 先生， SelloLiev Schreiber ， Ray Donovan\\n',\n",
       " '这个庞大的造纸厂曾经生产新闻纸，但改变了时代的意思，改变了市场——近年来它的一个最大订单是《情色小说》的报纸，《黑桃50黑桃》。\\n',\n",
       " '奥斯卡·艾萨克，《给我看英雄》\\n',\n",
       " '去年7月至9月的官方数据显示，内阁办公厅（其数据包括根据《法案》向10名唐宁街提出的调查）收到了497项请求。\\n',\n",
       " '欧洲委员会最初的副总统弗兰斯·蒂默曼（ Frans Timmermans ）透露，在12月来的人中，60%的人不是逃离饱受战争摧残的叙利亚的难民。\\n',\n",
       " '在这一点上，她患有黄疸，她的皮肤和白人的眼睛变成黄色，医生说医生在杂志《 BMJ 案例报告》中描述了她的病情。\\n',\n",
       " '据报道，斯皮尔伯格试图说服《威利·旺卡》明星威尔德（ Willy Wonka 的明星）从退休后出来参加 BFG 。\\n',\n",
       " '在秋季的声明发表后，他向议长投掷了毛泽东的《小红皮书》的副本，并拖出了一个几乎让人恼火的改组，仅仅集中关注工党内部的垄断，以及保守党政府所推行的有缺陷的政策。\\n',\n",
       " '这位领导人认为，当《卫报》在《卫报》发表后，他试图质疑科伯恩的权威。\\n',\n",
       " '一位忠实的粉丝成功说服了许多军事制造商给他的《星球大战》带来实质内容——并且创造了一个真正具有战斗力的复制品。\\n',\n",
       " '《 Trumbo 》杂志《黑屏》电影剧本作者 Dalton Trumbo 的传记，在影片中出演了四个提名，其中包括一位明星布莱恩·克兰斯顿的最佳演员和海伦·米伦的支持女演员提名。\\n',\n",
       " '《花花公子》，1985年\\n',\n",
       " '妈妈组：新的 CBS 展示了《从地狱中的天使》中的天使\\n',\n",
       " '把芹菜的心在鸡窝里煮大约20-25分钟，然后在存货中冷却。\\n',\n",
       " '与爱玛·多诺万在《星顿日落》系列中的城市，与爱玛·多诺万和推杆一起，得到灵魂的充实。\\n',\n",
       " 'Gisele Bundchen 在1999年9月的《时尚》杂志的封面上出现。\\n',\n",
       " '有关：《查理·黑布》周年纪念版的发行：“凶手还在外面”\\n',\n",
       " '扇贝在热锅里浸，加入香菜、姜和鸡丁的香味。\\n',\n",
       " '在《 Vearncomplan 》上有35年的音乐生涯，并于1981年发布了他的第一个带有黑人、人类特征的音乐。\\n',\n",
       " '据认为，获胜的门票可能是在 Warnont 郊区的《犹太新闻报》中购买的\\n',\n",
       " '2013年，《德国语法手册》发布了一套适合11张光盘的布莉贝的完整作品。\\n',\n",
       " '我现在复制了像西斯汀·麦当娜、金星诞生和在他的赛车中的 Phootus 的绘画，前南斯拉夫的《极光》和《晨星》。\\n',\n",
       " '《星球大战：力量觉醒： EMO Kylo Ren 》和“非常孤独的路加福音”在 Twitter 上有了一个史诗\\n',\n",
       " '在《卫报》的一封信中，这6个俱乐部致信 BFA ，要求根据安全理由要求他们的火柴被暂停，因为该国的安全状况。\\n',\n",
       " '但是，在《勒布勒斯》中发生的事情仅仅是第一步，是一张意向书。\\n',\n",
       " '当我穿过约翰·欧文的第14部小说《神秘大道》时，我不禁回忆起这次谈话。\\n',\n",
       " '他从那时起就成功地指导了成功的恐怖片《易趣》和《变形金刚》以及《快速和疯狂的特许专营盘》中的最新电影。\\n',\n",
       " '《西雅图时报》报道说，沃克是西雅图公园基金会的创始成员，他也是西雅图公园基金会的创始成员。\\n',\n",
       " '《快乐》明星 Dianna Agron 和 Mumford & Sons 歌手温斯顿·马歇尔订婚了。\\n',\n",
       " '韩国于本周五早些时候宣布，北韩成功地测试了氢核反应装置，因此恢复了对平壤的宣传。\\n',\n",
       " '根据《濒危物种国际贸易公约》和其他群体的规定，尽管贸易禁令，但偷猎者每年都会杀死数以万计的大象，以满足对材料的需求。\\n',\n",
       " '两位修女的发言人告诉《卫报》说：“一旦我们意识到这一情况，我们就采取了迅速和果断的行动。\\n',\n",
       " '杰米-李·柯蒂斯在夏威夷度假，在海滩上的泳装上发现了这一点。\\n',\n",
       " '自2010年奥巴马签署《平价医疗法法案》以来，共和党议员已经采取了多项废除措施。\\n',\n",
       " '英国是《罗马规约》的签署国，《罗马规约》于1998年设立了国际刑事法院，这意味着，如果英国未能适当处理指控，国际刑事法院可以自行展开调查。\\n',\n",
       " '“投标将于今年11月公布，明年年底或明年初发布，这意味着在凯恩斯的当地工作，”帕斯卡祖克总理于2015年9月告诉《凯恩斯邮报》。\\n',\n",
       " '我们热切希望了解为确保《守则》明确反映公众有权期望的标准而作出的变化。\\n',\n",
       " '2010年，通过下载和分发儿童色情制品，《 Vembe 》违反了他的释放条件。\\n',\n",
       " '在《教父》中，阿部与罗伯特·杜夫尔（ Robert Dufall and JoeSpotell ）一同出演《教父》（1972年）。\\n',\n",
       " '《星球大战》原创电影的创作者卢卡斯在2012年以4.05亿美元的价格出售了迪斯尼的专营权。\\n',\n",
       " '根据《信息自由法》获得的数据显示，自2011年以来， Dyfeed - Grains 已向埃塞克斯法院5个法庭支付67943英镑。\\n',\n",
       " '但是，进入政府后，政府以一种类似于马弗诺的黄色长筒袜在另一个经典的英语季节娱乐，莎士比亚的《第十二夜》上的屈辱而告终。\\n',\n",
       " '《新农场上的光电影院》放映电影鹅。来源：供应：供应\\n',\n",
       " '俄克拉何马高速公路巡警上尉保罗·蒂蒙斯说：“广阔的地形，加上周末猛烈的风暴，阻碍了救援工作。”\\n',\n",
       " '地震仪表明，在周二晚上朝鲜不是一次地震，但它可能不是氢弹，尽管。\\n',\n",
       " '信贷：《草》中的壮观景象\\n',\n",
       " '根据《卫报》的说法，迈克尔爵士说：“我们需要的——因为现在的经济与我开始教书时截然不同——是为了让更多的年轻人做比以前更好的事情。\\n',\n",
       " '阿德里安·莱利耶在2015年5月18日在迈阿密的迈阿密海滩游泳。\\n',\n",
       " '血液中浓度较高的人，血液中的碳饱和脂肪酸含量较高，患心脏病的风险较高。\\n',\n",
       " '它以两个晚上结束的海滩城市吉巴科亚，那里有石灰岩梯田和珊瑚礁进行探索。\\n',\n",
       " '当地的渔民和营救人员设法将一些鲸鱼拖回海里，但是动物们却一直在海滩上洗衣服\\n',\n",
       " '1个芹菜，加一些500毫升的蔬菜或鸡肉，或者一个好的立方体8-12切片的空气干燥的火腿盐和新鲜的\\n',\n",
       " '《2016年人民选择大奖》出来了\\n',\n",
       " '由《每日邮报》经济通讯员雨果·邓肯\\n',\n",
       " '本评论文章最初出现在《纽约时报》上\\n',\n",
       " '《觉醒力量》是只有24部电影中的一部，打破了1，100亿美元的标记，目前是第六片历史上最成功的电影。\\n',\n",
       " '霍克1月份对《纽约时报》说：“当我扮演这一角色时，我不想感觉像我一样。”\\n',\n",
       " '在海滩上站出来的汉姆娜·汉格斯\\n',\n",
       " '《新闻集团》( NewsCorp .)政治编辑女士( Mon Maiden 女士)在写一篇批评他的同事 Briggs 的专栏后，收到了一位部长的一份短信，她的文章是“疯狂的 fad ****女巫”。\\n',\n",
       " '天气造成的危险冲浪环境关闭了许多受欢迎的东郊海滩，包括塔拉马马，那里的强降雨和泛滥的暴雨水排看到污染水平激增。\\n',\n",
       " '据《华尔街日报》上一份报告指出，首席执行官和共同创始人 Jack Dorsey 是该项目的先锋，代号为“140+”。\\n',\n",
       " '玛丽亚·加西亚：《公报》，公报\\n',\n",
       " '我们正在调查被解救儿童的诉求，”《纽约时报》援引 Ashwani Kumar 的话说。\\n',\n",
       " '在网上出现反弹之后，有一个观众对电影“道德败坏”的抱怨不已，影片《奥斯卡奖得主埃迪·雷德梅恩》（ EduconReRedmayne ）从影院上映。\\n',\n",
       " '\"边境被禁止的\"\"边境\"\"、\"\"炸坑\"\"和\"\"炸鸡\"\"和\"\"加洛伊\"\"路线\"\\n',\n",
       " '在2014年3月6日， LightonMeester 在纽约参加了百老汇的《魔门和男人》。\\n',\n",
       " '方式:穿上这个开放式的和宽松的针织或一条裙子和长统靴。\\n',\n",
       " '《战争与和平》包含了浪漫与戏剧的结合，它被认为是一个漫长而又费力的小说。\\n',\n",
       " '最近，她在《 Forger 报》(《 Forger 报》(2012年)中主演。\\n',\n",
       " '这位备受爱戴的儿童作家于1943年逝世，而他的《布蒂·布斯》却未完成。\\n',\n",
       " '这部电影是《印度商业报道》的帕梅拉· Koh 制作的。\\n',\n",
       " '一只白狗在臭烘烘的沙滩上欢蹦乱跳地滚着。\\n',\n",
       " '图中的是南部陆地角法案（ Bucorvus b铅betateri ），这是来自非洲的一个大型鸟类，从《国家地理》杂志，到2016年2月。\\n',\n",
       " '最后有人说，“火鸡在哪里?”\\n',\n",
       " '1971年（《记录》）。\\n',\n",
       " '在《聚焦》的主题上，它是最受欢迎的电影之一，因为它在秋天电影节上首次放映。\\n',\n",
       " '《每日镜报》说：“最后”一些好消息，正如它预测的一年的廉价汽油价格。\\n',\n",
       " '他们把它叫做锡克（ Cake Tin ）牌，但威灵顿的 Westpac 球场变成了一个湿漉漉的、粘稠的松鸡。\\n',\n",
       " '演员阿伯·维索达，在《1975年的巴尼·米勒》中饰演的侦探费尔（ Phelfish ）中扮演了角色。\\n',\n",
       " '不过，雷德利在2014年为其剧本拍摄了《12年奴隶》的电影剧本奖，他强调说，为更反光的好莱坞行业而进行的斗争远远没有结束。\\n',\n",
       " '《愤怒之路》导演乔治·米勒:“我不会再做你的疯魔电影.”\\n',\n",
       " '位于大洋洲的海滩之家，实际上拥有葡萄园，并在当地生产自己的葡萄酒。\\n',\n",
       " '“任何与英国国民保健系统有关联的企业都应该支付他们应得的税款。”她告诉《独立报》。\\n',\n",
       " '“哦，哇，我赢了600场比赛了?”她说。\\n',\n",
       " '《磅法》显示出意识形态可能起作用的地方。\\n',\n",
       " '《2016年金球奖》： Ricky Gervais 的最好、最差和最冒犯性笑话\\n',\n",
       " '这涵盖了该国几乎90%的生产鸡蛋，其中包括来自主要超市的鸡蛋。\\n',\n",
       " '接下来，亚马逊从《纽约客》杂志推出了一个没有脚本的系列。\\n',\n",
       " '在《哈利·波特》和《诅咒孩子》中扮演赫敏的黑字演员遭到了批评\\n',\n",
       " '代表们坐在色彩鲜艳的气球上，参加了2016年在伦敦西部举行的玩具展览会。\\n',\n",
       " '但那些含有较多饱和脂肪酸的15或17，与食用乳制品有关，在2型糖尿病中风险较小，同样的模式也出现了。\\n',\n",
       " '在本周末《华盛顿邮报》的专栏文章中，著名学者 Fareed Zakaria 指出，这个群体对他们意识到难以改善其地位而深感震惊，因为他们的孩子的生活可能不会比他们自己的生活更好。\\n',\n",
       " '在纽约，要看《电台城》节目是圣诞节的一个仪式。\\n',\n",
       " '吃鸡尾酒香肠时需要紧急手术，因为它故意塞住钉子。\\n',\n",
       " '他还提到了《锡安长老的议定书》--一1909年的案文，目的是描述一个已经被认为是编造的犹太全球统治计划。\\n',\n",
       " '它不是在阿尔伯塔省的草原上，而是消失在辽阔的渔溪公园里，在加拿大落基山脉陡峭的山脚上消失了。\\n',\n",
       " '《先驱报》的调查记者 Kate McClalmont 将把著名的澳大利亚电影导演 Bruce Beresford 置于聚光灯下。\\n',\n",
       " '《星球大战：神枪手觉醒》作家劳伦斯·卡斯丹说韩圆的拍电影不会是一个起源的故事\\n',\n",
       " '尽管她的父亲在他的职业生涯的巅峰时期的电影中表演并演唱了《科尔号》的《科尔号》，但直到90年代才40多岁时，科尔才开始认真地表演。\\n',\n",
       " '一位教区职员弗兰克·帕特里奇( Frank Parrichard )把25公斤的车从 Pentath 海滩拽出来，然后用手推车把它带回家。\\n',\n",
       " '接着，埃弗隆就吃了药，赤身露体地躺在沙滩上，脸上带着血渍斑斑的彩旗，而他的未婚妻则绝望地担心他是否会及时把它赶回来举行婚礼。\\n',\n",
       " '该书自2012年在美国出版以来，仍是100强的亚马逊自助畅销书排行榜。\\n',\n",
       " '《流星雨》的《流星雨》中说，悉尼的 SiemeSixers 可以从他们的轰轰烈烈的连胜浪潮中驾驭所有的势头，以达到首次女性大博客的冠军头衔。\\n',\n",
       " '一些消息人士声称，该项目有一套“船期”，2019年，但《华尔街日报》指出，这一日期可能是工程师完成设计的目标。\\n',\n",
       " '俄亥俄州的父母休息2岁过后，在 Toillet\\n',\n",
       " '《走出困境》的时间表：关于 Screenfs 的电台 Mysteries 和 Thrluter\\n',\n",
       " '“宽阔的林荫大道和伟大的公民空间”：克里斯托弗·雷恩重建伦敦的计划。\\n',\n",
       " '她从腰围丢了三英寸，从14号裙子到12点。\\n',\n",
       " '住宅占地4，910平方呎，占地4，910平方呎，面积广阔，有一个以天花板为天花板的窗户，与世纪城及整个威尔郡大道走廊相连。\\n',\n",
       " '《商业杂志》对《商业杂志》的研究结果显示， Cadbury 公司的复活节生活中，这是其中一个主要的价值1000万英镑的主要因素。\\n',\n",
       " '她最近刚从洛杉矶回来，做了很多的电影导演，并且一直在将她的方式引入好莱坞的研究（今晨的家庭作业： Scorsese 被忽视的作品《爱丽丝不在这里生活》）。\\n',\n",
       " '但也有希望：两年前，《 Alfonso Caaron 的重力》以90分钟的运行时间证明，一部史诗电影的放映时间并不长。\\n',\n",
       " '前 WADA 的老板约翰·法布雷对《 Essendon 34》的上诉\\n',\n",
       " '关于瑜伽裤的争论影响了它的形象，并导致股票暴跌。\\n',\n",
       " '6月25日，《家庭暴力法》取消了克拉克的汽车驾驶执照，原因是医疗原因。\\n',\n",
       " '其中一名妇女和他在一起的厨师和他在一起的时候，他在几年前被拘留的时候，根据《普里莫诺通知》的说法。\\n',\n",
       " '在《法案》没有保留迈克尔·汉林后，她填补了这一职位。\\n',\n",
       " '悉尼交响乐团的《图雷》帮助老师们发出了学生的共鸣\\n',\n",
       " '《国家科学院报》的《论文集》报道，猫科动物的成员们以微弱的速度闪动了狗。\\n',\n",
       " '据目击者艾哈迈德·努尔的目击者艾哈迈德· Nur 说，他们随意向靠近海滩的人开火，他们在海滩上散步时正在沿着海岸线散步。\\n',\n",
       " '麦考伊·库金在2013年10月纽约公关店期间参加了机器人鸡小组的谈话。\\n',\n",
       " '在冈比亚一个海滩工作的供应商。\\n',\n",
       " '在接受《星期日泰晤士报》的采访时，麦法登反驳了他关于恐怖分子动机和与英国外交政策的联系的观点。\\n',\n",
       " '《每日星报》报道，这所房子有五间卧室、三个卫生间、三个接待室、一个游泳池、甚至是泰晤士河上的一个锚地。\\n',\n",
       " '2014年，一群研究人员将假尾巴绑在鸡身上，分析他们走路的方式，以了解“ T - Rex ”怎么可能在6，600万年前就已经有了失足。\\n',\n",
       " '南希曾开玩笑说：“肉毒杆菌、整形手术、滴鼻虫和鸡群——它们绝对是一件好事，只要你用它们来表达自己。\\n',\n",
       " '与随机男性的鸡尾酒，他们的工作不完全理解剩下的时间。\\n',\n",
       " '三年前，她获得了同样的奖金，她在高中的榆树乐队的榆树下获得了《欲望》，但她承认，在她获得国家部分之前，在一年被拒绝后，她几乎放弃了行为。\\n',\n",
       " '《纽约时报》杂志的国家通讯员 Mark Leibovich 提出的一些想法：\\n',\n",
       " '就像他的第一部小说《美好的谎言》的主角，尼古拉斯·瑟勒是一个神秘的国际人。\\n',\n",
       " '他现在警告说，“危险的新威胁鸡尾酒”，在英国的经济和政治辩论中，“日益自满”。\\n',\n",
       " '虽然财政部一直在提醒其他部门几十年来勒紧裤腰带，但在海沃德看来，它自己的行动太缺乏重点和等级森严。\\n',\n",
       " '他还在新加坡的 Marinana Bay Sands 上看到了自己的第一个天台鸡尾酒，第一次看到了一些肉的野生动植物（包括在 Lombok 的野生猴子阻塞道路），并在巴厘岛通过寺庙散步，吸收了一个完全外国的文化。\\n',\n",
       " '《在一个头巾中的爱》一书的作者 Shelina Janmohamed 在接受 BBC 采访时说，她被卡梅隆所报告的言论所触怒。\\n',\n",
       " '《动物之家》和《巴比伦第5号》的演员斯蒂芬?赫斯特( Stephen Furst )公开发布了他们所提议的会员资格和规则变化的奖项。\\n',\n",
       " '来自 Ebay 的“绿荫”电影\\n',\n",
       " '在2006年的演出中赢得了国际社会的巨大成功，并释放了她的第一个单曲《像这样的一刻》。\\n',\n",
       " '他的第5部《斯科塞斯》的电影《德尼罗报》展示了他的男高音-黑色喜剧的才华。\\n',\n",
       " '在《夜标准》的一个联轴节中，这对他们的动机是明确的：“我们的2016年使命是帮助伦敦的所有收入在住房的阶梯上找到一个立足点。”\\n',\n",
       " '在过去的四年里担任过法官职务的特许会计师，曾在《魔法百万人》中长期任职，他在上月退休后，将这份工作作为赛马的顶头狗。\\n',\n",
       " '据太阳报道，这名女子据说是29岁，当时穿着粉色上衣和深色裤子，在寒冷的夜晚，穿着粉红色的上衣和黑色裤子的汽车在一辆汽车里，两人都在拍摄。\\n',\n",
       " '凯特·赫德森摇晃了一件作物顶部和相配的裙子。\\n',\n",
       " '她自己的研究，发表在同行评审的《心理科学》期刊上，支持这一说法。\\n',\n",
       " \"黎巴嫩男子展示`以色列间谍秃鹰'，而`监视他们'\\n\",\n",
       " '这位演员也对他在《滚石杂志》发表之前同意将他的文章交给 Guzman 的说法而不屑一顾。\\n',\n",
       " '因此，有大量的黑鸡汤，有人参和其他草本植物的味道，凝重和闪亮，集中而纯净。\\n',\n",
       " '在电视戏剧表演中扮演优秀女性演员的电影《演员协会奖》之后，欧维娜·戴维斯在幕后评论了好莱坞的多样性问题\\n',\n",
       " '他说：“哦，天哪，那是不可思议的！”\\n',\n",
       " '据了解，据《每日电讯报》，年龄在18岁和32岁之间的《每日电讯报》，在本周早些时候在沃尔姆森的球场附近拍摄。\\n',\n",
       " '联合王国公司将停止运作并将其执照/许可证交还《公平竞争法》。\\n',\n",
       " '在对他提出强奸指控后，《劳动议员 Simon Danczukk 》正面临警方的调查。\\n',\n",
       " '奥兰多魔术队在伦敦的2月9日在伦敦的 OOB 主办了《 O2报》。\\n',\n",
       " '来自阿拉巴马州和普林斯顿的毕业生，罗杰斯在 M . A . S . H .之前曾有过许多短命的演出，他在西部片中的专长，如《原告》和 Stagreech West 的法律。\\n',\n",
       " '但我回到了10月下旬的放映电影《大短剧》，由制片人林达·比斯特和导演彼得·博丹迪维奇主持。\\n',\n",
       " '在他通过第三方向冈萨雷斯传递的课文中，《 El - Chapo 》写道：“我根本没有睡过觉。”\\n',\n",
       " '该公司的人力资源总监马克?西蒙斯( Simmons )对《卫报》说：“我们在欧罗顿的豌豆加工设施在旺季时雇佣了大约150人，其中大约一半是 Jark 提供的代理员工。\\n',\n",
       " '《时代周刊》摄影师 Jay L . Clendenin 的所有照片。\\n',\n",
       " '现在，它又回到了生活，酒吧和咖啡店，新一代的创新者，还有自己的海滩\\n',\n",
       " '麦基森（ Mickelson ）的《第三阶段裁武条约》（ Mickelson ）在上周的沙漠中将他的领带再次划掉，菲尔·麦克森（ Phil Mickelson ）期待着他的游戏将在“ Torrey Pines ”（ Torrey Pines ）中堆起。\\n',\n",
       " '那是稻草打破骆驼的背部，我就知道他不会再回去了。”\\n',\n",
       " '《工作周期》和《全国保健服务标准》网站都是有益的。\\n',\n",
       " '但是，尽管时尚的玩家喜爱这个品牌（《 Vogue 》（ Vogue ）（ Vogue 描述它是时尚的，令人惊讶的，而且设计得很好的）的时候，这个标签有着广泛的吸引力，横跨几代人。\\n',\n",
       " '如今的社交电视——《你的社交电视》\\n',\n",
       " '”格温恩说：“这是个非常聪明的例子，展示了品牌价值，销售附带交易和销售的飞行苏格兰人鸡尾酒。”\\n',\n",
       " '《幽灵冒险》这个赛季的首映式访问了“黑色大丽花屋”，这是洛杉矶历史上最引人注目的（至今仍未解决的）之一。\\n',\n",
       " '理查德·柯尔特在2006年打败西海岸鹰队后，打败了斯瓦特的球员。\\n',\n",
       " '在海滩度假期间，黛米·摩尔享受了一个泥浆水疗的治疗。\\n',\n",
       " '鹰警告公众不要因为穆雷的胜利而离开，因为 Hopman 杯是一个展览，没有任何排序问题。\\n',\n",
       " '我得去了解大卫，还和他约好了《独立报》，根据他的请求，为伦敦展出他的绘画作品，写了目录文章，并在 Soho 餐厅的私人房间里和他一起共进晚餐。\\n',\n",
       " '为什么大的，橡皮奶片上写着“ tjibrr ”，上面写在北欧，特别是康沃尔和布列塔尼的海滩上？\\n',\n",
       " '新闻电视台自2013年以来似乎没有聘请过她，当时她为他们制作了一部名为《判决》的纪录片。\\n',\n",
       " '根据《2020年计划生育》(2020年)公布的全球进度报告，在过去三年中，较贫穷国家的妇女和女童开始使用现代避孕工具的人数增加了2，440万。\\n',\n",
       " '山姆·史密斯从最新的詹姆斯邦德电影《画墙》中演唱，今天早上这首歌被提名为一个金色的世界。\\n',\n",
       " 'Soronzobold 的人坐在一片辽阔的平原上，四周是光明的植物。\\n',\n",
       " '另一个留着长长的金发，穿着红色和白色的格子衬衣，蓝色的紧身牛仔裤。\\n',\n",
       " '该研究发表在《美国心脏杂志》上，它发现，女性的副交感神经调节率降低了，这意味着它在缓解女性身体平静方面效果不大。\\n',\n",
       " '汤姆开玩笑说，自从秀开始，女人就一直在摔倒，因为《露丝女人》在她身后的屏幕上拍了一张照片，她说她会喜欢和他约会的\\n',\n",
       " '凯西·阿弗莱克，根据一个真实的故事，在60多年前来自美国海岸警卫队的一次大胆的救援行动中，开始了他最新的电影《最短的时间》。\\n',\n",
       " '在《灵林》与蓝军的比赛\\n',\n",
       " '学生们还说，紧身裤和短裙也有问题。\\n',\n",
       " '枪手在一系列袭击中杀害了17人，其中包括《查理周刊》和一个犹太超市。\\n',\n",
       " '新闻聚合来自 Facebook 、 Twitter 和新的数字渠道，比如《哈芬顿邮报》（ Hoffington Post ）或副总裁。\\n',\n",
       " 'Vigoda 在《人物》杂志开始流传34年后就去世了。\\n',\n",
       " '《市长法案》（ Bill de Blasio ）敦促百老汇剧院取消演出和餐馆的关闭，以便在公共交通关闭之前尽快派员工回家\\n',\n",
       " '今年，亚马逊实际上开始在《人类发展报告》中播放部分节目，但是竞争的标准却意味着电视制造商们并没有将他们的营销力量放在格式的背后。\\n',\n",
       " '保罗·安德森在《看不见的盲》系列中演奏了亚瑟·谢\\n',\n",
       " '维坎德是最热的，这在她在“前 Machina ”和《丹麦女孩》的作品中得到了称赞。\\n',\n",
       " '他穿着黑色短裤和一件蓝色的 T恤，或者是单身。\\n',\n",
       " '哦，你真的不喜欢蒸花椰菜吗？\\n',\n",
       " '这部系列片是由传奇纪录片导演德斯蒙德·威尔科克斯创建的，他还制作了《生命》和《人类生活》。\\n',\n",
       " '霍尔勋爵在辩论《 BBC 宪章》展期问题时，向他的教育和文化委员会作证，他说“现在是改变的时候了。”\\n',\n",
       " '向《浪潮》的首席执行官内西特表示敬意，他说，自从2000年成为该中心的守护神以来，他对受害者的处境有了个人和强烈的兴趣。\\n',\n",
       " '据《电报》说，卡尼因没有给经济预报员留下深刻的印象。\\n',\n",
       " '格林纳达旅游局的一位发言人没有回应《每日邮报》的电话，称谋杀对未来旅游业的影响。\\n',\n",
       " '英国演员爱德已经收到了他的第一个 BAFTA 提名——他负责支持《不国家的野兽》的 Actor\\n',\n",
       " '“它应该像生肉一样对待——不要以不同的方式看待鸡蛋，”他说。\\n',\n",
       " '晚上晚些时候，当他发现它已成为一个 Twitter 感觉的时候，他告诉了娱乐今晚：“哦，上帝，这是个趋势，对吧？”\\n',\n",
       " '这个名字背后的灵感已经被大量剖析了，大部分人都同意祖马是对马布的祖马海滩的一个点头。\\n',\n",
       " '他说：“如果你是精神病患者，如果你是一名罪犯，如果你是家庭暴力的虐待者，根据《宪法》，法律不允许你拥有自己的武器。\\n',\n",
       " '类似于其他秃鹰一样，雄鹰在天空中高高举起，寻找被废弃的猎物而去清理。\\n',\n",
       " '在2011年11月发布的一部音乐影片中， Beyra 在《我在这里》中共享了一些个人时刻，其中包括她童年时的视频片段，以及她结婚礼服上的超级明星们从未看过的镜头。\\n',\n",
       " '“香酥酥脆的酥点心，还有一种甘美的乳香格布》。\\n',\n",
       " '《未来愿景》，由 J . Daniel Batt 编辑，《救生艇基金会》，682页\\n',\n",
       " '这个月，本菲卡青年的名字又一次又一次出现，尽管有6，000万英镑的释放条款，而《每日星报》现在却报告说曼联愿意在中途和他们会面。\\n',\n",
       " '但是，在《45年》中获得奥斯卡提名的拉蒙女士说，这种政治上正确的思维方式本身就是种族主义的一种形式。\\n',\n",
       " '他的长期制片人托尼·维康蒂（ TonyViscipti ）表示，考伊知道他的癌症是不可治愈的一年，并补充说，他的最后专辑《黑星》是给世界带来的“分手礼物”。\\n',\n",
       " '他告诉我把我的裤子拉到我膝盖上。\\n',\n",
       " '谢赫证明了这项裁决的合理性，提到了《古兰经》中关于禁止“毒药、赌博、偶像崇拜和占卜”的诗句。\\n',\n",
       " '他说，工党政府必须制定出一个全面的计划，比如前自民党政府的60点《安全之夜》。\\n',\n",
       " '当然，这些卡片也可以用真正的钱购买，最优秀的玩家（包括《卫报》的科技编辑）已经在这些数字收藏品中减少了上百磅。\\n',\n",
       " '\"2013年6月6日6月6日， Elisabeta 杂志在她的 Insturam 照片中被一个泳池里的照片上写了出来，她说：\"\"我在美国的#《# un # un # un # un # un # met # met # interactive 》中#？\"\\n',\n",
       " '鹰队以34-20的领先优势，以 Penney Kirkk 为关键的两个关键的三个指针，以最快的速度开始。\\n',\n",
       " 'Naya Rivera 当天在马里布海滩度过一天。\\n',\n",
       " '在圣诞节和新年期间，在许多酒吧都有危险的鸡尾酒。\\n',\n",
       " '在1980年代， Minsky 出版了《思想社会》，它被认为是关于知识职能和结构的开创性文本。\\n',\n",
       " '那是校门，不是鸡尾酒会。\\n',\n",
       " '哦，等等……\\n',\n",
       " '虽然海莉承认自己的角色，包括在“第7天堂”中扮演的角色，但她也和她的姐妹合作看了电影《物质女孩》。\\n',\n",
       " '\"该公司最近的一个主要项目是将4000份 Betacam 录音带数码化，作为香港电台的《美国广播公司（ ABC ）》（简称\"\" ABC \"\"）。\"\\n',\n",
       " '在荷马的《奥德赛》中，佩内洛普（ Peneloney ）提出，有“两个大门”，梦想可以实现。\\n',\n",
       " '《阳江日报》说，改变的图像迅速传播到社交媒体，造成“不良影响”。\\n',\n",
       " '哦，时代的变化了。\\n',\n",
       " '“全景”、海洋全景，甚至是这3幅档案电影， BBC 自然历史博物馆的电影片段和场景由艺术家描绘，令人惊艳，令人沮丧，令人沮丧。\\n',\n",
       " '俄勒冈公共广播公司周四与抵制人士讲话，并将他们认定为大卫·弗瑞，他来自俄亥俄州，丈夫和妻子肖恩和爱达荷州的 Sandy Anderson ，以及内华达州的杰夫·比塔。\\n',\n",
       " '跟踪活动包括参与草地、林地、海滩或其他自然室外环境的活动。\\n',\n",
       " '我的妈妈仍然租房子（因为我16岁时的婚姻破裂，至今仍在工作，并且仍然为更广阔的世界做贡献。”\\n',\n",
       " '据美国国家气象局估计，洛杉矶可能将在周四上午的降雨量为2.82英寸，长滩海拔3英寸以上，帕萨迪纳在周四上午的时间内将达到3.75英寸。\\n',\n",
       " '这汤是松脆的、新鲜的蔬菜和香辣鸡丝的一种华丽的对比。\\n',\n",
       " '“哦，因为那是你儿子的名字？”保罗说。\\n',\n",
       " '这只老鹰在星期六在利姆斯打比赛。\\n',\n",
       " '在我的小说《 Beach Myself 》中，一个完全相同的双胞胎被诊断为双相障碍，她与她的姐妹互换，并陷入错误的生活，于是我决定用声音来反映我的主角的心理分解。\\n',\n",
       " '玛姬·史密斯，81岁，被提名为优秀女性演员，在《市区修道院》戏剧系列节目中出演。\\n',\n",
       " '图像版权盖蒂图像描述朱利安?卡斯特罗（左）告诉 BBC 《公平住房法》从来没有得到适当的执行\\n',\n",
       " '乔丹谈到了在《比尔·西蒙斯》节目中出现的打击。\\n',\n",
       " '国王和另一位妇女于下月在布里斯班治安法院任职，她的儿子根据《青年正义法案》被指控。\\n',\n",
       " '在1988年，我在一部短片《 Mella 的鞋子》中扮演了我的第一个重要角色。\\n',\n",
       " '但《星期日泰晤士报》发现，金融情报局是由可口可乐公司、百事可乐公司、雀巢公司和其他食品业高管组成的。\\n',\n",
       " '一位酒吧男的承认他被困在了亨利的裤裆上的纹身，因为他不可能用痛苦的激光手术切除它。\\n',\n",
       " '虽然热带的阳光开始在黑暗的街道上暖风，但是经过改造的古斯曼，《福布斯》杂志曾被命名为亿万富翁、克鲁兹（ Cruz ），而克鲁兹则变成了肮脏的稻草人。\\n',\n",
       " '多年来一直困扰澳大利亚的 STATE 表明，《音乐剧的幽灵》是在阿德莱德首演的。\\n',\n",
       " '《聚焦》将于明日由英国发布\\n',\n",
       " '一名报名者周日对《邮报》说，这些妇女都很有钱，他们可以自由地离开自己的10，000万英镑的背包“躺在无人看管的地方。”\\n',\n",
       " '《周》旅馆套房：波士顿特使酒店的“总理套房”照片\\n',\n",
       " '\"去年，《公平竞争法》开始了对私人银行分支机构的\"\"深度跳水\"\"。\"\\n',\n",
       " '12月，《体育日报》命名为“不”。\\n',\n",
       " '现在的情况已经不再是这样了，我们只需要看看《第四频道新闻》（ BBC 政治编辑）和凯西·纽曼。\\n',\n",
       " 'Paula Dawson 的《角石》酒店说有超过80毫米的记录。\\n',\n",
       " '反诽谤联盟的国家主任在向《纽约时报》采访时说：“这并不好笑。\\n',\n",
       " '格瓦乌不会说他是否写了新材料，他是演员肖恩·潘在滚石杂志发表采访时说，他在《 El Chapo 》 Guzman 杂志采访墨西哥药王 Joaquin 的时候是否写了新材料。\\n',\n",
       " '2008年，伦敦的“大椅子”表演由伦敦的200名高级公民表演；参加了一个新的“打破记录”事件的参与者尝试了世界上最大的宝莱坞舞蹈，也是《魔戒》中的“最伟大的芭蕾舞团”。\\n',\n",
       " '对于戏剧来说，这在《 Thrones 游戏》的任何事情都是平等的。\\n',\n",
       " '在《纽约公约》中对随机削减攻击的恐惧\\n',\n",
       " '这项研究于本月发表，在《卫生事务》期刊上发表。该研究报告了为期一年的随机控制试验的结果，以测试财政激励措施的有效性，以鼓励宾州医疗体系中的197名肥胖雇员中的体重减轻。\\n',\n",
       " '他的人布克奖得主，《一个终结的 Sense 》，从中产阶级的英语环境中，进入了斯大林主义的俄罗斯和作曲家德米特里·斯伯格科维奇的痛苦生活中。\\n',\n",
       " '由《每日邮报》报\\n',\n",
       " '“当她打电话给我时，我总是无视她的电话，因为她给我的家庭和她自己带来了耻辱，” Dis 先生告诉《4频道新闻》。\\n',\n",
       " '资料来源：《保护议程》。\\n',\n",
       " '\"官员们报告说，他们的命令被置之不理，他们说，他们\"\"用烟火轰击，用玻璃瓶轰击\"\"，根据《明镜》，证人受到威胁时，证人受到威胁。\"\\n',\n",
       " 'Manu Feilldel 和 PetersEvans 从2月1日星期一返回我的《厨房规则》。\\n',\n",
       " '在去年夏天乔治·奥斯本（ Martin Wheatley ）从《公平竞争法》中解脱出来的时候，这位顶尖的内部候选人就被认为是麦克德莫特。\\n',\n",
       " '他解释了他是如何在酒吧里喝酒，然后去镇上喝鸡尾酒来庆祝新年。\\n',\n",
       " '音乐音乐节目主持人：《爱&慈善》、《维奇完美2》和《暮色》杂志。\\n',\n",
       " '房产所有者克里斯蒂安·德·贝尔杜姆告诉《迈阿密先驱报》：这是真实的。\\n',\n",
       " '照片显示的是在白沙湾的风景如画的乐兰海滩上的可怕的垃圾尺度。\\n',\n",
       " '根据《华盛顿邮报》的报道，西弗吉尼亚部分地区的雄性小口鲈鱼（ homhouth ）多达90%，在过去的20年中，世界上有将近40种鱼类物种出现了越来越多的性别变化。\\n',\n",
       " '现在的《星球大战》：《星球大战：神枪手觉醒国际票房记录》\\n',\n",
       " '位于土耳其伊兹密省 Dikilli 的爱琴海沿岸镇海边海滩上发现的一只海豚\\n',\n",
       " '周四上午，在加州圣佩德罗的 Cabrillo 海滩上，超级冲浪击球手打破了破墙墙\\n',\n",
       " '加州《犯罪法》走得太远了，政府官员布朗说\\n',\n",
       " '“哦，哇，玩命长官”，26岁的牛顿说，有可能和曼宁一起去头部。\\n',\n",
       " '几周前，乔治·奥斯本（ George Osborne ）在《秋季声明》中试图把他将公共住房投资减半的工作作为一项增长。\\n',\n",
       " '在20世纪80年代中期，他成为全球愤怒的目标，在南非，他的专辑中记录了他的专辑《格朗格兰德》的一部分，蔑视对种族隔离国家的文化抵制。\\n',\n",
       " '像桑德斯本人一样， Simon 和 Garfunkel 都来自城市的外滩。\\n',\n",
       " '但在被击中后，让-琼-琼----让·珍妮，《生活在火星》中的 Rebel Rebel ，他与约翰列侬和阿什莉一起写了信——正是他常常决定着时尚。\\n',\n",
       " '我们建议公众远离海滩。\\n',\n",
       " '《2016年金球奖：完整名单》，最新情况如下：\\n',\n",
       " '\"定于1月27日在埃克塞特治安法院举行听讯，审议传票，根据第1871号《狗法》第2条，要求狗被\"\"控制或毁坏\"\"。\"\\n',\n",
       " '我穿着温暖的衣服——秃顶熔岩，红色的狐皮外套，因为它很容易就到了——20摄氏度——然后就到外面去了。\\n',\n",
       " '所有的猎人们都把鹰描述成他们家庭的一部分。\\n',\n",
       " '“他对此感到非常情绪化，对此很不舒服，”他说，“他告诉了民主党和《罗切斯特纪事报》。\\n',\n",
       " '他戴着黑色的棒球帽，黑色的短袖圆领衬衫和深色的黑色货车短裤。\\n',\n",
       " '2016年阅读的最佳书《每个人》都被赋予了‘古斯塔夫奏鸣曲’\\n',\n",
       " '但是，在对《议定书》的书面答复中，布肯郡说，内政部对 Dezful 在伊朗将面临危险并不满意，因为他已经用尽了所有可能的上诉权利，他将被驱逐出境。\\n',\n",
       " '英国广播公司的体育——《新圣歌》：威尔士冠军被清盘令震惊\\n',\n",
       " '\"更加引人注目的是，根据《宝洁公司》和《约翰·麦克纳马拉（\"\" Trumbo \"\"）》的改编，“魔术师”通过沉思的泪水流了出来。\"\\n',\n",
       " '“古老的 Ram Inn ”，它是二级上所列的，是在《多日书》中首次提到的，因为它回到了‘远古’的时代。\\n',\n",
       " '他的一个姐姐同意以匿名的方式与《邮报》谈话，他在被捕的几个月里在家中画了一幅令人震惊的生活画面。\\n',\n",
       " '公鸡和埃尔斯仍然很热衷，但这不会发生，不管是新的旧金山49的教练——不管是谁——不管他是谁——不管他是谁。\\n',\n",
       " '互联网也催生了在线玩家——比如批评、《卫报》和《 Buzzower 》——它们与传统媒体竞争。\\n',\n",
       " '我确实收到了一些《纽约时报》文章。\\n',\n",
       " 'Hacked 的存在，首先是菲利普·考利和丹尼斯·卡瓦纳在他们的书《2015年英国大选》中首次披露。\\n',\n",
       " '据《每日记录》报道，一名英雄的垃圾箱司机在他的心脏病发作时，他的车撞上了障碍。\\n',\n",
       " '气流的湍流2\\n',\n",
       " '朱诺：它让我想起了哦，那么宁静。\\n',\n",
       " '据英国《电讯报》报道，达尔福尔的家人承认，在他的母亲和姐妹们都注意到了杰哈德的声音后，视频中的那个人可能是前逃亡的城堡推销员。\\n',\n",
       " 'Corchado 告诉《邮报》说：“当你没有真正挑战个人，并同意将故事提交批准时，听起来更像好莱坞的娱乐。”\\n',\n",
       " '\"她进一步指出，必须保护\"\"不只是原则上的\"\"堕胎服务，\"\"实际上，\"\"禁止在诊所外的骚扰、程序的等待时间以及《海德修正》，后者禁止在大多数情况下为堕胎提供医疗援助。\"\\n',\n",
       " '瑞恩·集邮爱好者在沙滩生命接触报告中的关系\\n',\n",
       " '但是，当她的生活方式由她的“慷慨的父亲”供资时， Lafleur 女士告诉英国《每日邮报》她“不能等待”开始工作，这样她就可以自己挣钱。\\n',\n",
       " '《年龄的体育》编辑科洛伊·萨尔陶写道。\\n',\n",
       " '杰佛瑞?泰姆被授予 SAG 奖，以表彰他在《喜剧系列赛》中的杰出表现。\\n',\n",
       " '《泰晤士报》说，联合国和援助机构已要求紧急救助。\\n',\n",
       " '《教父》电影成功地播放了他的“ Barney Miller ”，它的标题是1974-82年，并以 hal Hal Linden 为标题。\\n',\n",
       " '他们说，他也不恰当地查看了《天体》中关于贸易讨论的笔记、《天体》的主要草案网页和一份载有关于休斯顿前景的信息的浏览网页。\\n',\n",
       " '在2012年，埃米利奥·帕奇的首饰最明亮的西装超过了博维对他们的影响，而亚历山大·麦昆得的裤子则是在博伊·霍奇·道士时期的宽裤。\\n',\n",
       " '当特朗普展示了丹尼斯·赖伊的《约伯记》时，他种在伊丽莎白·赫利（ Elizabeth Hurley ）上，并取笑李说他们是在约会。\\n',\n",
       " '在2011年赢得男人布克奖的《结束的 Sense 》，对一个人的过去进行了戏剧性的重新评估。\\n',\n",
       " '据《泰晤士报》报道，在该城镇理事会工作的员工中约有75%的人涉嫌跳过工作。\\n',\n",
       " '如果新装置是氢弹，它将表明技术已经发生了一步变化。\\n',\n",
       " '澳大利亚电影制作人本杰明?吉尔摩，为他的新电影《回坎大哈》访问阿富汗。\\n',\n",
       " '腿脚：和泰勒一样，令人惊艳的奥莉薇也有修剪工作的别针，完美的跑道\\n',\n",
       " '因此氢弹比裂变的炸弹造成的爆炸大得多。\\n',\n",
       " '前曼彻斯特机场公关人员凯蒂本周早些时候告诉她，她与苏特的分手是如何让她写这本书的，《孤独之心旅游俱乐部》。\\n',\n",
       " '2015年最畅销的美国专辑专辑\\n',\n",
       " '但是，新 Shephard 目的是让乘客进入亚轨道空间，而猎鹰9号的目的是进入较高的低地球轨道。\\n',\n",
       " '该研究报告在《零售杂志》上发表。研究小组分析了前21项调查研究的结果，这些研究审查了购物行为。\\n',\n",
       " '在《肮脏的祖父》中，罗蒙·德蒙·阿蒙·手淫到黄花店，在一个年轻女人的乳房里涂抹防晒霜，抽大麻，试着说粗话，肿块，磨碎，还有更糟的。\\n',\n",
       " '但医生们指出，美国医疗协会《日刊》中指出，医生的疲劳可能会导致医疗失误的几率提高15%，而卫生部并没有对其计划进行全面的风险评估。\\n',\n",
       " '喔，费茨：周日的比赛中，琼斯赢得了一场比赛，但是他们在比赛中失去了22-17到老的 PRex Ryan 的《布法罗法案》的机会，从而保住了季后赛席位。\\n',\n",
       " '这个团队改写了新约的雅各布森风格，用修订版的《圣经》来翻译旧约。\\n',\n",
       " '他的儿子丹和她的女儿卡米拉也参与了这一事业，她正在贝尔法斯特的第六场比赛中工作，丹在新的《神奇女电影》中拍摄。\\n',\n",
       " '在《星期日时代周刊》中，他说：“我让后面的房间的孩子们来玩微分，让可怜的小东西每次翻过来我的方向盘。\\n',\n",
       " '在2015年的一年中，《情感颂》被命名为“牛津字典”。\\n',\n",
       " '以参议院为例，将《能源法案》作为参议院的焦点\\n',\n",
       " '也叫《妇女日》，这是一个妇女的故事，她离开纽约去写她的第一部小说，并被一群当地人抓住，以便其中一个人失去童贞。\\n',\n",
       " '美国和俄罗斯于1991年签署了《裁减战略武器条约》，随着冷战的紧张局势有所缓解，第一个核时代结束了。\\n',\n",
       " '标志性的布赖顿海滩商店出售的前滨纪录为285，000美元\\n',\n",
       " '去年年底，歌剧界的一些最伟大的人物写信给《卫报》和《时代周刊》，警告说，为了省钱而拟议的努力将威胁到 ENO 的生存。\\n',\n",
       " '为了试图挽救他们的财政，他不得不面对他的高谈阔论。\\n',\n",
       " '客户们周日告诉《独立报》，在 Expedia 多次支付预定的假期后，银行已经有效冻结了他们的帐户。\\n',\n",
       " '当他的女儿奇迹般的通过她（并不伟大）接受耶西 J 的《妈妈的最佳》时，他说这是“我们一生中最伟大的时刻——而不是出生时。”\\n',\n",
       " '麦格雷戈，穿着白色裤子，穿着色彩鲜艳的衬衫和墨镜，试图在多斯安格斯面前露面，但巴西几乎没有在整个15秒的比赛中眨眨眼。\\n',\n",
       " '美国的71只猫头鹰\\n',\n",
       " '《规则》说，所有被授予的人都应该穿着专业的服装。\\n',\n",
       " '《大宪章》最初是以英国皇家空军的名义，在二战期间是威灵顿伯伯的家\\n',\n",
       " '第21轮，2000年，《殖民体育场》\\n',\n",
       " '2.这个日期成了会场历史上最畅销的日子\\n',\n",
       " '为期5年的《萨沃勒鳄鱼野生动植物贸易管理计划》在元旦起生效，该计划允许每年收成90，000个活蛋和1200个鳄鱼。\\n',\n",
       " '伊朗男子在德黑兰的旧集市上出售色彩鲜艳的绳索，主要经济活动地点在德黑兰。\\n',\n",
       " '米兰达为汉密尔顿的《姐妹篇》写了这篇文章，《利安尼察》是由蕾妮·艾兹斯·葛丽斯·葛丝梅斯来演奏的。\\n',\n",
       " '在中国， D - Strong 是一个趋势，从中国长城到他的家乡，成千上万的人涌上了 Misquamiciut 州海滩星期天，向他表达敬意。\\n',\n",
       " '\"《雅加达邮报》报道说，我们必须解决这一问题，\"\"协调人类发展和文化部的代理秘书长 Agus Saltono \"\"。\"\\n',\n",
       " '2009年2月-《时尚》杂志的三月封面。\\n',\n",
       " '一群徒步旅行者，在俄亥俄州发现一棵树被烧后，显然被吓坏了。\\n',\n",
       " '第9.16(70)号《西海岸》\\n',\n",
       " '伊格尔暗示将有两个关键的游乐项目：一个在“定制秘密任务”号上，球迷们可以控制千年隼的千年鹰号。\\n',\n",
       " '除了它自己的聚会外，《花花公子》杂志也经常出租，用于电影放映，公司聚会和慈善活动。\\n',\n",
       " '在《教会时报》上撰写的文章中，他说哈马尔教堂的祭司们“代表了一条新的途径，与大多数教区居民的文化联系在一起”。\\n',\n",
       " '我们爱你对 Rey 的热情，并且很高兴我们将包括她在今年晚些时候的《星球大战》比赛中。\\n',\n",
       " '詹姆普蒂斯塔·瓦伦扎的艳丽的豹纹图案增添了魅力——就像2000英镑一样。\\n',\n",
       " '当这位不幸的女人被抛进疯狂旋转圆盘中间，她的牛仔裤环绕在她的膝盖上，露出了裸露的底部，这对夫妇为维护她的尊严所做的一切都是徒劳的。\\n',\n",
       " '不过，在一审判决中，西施绍尔普姆西城的法院说，《仲裁示范法》的高管和最高雇员并没有受到刑事疏忽，他们也没有在40个月的法律程序中犯下其他罪行。\\n',\n",
       " '英国《金融时报》称，谷歌和苹果在公众的反弹过程中正在进行反击。\\n',\n",
       " '我一边吃它，一边看《单身汉》开车回家，指出这是“舒适的食物”。\\n',\n",
       " '像《丹麦女孩》和《唐erine 》等系列的电影，诸如亚马逊的透明父母，吸引了数百万观众。\\n',\n",
       " '图片上的图片描述她说她被《花花公子》杂志 Costsby 麻醉了\\n',\n",
       " '为了给里士满带来领先优势，怀特最后一两分钟就踢了一个进球，但是， Dockers 的守门员在比赛中的进球为12.9秒（81）——12.8（80），他在进球的84秒后，击败了海登《 BallantyBallantyBallantyBallantyBallanty》。\\n',\n",
       " '他甚至可以欣赏那些在这几年前处理人质危机一章的电影《 Argo 》，但他说这是很难看的。\\n',\n",
       " '\"其案文如下：\"\"《无声的夜晚》\"\"。\"\\n',\n",
       " '他的逝世是在他的新专辑《黑星》的新专辑发行后几天。\\n',\n",
       " '我不认为有任何队伍会这样说，“哦，他是罗伯特·格里芬3号。\\n',\n",
       " '更多来自《华盛顿邮报》和《信使号》。\\n',\n",
       " '然后，卡烈特平静地脱掉裤子，开始对自己，抱怨说说出来。\\n',\n",
       " '他来到史诗《黑帮》，但他还是像有火灾一样的人，他把他描述为“一个人可以拥有的最好的朋友。”\\n',\n",
       " '相关信息：《记忆中的天真》—— Orhan Pamuk 的伊斯坦布尔变得既陌生又漂亮\\n',\n",
       " '16岁的诺埃尔·德拉迪瓦·托马斯在告诉《星球大战》《凤凰报》时，在学校开枪时，他正从午餐回来。\\n',\n",
       " '布拉克斯勋爵是被指控尼克虐待的一些引人注目的人物之一，他包括前首相兼前任内政部长爱德华?希思爵士。\\n',\n",
       " '还有很多眼珠：詹妮弗·洛佩兹（詹妮弗·洛佩兹）在《岩石》（ The Rock ）上露面时穿上了黄色的眼睛\\n',\n",
       " '根据美国糖业联盟提供的数据，甘蔗在去年美国种植的蔗糖中占43%。\\n',\n",
       " '一位住在伦敦的31岁的美国电影制作人告诉《独立报》：“由于我的残疾----脑瘫，我被禁止进入航班，这是违法的。\\n',\n",
       " '俄亥俄州3岁的罗利，请她的保姆萨曼莎·帕森斯太太，他是一位艺术家，让她的妆可以化妆\\n',\n",
       " '他最后一次提出的“《辣妹》”（2003年）在希腊是一个巨大的打击，他的最新生产“ Mythkathy ”（ Mythfat ）的希望很高。\\n',\n",
       " 'Hyman 说：“专业和大学与青年体育之间的界限实际上已经缩小了。”这位作者也是《城市最昂贵的游戏：青少年运动的成本上涨和对今天家庭的不满》一书的作者。\\n',\n",
       " '圣地亚哥国家气象局的气象学家詹姆斯·托马斯说，地面还没有饱和。\\n',\n",
       " '最初出版的《闪闪发光的》\\n',\n",
       " '《1956年的 FE McWilliam 》的 Elisabeth Frink\\n',\n",
       " '我总是穿着深色裤子，以防万一。\\n',\n",
       " '他们也许不是亿万富翁，但是有许多人由于最新的《权力舞会》而成为百万富翁。\\n',\n",
       " '凯瑟琳·泽塔-琼斯，通过“饱和”她的面部表情来反对这个问题。\\n',\n",
       " '前临时领导人哈曼告诉今晚《新闻周刊》的女性需要被任命为顶级职位。\\n',\n",
       " '到目前为止，民粹主义者都有机会参加《公约》的战斗，甚至阻止了后院里的建立，赢得了提名。\\n',\n",
       " '这辆卡车，经修改的1992年《 Petrogt 379》，将在最近的《变形金刚》电影中被以“大黄蜂汽车模式”的经典 Camaro 一起被拍卖。\\n',\n",
       " '她在《 LaLa 》上说的最后一句话是“我爱你”，他说他永远都会记得的。\\n',\n",
       " '在英国拍摄的大片大片，包括新的《星球大战》电影，在灯光技术人员威胁到工资的情况下，可能会受到延误。\\n',\n",
       " '当她丈夫在一条街道上散步时，她丈夫手里拿着肯德基一只大桶，另一只手里拿着一只炸鸡，这是个“幸福的结婚周年”。\\n',\n",
       " '丹卡佐克担心他的行为已经损害了他对恋童癖者的“好工作”——他揭露了西里尔·史密斯爵士，并让 Janner 勋爵就历史性的性侵犯案件提起诉讼。\\n',\n",
       " '随着雷内脆弱的形象在她身后巨大的屏幕上，那星唱了《第一时间我》，《你的脸》。\\n',\n",
       " '资料来源：《先驱太阳报》\\n',\n",
       " '《战争与和平》明星莉莉·詹姆斯和她的演员马特史密斯似乎是这样想的。\\n',\n",
       " '4.奥巴马的第4起《睡梦》\\n',\n",
       " '《天使书籍城堡》是上海迪士尼乐园的中心。\\n',\n",
       " '从7月起，安倍的关注点将是确保7月份上议院选举的胜利，日本政治专家迈克尔?乔克( Michael Cucek )表示，他将在东京的早稻田大学任教。\\n',\n",
       " '\"墨西哥总统 Enrique Penna Nieto 星期五在他的推特账户中写道：《 El Chapo \"\" El Chapo \"\" Guzman 案》的捕获：\"\"任务完成：我们有他\"\"。\"\\n',\n",
       " '7名新生穿白色裤子和衬衣的照片上月出现在社交媒体上。\\n',\n",
       " '政府削弱了英国公民对官僚机构的司法行动进行审查的能力，并提议废除《人权法》。\\n',\n",
       " '接下来的专辑《我需要安吉尔》的福音节目为我们提供了一个让最坏的异教徒忏悔的赛道。\\n',\n",
       " '《星球大战》案\\n',\n",
       " '然后他把她推到了接待区，然后把她带到一间公司的一间储藏室里，他把她的裤子拉了下来，对她进行了性侵犯，她的陪审员被告知。\\n',\n",
       " '他还在 Matlock 中反复出现，并出现在特派团：不可能、 Ironide 、 Kobjak 博士、 Killighan 博士、 Fugibing 公司、《联合国难民事务委员会》和 Seifeld 案。\\n',\n",
       " '在2007年1月底，在德文郡的布兰科海滩，“沙查”对几天前发生的事情并不完全公正。\\n',\n",
       " '去年夏天在市中心开放的小7座餐厅里加入了水稻酒吧，而在 Korein 公司的贝莉& Snowout 也加入了 Oi Asian 融合行列。\\n',\n",
       " '《公报》在一份声明中说：“非常接近。”\\n',\n",
       " '位于塔斯马尼亚西北部的约有150名居民，在经历了巨大的丛林大火的海滩上，每天都呆在沙滩上，被船只迁走了。\\n',\n",
       " '它的员工告诉《卫报》，该项目之所以提出上诉，是因为艺术家“非常擅长研究艺术、科学和技术的趋同。”\\n',\n",
       " '现在，六年后，路易斯安娜出生的歌手-歌曲撰稿人在他的第三张工作室专辑《 CanceroTale 》中出局，这也是越来越多的评论。\\n',\n",
       " 'Pash 在社交媒体上的发言很快就得到回应，而社交媒体在《游戏》中扮演了一个简单的角色—— Dash ，后者是2011年至2014年在 BET 上播放的一个受欢迎的原创节目。\\n',\n",
       " '在本周三《卫报》的聪明专栏中，拉斐尔说，柯比纳族——麦道士，是当时的动力——控制了党，并需要时间去战胜它，选举失败或失败。\\n',\n",
       " '女演员和前模特 Krysten Ritter 被发现在海滩上，9月。\\n',\n",
       " '这里有很多东西可以吸引你，从世界驰名的游戏储备到酒厂，还有海滩和企鹅公园的海岸线。\\n',\n",
       " '《纽约邮报》报道，股东 Starboard 价值计划提名互联网公司董事会成员。\\n',\n",
       " '她向所有的歌曲学习歌词；她给了我几行《夜的音乐》，咯咯地笑。\\n',\n",
       " '在最近发表的一篇在《国际事务》1月刊上发表的文章中，我们研究了这个看似矛盾的文章。\\n',\n",
       " '目前，尼普林和派拉蒙正在做这项工作，不过，根据截止日期，他和一支由最喜爱的人物组成的团队正在向前推进，这一部电影是由《火龙》的《杰哈德·赫斯》所指导的。\\n',\n",
       " '或者你还在问奥斯卡·艾萨克是否真的能赢得他在大卫·西蒙给我看《英雄》的表演，或者因为他也是历史上最大的电影电影《星球大战：神枪手觉醒》的明星？\\n',\n",
       " '预计洪水也会来自阿肯色州和俄亥俄河以及一些支流。\\n',\n",
       " '伊布瓦拉的37年历史中12个月中最大的“鹰派”\\n',\n",
       " '《交易协议》或《不交易官员》的主持人说：“我很荣幸被邀请担任第一个国家彩票公司的东道国，我感到很自豪的是，我将在今晚的这一历史性时刻推动这个按钮。”\\n',\n",
       " '安迪·穆利根的酒清器可从《卫报》书店买到。\\n',\n",
       " '这辆车将为新轿车提供猎鹰门和同样的“煎饼式电池”平台。\\n',\n",
       " '该航空公司有33家鱼鹰在库存。\\n',\n",
       " '白宫的立场破坏了朝鲜的激烈言辞，它以“最完美的方式”管理氢弹试验的“惊人事件”。\\n',\n",
       " '在后台是西方的妻子 Kim Kardashian ，他告诉《美国偶像》主持人 Ryan Seacrest 说：“我真的不紧张，因为我真的相信他。\\n',\n",
       " '在南威尔士的 Lavernawk 点附近的海滩上发现的人，虽然比其著名的表弟小很多，但却是一个不成熟的标本。\\n',\n",
       " '《镜报》报道俱乐部传奇菲尔·汤普森认为红军应该签下查理·奥斯汀。\\n',\n",
       " '《爱米莉·索恩》( Emily Thornberry )因在本周被任命的第一次电视采访中被解雇了埃德·米利班德，她最出名。\\n',\n",
       " '去年他在电影《密西西比》电影《密西西比》中扮演的角色获得了一些最好的赞扬。\\n',\n",
       " '《新闻》和《日刊》上说，作为所谓的“金融城交易”的一部分，在“香港仔”方面投入巨大的投资，可能会在石油低迷的时候为该市带来一个光明的未来。\\n',\n",
       " '“我认为改变国旗是个不错的想法，但我不支持让原住民的旗帜代替欧盟的旗帜，”皮尔逊对《每日邮报》说。\\n',\n",
       " '俄亥俄州农业部报告说，它在这个品牌的“地绿斯”包装沙拉中发现了雷斯特菌细菌。\\n',\n",
       " '布莱克在《每日镜报》中透露，她收到了仇恨邮件，甚至在肥皂上也有死亡的威胁。\\n',\n",
       " '墨西哥发布了《 El Chapo 》的录像带\\n',\n",
       " '电影中展出的是美国电影节的第一个太阳节，其中包括了《欲望号》、《魔戒》和《仲夜牛仔》。\\n',\n",
       " '店:韩明小姐声称在伍斯特州 Warndon 购买了《亚布边新闻》的门票。\\n',\n",
       " '西蒙现在85岁了，他和他的妻子住在伦敦西北部，他在《英国皇家空军条例》休假期间在利兹的一个舞会上见面。\\n',\n",
       " '《鲁比·斯莱斯·特朗普》的讲话记录表明，他已经“动武”了，但这并不意味着他应该成为总统\\n',\n",
       " '“这是我最擅长的事情，”她在幕后对记者说，她在电视电影《美国恐怖故事：酒店》中出演了 FX 恐怖选集中的角色，她在幕后对记者说。\\n',\n",
       " '最初被发表的 WITTCH ： Reese 的酒醉后的熔毁\\n',\n",
       " '鹰支持复兴，而科尔比先生则反对。\\n',\n",
       " '这位检察官说，他穿着牛仔裤和一件棉袄，在被警察射杀之前，挥舞着一把屠刀，一边喊着“ AllahAkbar ”，然后被警察射杀。\\n',\n",
       " '《纽约时报》发现寻求庇护者称，他们的门是狗屎、蛋、石块和国民阵线的标志。\\n',\n",
       " '对于书籍来说，你可以轻松地替代亚马逊鹰派的任何其他产品。\\n',\n",
       " '我们在看氢弹吗？\\n',\n",
       " '由于 WDA 案的结果是周二早上的“ Essendon 足球俱乐部”，《星期日时代》回顾了补编丑闻是如何在实地发生的。\\n',\n",
       " '金管局颁发的全新《新观》\\n',\n",
       " '在发表的《 Eko Atlantic 》中，对这一说法表示了一定的支持，主要表现为他们描绘的皮肤明显轻的群体，这种群体的形象绝不象今天的拉各斯的代表性样本。\\n',\n",
       " '此前有4人在《自由储备》案中承认有罪，包括布洛夫斯基的共同创始人弗拉基米尔·卡茨。\\n',\n",
       " '几年前，我接到一名记者打来的电话，问我是否会和他谈谈他写的一份《伦敦日报》中关于性别歧视的文章。\\n',\n",
       " '在2018年6月22日，当《侏罗世》世界2号传到电影院的时候，粉丝们可以看到 Claire 穿得非常漂亮的靴子。\\n',\n",
       " '分手后，他继续打造一个成功的独唱生涯，他的独唱独唱专辑《不快乐的阿尔福德》（1982），金牌。\\n',\n",
       " '他在电影《》中的突破。来源：供应\\n',\n",
       " '卢卡斯在致辞中说：“我一直在与迪斯尼合作40年，选择他们作为《星球大战》的守护者，因为我非常尊重公司和鲍勃·艾格的领导能力。\\n',\n",
       " '显然，新英格兰爱国者在那天晚上也打败了西雅图的海鹰，但是——就像右鲨鱼一样——没有人在乎或记得。\\n',\n",
       " '排名第八的唯一一个例子是一个德国式的轻装裤，一个科希奇。\\n',\n",
       " '享受海滩，享受大海洋路上的乐趣，欣赏到怀河和酒店去用餐，但是要尊重他们。\\n',\n",
       " '这是个大的：他的第六版《维吉尔史诗史诗》，它跟随埃涅涅德进入阴间。\\n',\n",
       " '也许，他的最后一个屏幕角色在艾伦2000年的电影《小时代》中爆发了。在50年代的这一行业中，有一半的喜剧双子尼克尔和5月份在未来著名的导演麦克·尼科尔斯（ MikeNichols ）的陪同下闯入了这一行业。\\n',\n",
       " '这名28岁的球员上星期赢得了他的第五次巴隆，在队友内马尔和皇家马德里的克里斯蒂安·罗纳尔多之前，他随后发表了《法国足球》杂志的创始人奖。\\n',\n",
       " 'John Kasich ，俄亥俄州州长\\n',\n",
       " '模特忒阿杜拉·理查兹——《滚石石》中30岁的女儿，已经严重地双膝跪下了。\\n',\n",
       " '孩子们的书《希拉里·罗德姆·克林顿：一些女孩是由米歇尔·马克尔生出来的》一书， LeUen Pham 则致力于在下一代人心目中灌输正确的希拉里态度。\\n',\n",
       " '《星期日邮报》分享了收集到的信息，这是我们与城市警察局调查的一部分，也是由伦敦警方负责的国家欺诈报告服务、行动欺诈。\\n',\n",
       " '“当涉及人类悲剧时，没有动物会靠近蚊子所造成的破坏”，这两个人在《时代》的一篇文章中写道。\\n',\n",
       " '这位喜剧演员和电影制片人在《纽约女演员莱斯》的《纽约人》杂志上说，关注黑人演员与白人演员之间的工资差距很重要。\\n',\n",
       " '在利兹，《曲线》的“人”在周末都满是在周末，但至少我知道我不能在那里吃饭。\\n',\n",
       " '这也揭示了英国《时尚》已经走过了100年的走钢丝——在真实与不可联系的生活之间的平衡，梦想和严酷的现实。\\n',\n",
       " '下周末将为观众提供的电影将是：利昂门的《选择》、《环球》、“凯撒”（凯撒、凯撒）、以及索尼的和屏幕宝石““骄傲与偏见，以及僵尸。”\\n',\n",
       " '今天，成百上千的全球游客乘坐火车、地铁和地铁，他们没有穿任何裤子，因为那是“没有”的日子里的一部分。\\n',\n",
       " '如果这意味着要放松卫生间门，并且对代名词更多的思考……哦，这不是付出巨大代价的，是吗？\\n',\n",
       " '它也将被驻东京的鹰派占领，成为日本军队更广泛的军事作用的理由。\\n',\n",
       " '相关内容：从卡达西哈教到《完美》：鲍勃罗如何获得21世纪的狂欢？\\n',\n",
       " '纽约——布鲁克·伯克当天将《花花公子》拿下了，但她没有对《花花公子》杂志的《裸照》有任何问题。\\n',\n",
       " '在烤、烧烤、甚至炸的橄榄油里，鱼是一种完美的基础，为清淡的食物供应充足的鲜艳的营养丰富的蔬菜。\\n',\n",
       " '星球大战的导演乔治·卢卡斯接受了坦率的采访，承认他对最近的《星球大战》电影感到失望。\\n',\n",
       " '他对《电报报》说：“这是破冰，”圣保罗说，“我对所有的人都做了一切，这可能意味着我可以挽救一些人。”\\n',\n",
       " \"其中一个最早的角色是 Jay ’ Boom ' DeBoom 在《 X 档案》第3季扮演的角色。\\n\",\n",
       " '1978年，在汉德乐队的陪同下，广播电台发布了《弗洛里舞》，这首歌在英国排行榜上达到21点。\\n',\n",
       " '嘉特·格林威尔在他的处女作小说《》:“我从14岁起就一直在巡航\\n',\n",
       " '在《史密斯与魔鬼》的例子中，这是一个狡猾的故事——一个骗子欺骗的故事，显示出一个非常古老的挑战。\\n',\n",
       " '正如维克托·塞伯斯特在他的新书《1946年》中指出的那样，乐观主义者供不应求。\\n',\n",
       " '尽管有近200个国家签署了《巴黎气候协定》，他们同意减少碳排放，但 Krauss 表示，“对抗气候变化的战斗还几乎没有开始”。\\n',\n",
       " '在世界贸易中心遭到袭击后，乔纳森和 Alexandra Zizmor 在《纽约的 Salute New Yorkers 》中播放了一个新的广告，内容是“他们的力量和勇气”。\\n',\n",
       " '蓝色公司的首席执行官史蒂夫?科恩( Steven Cohen )告诉商业杂志《零售周》，他想关闭的商店中有很多是俱乐部的店店主，他们坚持认为业务的核心是有利可图的。\\n',\n",
       " '2009年至2013年期间，布朗根据《社会保障管理法》的规定，对两项罪名提出了起诉。\\n',\n",
       " '如果不是这样，那就会是由猫印制的纯透镜面料，而她的尼龙背心和裤子的裤子，就会对那些在“网络犬”之类的商店里呆过的人很熟悉。\\n',\n",
       " '他对 BBC 《星期日政治》说：“我认为现在可能性不大。\\n',\n",
       " '这项研究发表在《科学》杂志上。\\n',\n",
       " '在《滚石》杂志上，格兹曼说他是世界上海洛因、甲基苯丙胺、可卡因和大麻的主要供应者。\\n',\n",
       " '今年秋天，阿里·史密斯（ Hamish Hamilton ）史密斯正准备在瞬间推出一部新的小说，接下来是最后一位获奖者的最后一部小说《做故事》和《公共图书馆》。\\n',\n",
       " '安德烈·拉塞尔在悉尼穿着《 Thunder 》中的石灰。\\n',\n",
       " '\"内阁被告知，在澳大利亚公务部门和相关的法定机构中提出的诉求，以及电信和 Qantas ，\"\"在一些重要方面似乎与《协定》不同\"\"。\"\\n',\n",
       " '当他到那里时，他们说他们想让克莱森成为美国俄亥俄州的大学足球界的阿拉巴马州。\\n',\n",
       " '由拉塞尔·霍根撰写的《哈利·霍恩》( Liddley Walker )一生的书\\n',\n",
       " '澳大利亚网球联盟拒绝就《纽约时报》在周日的澳大利亚公开混合双打比赛中报告的可疑交易发表评论，并在一份声明中重申，提鲁的政策是“审查并调查所有有关网球的腐败指控”。\\n',\n",
       " '2011年，他对《华尔街日报》说，中国所有的进口货物都应该有25%的关税。\\n',\n",
       " '尽管有10年的喜剧喜剧，《周六夜》和自己制作的电视连续剧，艾美·舒默在2015年做了自己的表演。\\n',\n",
       " '上周在《儿童权利公约》的《儿童权利公约》伙伴会议上， VRML 共同创作者马克·普莱斯斯发表了主旨讲话。\\n',\n",
       " '朝鲜中央通讯社的韩国中央通讯社说，朝鲜政权在东北部的 Puntgye - Re 地下试验场引爆了一个氢装置。\\n',\n",
       " '鲁珀特·默多克（ Rupert Murdoch ）宣布，他在伦敦《时代周刊》的出生、婚姻和死亡条款中加入了杰里霍尔。\\n',\n",
       " '赫斯特的最佳独唱是在阅读约翰·列侬的《我的爱》，这是由全体成员的完美意识。\\n',\n",
       " '博伊和亨德森的粉丝们都在愤怒地表达了他们对《迷宫》系列的想法的愤怒：“吉姆·亨森和大卫·博伊已经死了。\\n',\n",
       " '《泰晤士报》称，这些事件引发了对其吊舱任何剩馀成员的恐惧。\\n',\n",
       " '精子鲸被搁浅在诺福克海滩上的海滩后死去\\n',\n",
       " '\"插图作者：\"\"自由之声：民权运动的精神\"\"，《公民权利运动的精神》，由 Ekua Holmes 表示，由 Carole Boston Weatherford 撰写。\"\\n',\n",
       " '但是不败的《猫》再次看上去在几个星期前没有后卫贾里德·里尔斯，另一个膝盖伤病困扰了前德蒙。\\n',\n",
       " '在一座建筑物内的《吉尼斯世界纪录大全》中，旧的 Vine 长大了，上面写着，该植物甚至可能有400多年的历史，但争论仍在继续\\n',\n",
       " '泳滩碧昂及蓝常春藤\\n',\n",
       " '联邦政府的新饮食指南让美国人在每天摄入含胆固醇丰富的食品，如鸡蛋，每天喝5杯咖啡。\\n',\n",
       " '在开放的周末，在英国和爱尔兰的电影院里，几乎每10次就有9次是为了看《星球大战》。\\n',\n",
       " '10月，俄亥俄州拖延了一年的处决，而它却在寻找毒品。\\n',\n",
       " '公鸡合作社: MitcherPearce 。\\n',\n",
       " '《美国国家地理杂志》杂志，《家庭犬》的照片，2月。\\n',\n",
       " '美国得克萨斯州大学( Austin )奥斯丁大学( Austin )的教授、心理学教授和市场学教授、《智能变革的作者》( MarketChange )也同意这一点。\\n',\n",
       " '“哦，不，”其中一个女人喊着，“我们该吃什么？\\n',\n",
       " '卡莱尔开始了一首成功的独唱歌曲《关于你的歌》，《关于你》的歌曲《天堂》在地球上的一个位置上，《天堂》中的第一首单曲《天堂》。\\n',\n",
       " '他用《古兰经》的诗句来支持他的发言：“酒、赌博、偶像和占卜的箭，都是对撒但所做的可憎的恶事，所以避免他们，所以你也许是有意义的。”\\n',\n",
       " '在星期二晚上11点左右， MitchePearce 和他的公鸡在皇家橡树宾馆中被绊倒，这应该是他们的澳大利亚日节日的结束。\\n',\n",
       " '《 Moss 评论》发现，被拘留中心的许多寻求庇护者担心自己的人身安全。\\n',\n",
       " '去年，伊尼兹鲁人在《最佳影片》中输给了理查德·林肯德（ Richard Linket ）的“最佳形象”，但他对此有一个规模和眼光，今年大多数其他被提名人都会显得微不足道。\\n',\n",
       " '专家们已经敦促孕妇避免在一项密歇根大学发表在医学期刊《 JAMA 儿科学》杂志上发表的研究报告中避免邻苯二甲酸盐，因为在怀孕期间尿里的一些邻苯二甲酸盐含量与早产的几率有关。\\n',\n",
       " '据法国的《公共》杂志称，勒庞先生要求扎布巴特全面书面道歉，约40，000英镑。\\n',\n",
       " 'Fitbit 可能是最畅销的健身系统品牌，但公司的观察人士对它进入新领域的行为表示怀疑。\\n',\n",
       " '根据外包，美宝，美宝，是“一个沉睡的南方海滩村庄，让我们提供了百万亩的更好的部分，用于从盐沼，从盐沼，从盐沼到咸水的内陆河流。”\\n',\n",
       " '如果移徙者选择在海滩上登陆，就必须由塞浦路斯政府而不是英国政府负责。\\n',\n",
       " '国王队在比赛结束后希望恢复，在10分之内得分，但只需投奔鹰队，以填补28点最大的领先优势。\\n',\n",
       " '但如果从长远来看，你从20世纪福克斯的《辛普森一家》的原创作品中获利，那么请注意：这是一个严肃的法律问题。\\n',\n",
       " '据《纽约时报》报道，这一事件是在英国广播公司和布泽尔报道的一周后的一个星期内发生的。\\n',\n",
       " '据商业杂志《公司》报道，今天，艾森斯斯是英国第二增长最快的 IT 公司。\\n',\n",
       " '与此相关的是：在《巴黎简报》中出现了评论\\n',\n",
       " '《生活》杂志\\n',\n",
       " '布朗的女发言人 Nicole Perna 在《说唱》的一份声明中宣称。\\n',\n",
       " '在12月2日关于叙利亚问题的议会辩论中，默纳纳汗先生在《天空新闻》的工作模式中，在威斯敏斯特学院的绿色学院中拍照\\n',\n",
       " '肥田热身赛：《力球奖》为7，000万美元\\n',\n",
       " '在电影《快乐圣诞》中，劳伦斯也主演了作品，他的作品受到了评论家们的欢迎。\\n',\n",
       " '青年党发布《青年党释放》录像：唐纳德·特朗普\\n',\n",
       " '詹妮弗·洛佩兹选择了一个大胆的、白色的考夫曼·佛朗哥·佛朗哥·佛朗格·佛朗朗在《行星好莱坞度假村》中的首映式。\\n',\n",
       " '在帝国皈依基督教的过程中，许多人都被熔化了（如今，人们看重的是 Marcus Aurelius 的雕像，这只是因为它被误认为是第一个基督教的皇帝君士坦丁）。\\n',\n",
       " '据《快报》报道，著名的海龟住在大西洋沿岸的圣赫勒拿岛上，他的健康水平下降，嗅觉丧失。\\n',\n",
       " '对于那些喜欢在展馆的隐私之外冒险的人来说，佩格海滩也为所有客人提供了一个无限的游泳池\\n',\n",
       " '她的书《孤独之心旅游俱乐部》将于明日发行，并详细介绍了她在泰国、印度和尼泊尔发现的自我发现的冒险经历。\\n',\n",
       " 'EPA 的科学小组负责人安东尼·博纳说，菲利普港的所有海滩在夏天都表现良好，因为没有多少降雨。\\n',\n",
       " '这些研究结果今天发表在《自然通讯》上。\\n',\n",
       " '\"毕竟，这就是给我们带来标志性喜剧的人，如\"\"茅德\"\"，《好时代》和《家庭中的所有人》。\"\\n',\n",
       " '《每日邮报》的一位发言人说：“我们在慷慨的慈善支持下，为那些曾经是冷呼鲨鱼的受害者和他们的私人信息中的一种玩世不恭的交易的公众揭露了严重的过失。”\\n',\n",
       " '\"根据《都柏林规则》，英国允许移民返回\"\"入境点\"\"，无论在哪个大陆上\"\\n',\n",
       " '就在几个月前，纺织巨头似乎在高飞高飞，推出了《金融时报》简介：“生活中国梦”。\\n',\n",
       " '《版权先驱报》\\n',\n",
       " '在周末的海滩上发现了那些死鲸鱼，据信这些鲸鱼是同在林肯郡 Hunstanton 、 Norfolk 和 Wainbus 发现的同类动物的一部分。\\n',\n",
       " '《史密斯之歌》的迷人解释……\\n',\n",
       " '注：最新电影改编的尼古拉斯·瑟克宝小说，《选择》，2月2日开幕。\\n',\n",
       " '《美丽的州广场》：一年多一次，终于到了30家\\n',\n",
       " '在 Gillian Armstrong 的电影《女电影》中，他的“身披”服装设计师和凯利是由戴伦?吉尔斯纳（中心）扮演的。\\n',\n",
       " '\"《2015年反恐和安全法》规定，学校有义务\"\"适当考虑有必要防止人们被卷入恐怖主义\"\"。\"\\n',\n",
       " '《 PDO 》对全球变暖的影响可以比作一个楼梯，在一段时间里，温度升高，通常是10年以上，然后是向上的。\\n',\n",
       " '看着过去的起伏，我们曾有过乐队，如《狂人战士》、《狂热者》中的列昂诸王——那些在外面卖场比赛的乐手列王——他们从铁板标签上获得了重要的支持，也成为了商业电台的主要支持者。\\n',\n",
       " '现世：密苏里，《押解金法》，更多的枪击死亡事件\\n',\n",
       " '据报道，这位女演员称，她丈夫对她说，如果她在百老汇演出《奥德赛》中扮演一个角色，他就会和她离婚\\n',\n",
       " '阿根廷的主教们放弃了带花边的海滩照片，报告说\\n',\n",
       " '这对冒险的夫妇，前往夏威夷的科宝库阿湾，在曼尼尼海滩公园的台阶上，你可以在那里打上鼻子，然后你就可以在那里打呼噜了\\n',\n",
       " '心胸开阔的人会认为是宝贵的，而思想封闭的人会认为是可悲的。\\n',\n",
       " '当伯内特告诉《纽约时报》时，当她去年学到“ SAG ”的荣誉时，她“吞吞吐吐”，她看起来在周六晚上在 Shrine 博览会的舞台上当头露面。\\n',\n",
       " '但是，读者和好莱坞的同事们都被《我的生命》中的故事所震惊。\\n',\n",
       " '他们不那么性感的“肯德基夏令营”广告中，桑德斯上校看到两个守卫，就像他们打了一只鸡。\\n',\n",
       " '正如《每日快报》去年4月宣布的那样：“专家警告说，在英国发生旱灾之后，禁水管道和骤雨可以成为夏季的主题。”\\n',\n",
       " '他的电影《电影》是一份丰富多样的文件，他的作品是史蒂芬·瑟里奇的电影，直接视频素材，但很少有糟糕的表现。\\n',\n",
       " '他说：“我不知道它是怎么运转的，因为如果你将脂肪熔化在身体里，那么它就一定要去某个地方了。”\\n',\n",
       " '影片《女王的拉蒂夫》（戴着迈克尔·科斯特洛的穿着）赢得了 HBO 的电视电影或迷你电影最佳女主角，并与安东尼·麦基一起在红地毯上亮相\\n',\n",
       " '下面，我们列出10个可能是你在《伯恩斯夜宵》里的招牌。\\n',\n",
       " '4月9日（就像《时代周刊》报道了福隆对米利班德的攻击一样），《卫报》的头版报道说：“工党日的工党领袖们是保守党的受害者。”\\n',\n",
       " '因为我发现它在我的牛仔裤口袋里，我和我的女儿用吹风机把它烘干了。\\n',\n",
       " '阿兰·弗兰克的小说《温迪-霍华德-瓦特历险记》于1月21日由 Muswell Press ($7.99)出版\\n',\n",
       " '他决定自己作为一个独唱演员出场，释放了《麦田唱片》和他的首张专辑《大卫·博伊的世界》的三场单曲。\\n',\n",
       " '在《摩门书》中，《狮子王》和《坏女作家》的票房收入超过2亿美元，而阿拉丁则创下了破纪录的2，398，110美元(1，133，907英镑)。\\n',\n",
       " '在电影《顶级火炮》中展出的 Forresal -级承运人今年将被取消。\\n',\n",
       " '在2013年的《太阳报》上，德莫特说他的布赖顿市雇用了25名员工，“干得不错”。\\n',\n",
       " '通过充实精神疾病的经验，他们将我们推向了真相，正如马特·海格在他的回忆录《发表的宣言》中所说的：“如果他们要求正确的职业，每个人都有一个标签”。\\n',\n",
       " '早餐是一个大型的 BAP ，你可以选择咸肉，鸡蛋，蔬菜香肠，蘑菇和类似的，茶或咖啡在一个烧瓶里。\\n',\n",
       " '这位现年69岁的歌手，于昨天去世，他为他的新专辑《百星级》中暗示了他的病情的晚期。\\n',\n",
       " '像《星球大战》一样，岩石就像《星球大战》的《星球大战》。\\n',\n",
       " '上周日，《星期日泰晤士报》表示， Facebook 抵制由 HMRC 调查在2010年至2014年期间在英国缴纳的税款。\\n',\n",
       " '最初的1977年《星球大战》排名第二，约为15亿美元，包括再发行。\\n',\n",
       " '克林顿就在这个故事的早期，显然是在一个博客上阅读，这个博客上提到的是《耶路撒冷邮报》的一个项目，而不是很高兴。\\n',\n",
       " '美国总理马尔科姆·特恩布尔对无家可归的人肯特克尔·克里普说新的《应用程序》的质疑。\\n',\n",
       " '在《威尼斯商人》中，莎士比亚以这种善良的风度闻名，宽恕行为：\\n',\n",
       " '黄表示，许多中国人不熟悉《星球大战》。\\n',\n",
       " '她为被允许穿长裤的妇女赢得了比赛。\\n',\n",
       " '乔治?梅森（ GeorgeMason ）最终没有签署《宪法》，他建议增加通过的“重罪和轻罪”。\\n',\n",
       " '报告还发现国务院要求的《信息自由法》的时间远远超过其他部门的要求。\\n',\n",
       " '最后一首关于他们的最后专辑《恐怖的暮光》的歌曲，我们是怎么到这里来的？\\n',\n",
       " '因为他们确实有时间失去效用，所以最好还是要买少量的碳酸氢钠和少量的粉。\\n',\n",
       " '加州大学洛杉矶分校的最新年度《好莱坞多样性报告》得出结论说，在镜头前和后面，女性和少数群体的人数严重不足，尽管观众对电影的不同表现表现出强烈的欲望。\\n',\n",
       " '以一种怪异的方式，（新）《星球大战》是一部公路之旅电影，所有这些事情都发生在千年隼号上。\\n',\n",
       " '《悉尼晨报》专栏作家理查德·格洛弗将接受澳大利亚女演员、剧作家和编剧凯特·穆冯尼的采访。\\n',\n",
       " '俄亥俄州(11-1)是卫冕冠军，在10周前的第一个星期里一直是冠军。\\n',\n",
       " '\"知情人士对他的行为的调查告诉了《星期日泰晤士报》的一项指控，即 Danczuk 先生滥用他的职务\"\"看来是\"\"的，而且该案可能严重到足以将他逐出政党。\"\\n',\n",
       " '在1月16日，在“绿湾饼者”和《亚利桑那》之间的司际比赛中开始，“帕拉斯”的“龙”龙“龙”“龙”“龙”“龙”“龙”号叫“尾翼”。\\n',\n",
       " '在同一作品《哈德逊河学校》中， Searcy 的牙科保健师透露，她的牧场主父亲用他的女儿哭来诱捕杀死他的羊群的狼人。\\n',\n",
       " '这个问题涉及到 Spiers 先生为《先驱报》撰写的一个在线评论作品。\\n',\n",
       " '在《牛津联盟》的辩论中，罗兹的共同创始人 Ntokoszo Qwainbe 必须放弃竞选。\\n',\n",
       " '一位选民告诉我这是迈克尔·基顿在影片中穿着卡其布裤子的电影。\\n',\n",
       " '哦，在水官僚间可能有罕见的电话投机，或者是通过官僚机构的快速旅程，看看奥普总统的首页照片中的厚白色粘合剂是否堆放在墙上的黄金可以匹配。\\n',\n",
       " '《复仇者》和《英雄》明星杰里米雷纳尔和克里斯托布·温特斯改写了这座曾经属于传奇编剧兼导演普雷斯顿·斯托敦促的洛杉矶房子。\\n',\n",
       " 'KellyRowland 证实了她与她的经理，蒂姆·威瑟斯彭的关系，《女王拉蒂法》节目。\\n',\n",
       " '1997年对《纽约书籍评论》的审查\\n',\n",
       " '在《 El Chapo 》 Guzman 的 Joaquin 的新释放的照片中，他的头和胡子都被他的头和胡子刮了。\\n',\n",
       " '他的朋友告诉《星期日泰晤士报》，这就是为什么他要在叙利亚生活和参加伊希斯的事情都不会有任何意义。\\n',\n",
       " 'Falconer 勋爵告诉 BBC 的安德鲁·马尔显示“我当然不会开除他”。\\n',\n",
       " '在《星期日财富排行榜》中，2007年他名列第14位，而福布斯全球亿万富翁名单上的排名为194人。\\n',\n",
       " '在《弗兰弗瑞》的写作中，克里斯蒂安·戈林大体上是同意的。\\n',\n",
       " '\"该杂志第13期《大别》的题目是：\"\" Rafidah ：从 Ibn Saba \\' to the Dajjal \"\"。\"\\n',\n",
       " '《国家评论》的编辑说，“富有的洛莉”花了数周时间，从《每周标准》威廉·克里斯托尔到 RetriceErickickson 等从《标准周刊》到 RetricErickickson 等。\\n',\n",
       " '这一套是在美国电视喜剧《2名女孩》的基础上很松散的。\\n',\n",
       " '摩根弗里曼凭借《百万美元宝贝》赢得了最佳支持演员奖。\\n',\n",
       " '然后，面试者询问《教程》是否应该实行配额，对此，她回答说：“为什么把人归类？\\n',\n",
       " '碧昂斯在她的独唱单曲《单身女士》（《安乐》）中表演了一首。\\n',\n",
       " '其宣誓证词是由加州大学伯克利分校调查报告程序获得的，并与《纪事》合用。\\n',\n",
       " '导演毕竟以一种截然不同的风格制作电影的美名，那就是荒谬的，满嘴脏话的，他的裤子里威尔·费雷尔就像狒狒一样尖叫着。\\n',\n",
       " '哦，你有没有听说福克斯新闻说特朗普愿意重新加入这场辩论，如果他们为他的慈善事业拨出了500万美元？\\n',\n",
       " '梅瑞狄斯·维埃拉让我们来看看《科尼》的故事。\\n',\n",
       " '纽约《纽约公约》顶楼75万美元\\n',\n",
       " '人们将不得不放弃驾车，但没有计划修建一条铁路线[去北部海滩]。\\n',\n",
       " '最初看来，他们成功地进行了一次核试验，但未能成功完成第二阶段的氢爆。\\n',\n",
       " '《杂货商》杂志的数据显示，英国 PG 方案的销量在今年9月份下降了6%，为1，600万英镑。\\n',\n",
       " '我们的车前车灯在我们的定名车下了车，在夜鹰的“堡垒”外面接我们去接我们。\\n',\n",
       " '她对《镜报》说：“大家显然都清楚，去年的事情并不像他们所做的那样好，而且我认为经验教训已经学了。”\\n',\n",
       " '“ Chi - Raq ”导演李光耀带来了他的《流行纪录片》，“迈克尔·杰克逊从 Motown 到长城的旅程”，今年“跳到太阳舞”。\\n',\n",
       " '用清水来填饱肚子，然后煮，加蔬菜。\\n',\n",
       " '我的希望是，我们可以炮制一些地方和联邦货币的鸡尾酒，这样我们就能做到这一点。\\n',\n",
       " '黛博拉·罗伯兹，《时尚婚姻》中的《婚姻》， Done\\n',\n",
       " '\"2014年，她被授予澳大利亚勋章( OMAM )，为国际社会服务，昨天的同事和朋友对\"\"恢复希望行动\"\"脸书网页表示敬意。\"\\n',\n",
       " '尽管他们所有的技能、经验和经历都是如此，但是这些条件的结合和精心准备、尴尬的维京团队证明了海鹰的挑战。\\n',\n",
       " '周六的结果还使她在所有学科中获得了74分的职业世界杯冠军——只有12秒的《英吉塔·史登马克》创下了86次的纪录。\\n',\n",
       " '然而，《先驱太阳报》报道，“手机故障”是“不得罪”的人。\\n',\n",
       " '一只戴着兜帽的鹰在墙上镶嵌着剪影。\\n',\n",
       " '拉姆伯特说:“哦，不，亲爱的，你是公司!”\\n',\n",
       " '这位女演员在2008年对《 OK 杂志》的薄薄背景下一直受到批评，“我不太瘦。”\\n',\n",
       " '你的牛仔裤里藏着什么东西？\\n',\n",
       " '就在本周，宜家公司的老板说，我们可能会受到饱和点或“高峰”的影响——这种状况可能被称为“顶峰窗帘”。\\n',\n",
       " '我每年都会吃油腻的食物，吃白面包火鸡三明治，加黄油，和无尽的布丁和奶油。\\n',\n",
       " '本周末他们在放映《星球大战》——《神枪手觉醒》。\\n',\n",
       " '不幸的是，沃尔什（ Walsh ）曾出演了吉姆·凯利的好莱坞喜剧《阿丝·温图拉》：“宠物侦探”，因为他错过了比赛场地目标的比赛，他在时钟上只有几秒钟的时间。\\n',\n",
       " '《 Essendon 34》在强硬电话结束后立即生效，并有权利接受法院诉讼。\\n',\n",
       " '变性人在统一政策的历史性变化中，可以选择穿裙子、衬衫、裤子或衬衫到一所顶级公立学校。\\n',\n",
       " '《美国原子科学家公报》将在华盛顿的隐喻时钟上公布。根据公告，时钟反映了世界对核武器、气候变化和新技术灾难的危害。\\n',\n",
       " '扎克伯格喜欢分享可爱的马克斯的照片，包括她的一个穿着绝地的服装，以纪念《星球大战：神枪手觉醒》的释放。\\n',\n",
       " '在他的圣诞火鸡里，西蒙·丹泽克可能反映出他有很多值得感激的地方。\\n',\n",
       " '图像版权空图片描述 Activist William Callay 的努力导致拍摄了一个拍摄视频，该影片《黑人生活物质》的帮助，在网上传播。\\n',\n",
       " '全球各地买家的疯狂收购导致了上榜的唱片销量，其中排名第三的是一个惊艳的弗兰克科尔特，拥有巨大的血统，售价为130万新西兰元（1.19万美元）。\\n',\n",
       " '有一次，布奇艾德宣布，他在一只鸡舍里工作会更舒服，所以他在工作室里，从木板和鸡线为他做的工作。\\n',\n",
       " 'Basil . Blackshaw :《社会边缘人》于周四晚22时在英国广播公司的2北爱尔兰举行。\\n',\n",
       " '《海鹰-海盗》的季后赛天气预报：寒冷的寒冷\\n',\n",
       " '在《巴黎评论》（巴黎评论）的结合之后，“勃朗化芒果”（ BranimeReview ）中出现了真正可怕的相似之处，这让人感到意外——《惊喜城——火焰城市》中的病毒。\\n',\n",
       " '不到一英寸的雪量就足以使马里兰州、马里兰州和维吉尼亚北部的道路畅通无阻，造成了数百起事故，导致司机被困几小时。\\n',\n",
       " '他穿着黑色的 T恤和黑色的冲浪板短裤。\\n',\n",
       " '宽边风就像2011年在朗文时装表演中出现的《男人》中的一个博伊穿的。\\n',\n",
       " '那是他们后来在《星际迷航》中提到的音乐会，它是红色迈克尔·哈奇的作品，它的基础是球迷们在等待着购买布伊的门票。\\n',\n",
       " '据《卫报》报道，她现在将有效地承担大约10亿美元的牌照费。\\n',\n",
       " '她写道，如果我相信《地狱》， Coordby 就会去那里。\\n',\n",
       " '寒冷的天气：这些旅行者穿上外套，但是不穿裤子，但在车站里面却显得很舒适\\n',\n",
       " '这是她第五回合的《流行偶像后代》中的评委，他产生了凯丽·克拉克森和嘉莉德伍德。\\n',\n",
       " '正如你可能想象的那样，《太阳报》文章中讨论的时间和细节以及对麦当劳事件的兴趣一直在我们的青睐之中，而且它也在帮助“吸引投资，”伊曼纽尔·亨利写道。\\n',\n",
       " '这就是《印度刑法》的规定，它不仅禁止同性性行为，而且禁止性交以外的任何性行为。\\n',\n",
       " '相关信息：弗兰西斯·哈迪格·斯考普·斯考普勒斯的孩子们的书《2015年》与谎言树\\n',\n",
       " '但我也认为，由于话题的原因，大多数老男人对自己50年代的想法都太死板了，甚至想看《暮色》杂志或“房间”。\\n',\n",
       " '雷伊，大德龙和芬兰人永远不会做出一个错误的决定——比如，比如，莱娅的决定，他们都应该在《星球大战》的垃圾堆里潜下去。\\n',\n",
       " '1981年2月，在曼诺普尔的席德，有两个勇敢的女孩，两个运动的黑带、高腰裙装和高腰\\n',\n",
       " '在周三晚上，在孩子们的儿童书中，《说谎树》获奖的人将知道她不希望得到这个荣誉。\\n',\n",
       " '《教程》是一个机构，他们都说，彻底和及时的变革不可能很快就发生。\\n',\n",
       " '密歇根州政府还在与环保署协商后放弃了冲厕前的指令，而俄亥俄州的环保局局长则要求对测试进行修改，比如在费城使用的测试，“错误和不负责任”。\\n',\n",
       " '神秘围绕着被发现的第二个安全隐患发现隐藏在毒品主 Pablo Escobar 在迈阿密海滩的前家园。\\n',\n",
       " '5/7 Paul Smith 先生裤\\n',\n",
       " '在1月19日星期二，在阿富汗首都喀布尔的屋顶上，一个男孩挥舞着一根棍子，他的家鸽在屋顶上飞过。\\n',\n",
       " '根据对《电讯》发布的信息自由要求的回应，警方每周处理近5起案件，其中包括未经同意的18岁以下人士的明确照片。\\n',\n",
       " '2014年，中国食品安全问题超越国界，其中包括 KFC 、 Starbucks 和 MacDonald 等公司的上海供应商发现了不卫生的和过期的鸡肉。\\n',\n",
       " '她的心脏将会继续穿黑色的衣服，包括一个高领毛衣，裤腿和双膝的高跟鞋。\\n',\n",
       " '在2010年与《》杂志《时尚芭莎》的访谈中，超级模特说她的丈夫永远也不会吵架，说她在这么多年之后仍然觉得自己是个好男人。\\n',\n",
       " '花园的石珊瑚（凸起的床）种了一个单一的，鲜艳的红色的天竺葵。\\n',\n",
       " '将鸡放在微波炉中调味后，最好先将鸡放入微波炉内，然后再把你的手彻底洗干净。\\n',\n",
       " '《洛杉矶电影评论》编辑汉克·科尔赢得了他的电影编辑荣誉，以及他用不同的方式将影片剪下来的方式与人物的性格一致。\\n',\n",
       " '黄金海岸骑师杰夫·劳埃德，保罗·哈姆姆斯利和迈克尔·卡希尔将参加《魔法百万年》旧习。\\n',\n",
       " '忘了《时尚饮食》和《干洗一月》，现在是时候拥抱斯堪的纳维亚的 Hygge （声明了），感觉舒服和舒适，以及对自己有好处的时候了。\\n',\n",
       " '在伦敦的杜莎夫人蜡像旁边，凯莉米洛坐在她的新蜡笔旁，为她的新圣诞专辑《凯丽·圣诞节》宣传\\n',\n",
       " '在 Twitter 上的 johnmyers @ johnmers 注册，为我们的每日《基本政治通讯》注册\\n',\n",
       " '这些人从出生起就对鸡有轻微的加重和附加的重量。\\n',\n",
       " '他说，用一种怪异的方式，我变得更加节俭和小心，他还开着自己的旧汽车，穿着一条牛仔裤，每周吃几次便宜的食物。\\n',\n",
       " '这名美军士兵因泄露国家机密而被判处35年监禁。上个月，他在《卫报》上写道，在她被监禁时，她觉得“没有空，我觉得不存在”。\\n',\n",
       " '使用一些彩色铅笔和遮荫技术，霍华德充满了空白，放慢了他的幻觉生活\\n',\n",
       " '《卫报》关于欧洲庇护问题的看法：北\\n',\n",
       " '不过，《露》的故事在洛雷雷第一次破门到了哈珀的杂志的网页之前已经有10年的时间了，她的小说原本是在那里进行的。\\n',\n",
       " '在其他地方，穿上双排扣的检查夹克和折绒裤子被带了带黑色和白色斑点运动鞋的电梯。\\n',\n",
       " '我看着这个8岁的孩子，心想，“哦，我的天啊，这对一个8岁的年轻人来说是惊人的目标。”\\n',\n",
       " '《原子科学家公报》是世界上最先进的科学界之一，它将宣布它正在推进世界末日的时钟。\\n',\n",
       " '莎士比亚的《环球》：自1997年以来，重建的剧院一直是娱乐人群。\\n',\n",
       " '备用件包括穿的褶边裙子和茄克衫在一个漂亮的淡紫色的蓝色。\\n',\n",
       " '他带着黑色的棒球帽，黑色短袖衬衫和深色的黑色货车短裤，逃离了现场。\\n',\n",
       " '图像版权--辛普森/ E . Gregoryanz 图像描述一个金刚石顶锤装置中压缩的氢分子的艺术家的印象\\n',\n",
       " '弗朗西丝·科班和考特尼参加了《库尔特·科伦：2015年黑eck 蒙太奇》，在2015年的圣舞电影节上在马克剧院举行，1月。\\n',\n",
       " '在谈话过程中，他询问了女孩的内裤，甚至还给我发了一张照片，照片上她的名字涂满了她的姓名，在他的胸口上写着她的名字。\\n',\n",
       " '不管海鹰在亚利桑那的结果如何，他们可能会在周日晚回到西雅图，然后才知道他们去哪儿了。\\n',\n",
       " '《 Stenberg 》是该杂志二月的封面故事的主题，这个故事由艺人 Solange SolidelKnowles 讲述她在社会正义活动中的成长历程。\\n',\n",
       " '“直接对消费者的医疗保健即将到来，” Wojckicki 在上月的《财富》( Fortune Women )下月召开的“最有力的妇女”会议上说。\\n',\n",
       " '温赖特赢得《卫报》的一次采访后，在一次与马克·劳森的采访中表示，她将尝试让后续行动更加有趣。\\n',\n",
       " '在下载最新软件之前，我发现它不停地砸我的 iPhone ，同时我对《 Fitbit 应用程序》没有任何技术故障。\\n',\n",
       " '媒体政治编辑罗伯特·菲尔斯顿在周二晚上接受大学挑战主持人杰里米·帕克斯曼在《广播时报》的采访中亮相。\\n',\n",
       " '《周刊》的《1月》\\n',\n",
       " '海滩上有很多人，但还不错。\\n',\n",
       " '《圣经》中的一位天才著作《 Ruins 的上帝》讲述了二战轰炸机乌苏拉的弟弟泰迪德·托德的故事。\\n',\n",
       " '《旅程：一个难民的奥德赛》，从叙利亚到瑞典——《卫报》\\n',\n",
       " '在《圣经》中，他是一个非常有趣和迷人的舞台，他曾在 Bhaduri 拍摄过一部电影。\\n',\n",
       " '这部电影以《乔伊·曼加诺》的真实生活为基础，这位美国发明家和女商人在创建了“奇迹 Mop ”后，于90年代获得了名望。\\n',\n",
       " '不幸的是，《丹麦女孩》奥斯卡的荣誉，如果是这样，将只会鼓励好莱坞采取更多的措施，以获得真正的和赶上更广阔的世界。\\n',\n",
       " '来自河南省郑州市的一位女士张贴了一张由她自己的脂肪制成的肥皂，她的前男友因为他认为自己超重而被她的前男友们甩了出来，《人民日报》在线报告。\\n',\n",
       " '在海鹰王战胜维京人的历史上第三起最冷的游戏\\n',\n",
       " '在休息之后，迪伦的专辑《欲望号》发行后，参观了次年4月。\\n',\n",
       " '例如，白鸽与深灰色，海军，骆驼，卡其布和淡粉色搭配起来，但很可能用黄色、红色和生动的绿色。\\n',\n",
       " '这位官员说，美国仍然不接受朝鲜的说法：朝鲜试验了氢弹，但在试验结果被证明无效后进行了空气采样。\\n',\n",
       " '巴拿马《新闻先驱报》( http :// bit . ly /1RSUV )报告说，这名妇女在被送往医院时，就需要将儿童从世界的尽头送到医院。\\n',\n",
       " '莫斯科的圣诞节消息是为乌克兰的和平祈祷。在饱受战争蹂躏的国家，信徒们也是这样做的。\\n',\n",
       " '《原子科学家报》（《原子科学家报》（ BPA ）的背后的团队）表示，站在那里的人仍然是“不好的消息”。\\n',\n",
       " '我还能听到安娜·德·德维尔·史密斯是南非孤儿院的负责人，他的父母对一个即将死去的女孩表示感谢，因为她很荣幸能在《让我冷静的》中照顾她。\\n',\n",
       " '他的最后一部电影是灾难性的和抛弃的喜剧《惊悚的爱情》；在那之前，这是对真正的犯罪喜剧--戏剧美国人的喜爱。\\n',\n",
       " '在首次提出死刑时还不清楚，但《巴比伦法典》(《巴比伦法典》(由左、左)在其日期为17722BC 时，建议对25种不同的罪行判处死刑。\\n',\n",
       " '乔·威尔逊分享了一份《格拉斯哥杂志》《卖家》的照片，他买了一杯咖啡给他喝，并说谢谢你喝了热的饮料、金钱和他过去送给他的钱。\\n',\n",
       " '鉴于角色分支绝大多数是白人——《2012年时报》（2012年）的分析显示，当时的数据是88%的——是否存在种族偏见，有意识与否？\\n',\n",
       " '《福克斯新闻》、《现实电视》、这本书、源源不断的社交媒体纷纷窃窃私语，在亚利桑那州的那所房子里，一家与警察和布里斯托尔的宝贝妈妈上演了一场“家庭跑”。\\n',\n",
       " '保罗·麦凯纳在过去的10年中一直为人们创造出一个有前途的人，他能让他们变得苗条、富有、自信、快乐、聪明、戒烟和在一系列畅销书中睡懒觉。\\n',\n",
       " '海滩可以跳到缆车上去就可以了。\\n',\n",
       " '最近几周， Borusia Dormund 前锋皮埃尔·艾博尼可与伦敦北部一直有联系，但《每日邮报》称， Igalo 可能是一个更便宜的选择。\\n',\n",
       " '照片显示，他穿着脏兮兮的牛仔裤和 T恤，戴上手铐，戴上手铐，坐在凳子上，周围有警察和当地村民参加了狩猎。\\n',\n",
       " '他们是秘而不宣的，主要是夜间的独居和广阔的地区。\\n',\n",
       " '”汇丰银行( HSBC )的报纸《费思》( Feting )指出，中国的年通胀率为1.6%，周末公布，这对亚洲来说是坏消息。\\n',\n",
       " '这位博客作者写道：“德国的金发女人根据《古兰经》的说法，是可以根据你的怪念头或被奴役的行为而被虐待的女人。”\\n',\n",
       " '《科学报告》发表的这些研究结果揭示了这些动物的肌肉细胞的局限性——以及它们为什么需要如此迅速的原因。\\n',\n",
       " '新南威尔士地区艺术的首席执行官伊丽莎白·罗杰斯（ Elizabeth Rogers ）赞同布朗的呼吁，要求将资金用于在悉尼《生物多样性公约》之外的文化建设项目。\\n',\n",
       " '她是70年代电视连续剧《神奇女声》。\\n',\n",
       " '美国政府专家不相信该装置是氢弹，但表示将需要好几天时间才能确切地确定朝鲜发射的核武器类型。\\n',\n",
       " '吃了面包和灌木鸡尾酒!\\n',\n",
       " '据《每日快报》报道， Ivanovic 在斯坦福桥的时间为12万美元-1周，根据《每日快报》，这笔款项将继续作为他新合同的一部分支付。\\n',\n",
       " '在伦敦，莎士比亚的《环球》将从7月份开始展示来自不同来源的租借拷贝。）\\n',\n",
       " '在回应中，蒂尤说，它有一个“零容忍的做法，它与包括终身禁令和惩罚性金融惩罚在内的《网球反腐倡廉计划》的充分权力一起得到执行。\\n',\n",
       " '《崇高：河流》，多弗曼的表演贯穿于2月2日。\\n',\n",
       " '尽管美国电影在电影中的主要类别中再度占据主导地位，但英国优秀电影《 The Loblster 》、《丹麦女孩》和《前马吉娜》在内的一些著名人物都有一些著名人物。\\n',\n",
       " '《 El - Chapo 》揭示了什么？\\n',\n",
       " '然后它就去露营了一些内裤，而且每个人都向我们保证他们“走出了舒适的地带”。\\n',\n",
       " '但是，詹鹰队很冷淡，而且， Cycl克隆最终在3个月的3月40日的3号比赛中，最终将比分扳平。\\n',\n",
       " '死亡的鲸鱼从骷髅沙滩被移走\\n',\n",
       " '40多年前，通过了《公平住房法》，以结束住房方面的歧视，但它没有得到适当执行。\\n',\n",
       " '在星期三晚上的《九报》中，有超过627000美元用于森林火灾的受害者。\\n',\n",
       " '《每日星报》的照片在全国电视颁奖典礼上取得了成功之后，今天的晨星主持人威尔·威洛比和菲力浦·斯舍菲尔德在这一节目中扮演了重要的角色。\\n',\n",
       " '助产士们正在思考：“哦，上帝啊，我得把这个女人报告警察，因为她在这里是犯罪行为的中间吗？”\\n',\n",
       " '《星球大战》周刊《星球大战：垄断游戏》于今年9月在电影上映前几个月上映，《星球大战》公布在《星球大战》周刊《星球大战》周刊《星球大战》上。\\n',\n",
       " \"2014年，为电影《卖座电影》获得认可的黑人人才赢得最佳影片和最佳改编剧本奖，并获得了最佳支持女主角卢塔塔· Nyong '。\\n\",\n",
       " '伊恩·博姆森曾在《达勒姆》的全面纪录中记录了他创下的《双年史》的纪录，他对本·斯托克斯表示了敬意。\\n',\n",
       " '现在正在 Hermoosa 海滩打开 Bait & Brau ——艾米?斯卡特特佳\\n',\n",
       " '无衫的提姆·特首悬挂在沙滩上，在夏威夷度假时冲浪，10月在夏威夷度假。\\n',\n",
       " '拉塞尔·克劳（右）在《 Les Misrables 》（ Les Misrables ）的音乐改编（左）中看起来相当相似（左），他是维克多雨果1862年同名小说中的主要对手\\n',\n",
       " '10月，《华盛顿邮报》报道说，发生在机场附近一个帐篷市300多名移民的骚乱之后，卡尔登小镇发生了紧张局势。\\n',\n",
       " '乔治·卢卡斯道歉，澄清了关于新《星球大战》的评论\\n',\n",
       " 'Nadim Ladki 是黎巴嫩《每日星报》的编辑。\\n',\n",
       " '《公约》再一次得到了他的青睐。\\n',\n",
       " '但专家表示，朝鲜声称，爆炸的爆炸装置是氢弹，需要核实。\\n',\n",
       " '所有这些都是实质性的变化，将使我们的治理工作向更广泛的成员开放，并对《教程》产生重要和积极的影响。\\n',\n",
       " '星期五的早晨，悉尼的北部海滩沃洛沃斯在悉尼的北部海滩上对学校进行了威胁，悉尼西部的里士满和佩列兹也出现了威胁。\\n',\n",
       " '我认为真正的雄鹰猎人还活着只有50-60。\\n',\n",
       " '就像《西区故事》！\\n',\n",
       " '英国《金融时报》描述：英国《金融时报》表示，总理乔治·奥斯本将警告称，今年将是英国“衰退开始”，除非中国坚持艰难的经济改革。\\n',\n",
       " '\"同时，电影节还放映了一些电影，如\"\"作者----《年鉴》\"\"、\"\"某些妇女\"\"、\"\"自由世界\"\"、\"\" Christine \"\"和\"\" Sued \"\"。\"\\n',\n",
       " '博伊也成功地展示了一个成功的表演生涯，包括他作为一个寻求帮助他死去的星球的外国人，他在1976年的《男人》中找到了他。\\n',\n",
       " '事实上，氢弹已经成为拥有最大核能力的五个国家的全球标准：美国、俄罗斯、法国、英国和中国。\\n',\n",
       " '艾伦·索金，《史蒂夫·乔布斯》\\n',\n",
       " '一家商店的发言人对《电报》说：“他住在隔壁的房子里，他每天都在这里。”\\n',\n",
       " '《费城每日新闻》报道，这位61岁的受害者，身穿一件橘红色的长袍，1月他在加油站当彩票，当时他在一家彩票公司上班。\\n',\n",
       " '还有更多消息：他以前的农场被拖拉机犁过，现在是一名埃塞俄比亚商人，他在河岸上种植的稻子和豆子。\\n',\n",
       " '这符合1973年《公共记录法》，其中说个人或私人记录不应在一段时间内开放供公众查阅。\\n',\n",
       " '他在《北爱尔兰的戏剧和社区》的服务清单中获得了 OBE 。\\n',\n",
       " '尽管最新的《爱德·梅因斯登记册》的调查显示，鲁比?克鲁兹( Donald Moones )将唐纳德·特朗普( Donald Trump )拖到一边。\\n',\n",
       " '《应用自杀干预技巧训练》将帮助卡伯兰人认识到，当有人可能会自杀时，他们可以教他们怎样去接触他们，并确保他们的安全。\\n',\n",
       " '考伊拍摄了《让我们在澳大利亚舞蹈和中国女孩》的录像，这两个视频来自于1983年的《让我们的舞蹈专辑》，他最成功的唱片。\\n',\n",
       " 'Mila ，离开了，桑妮都吃了被认为有毒的煮熟的鸡。\\n',\n",
       " '这让我想起了我在《21世纪会议》期间遇到的第二个故事。\\n',\n",
       " '气象学家说，来自华盛顿到波士顿和俄亥俄山谷的数百万美国人可能会被暴风雪所笼罩。\\n',\n",
       " '伯特是票房之王，他的电影中包括 Deliverance 、 Maskey 和 Bancet （与萨莉菲尔德相比）和《沉默电影》\\n',\n",
       " '尽管有知识说搬到《梦想之剧院》是一个很有吸引力的建议，但曼联的高层们还是表示，他们将允许现任经理路易斯·范加尔至少能看到他的合约持续到2017年。\\n',\n",
       " '他在第一个奥斯卡金像奖之后受到了风头，现在在《丹麦女孩》第73届年度金球奖中获得了三项提名，其中包括最佳男主角。\\n',\n",
       " '《星期日泰晤士报》报道说，教育部正在调查这一事件，学校可能违反《平等法》。\\n',\n",
       " 'Kate Hudson 和 Chris Martin 于2015年3月14日在加州 Malibu 的沙滩上联合起来。\\n',\n",
       " '哦， AFL 的职业你将拥有（与舒斯医生道歉）\\n',\n",
       " '克鲁兹在《福克斯电视台的新闻》上说：“对 Marco 的投票将是一项大赦的投票。\\n',\n",
       " '去年从霍巴特搬到悉尼的花卉，是由尼克·福尔克撰写的《古龙街》丛书的插图。\\n',\n",
       " '《先驱太阳报》的理解是许多员工在上周四的最后一天，即2015年的最后一天举行了一个被列入名册的日子，而这一天不是公共假日。\\n',\n",
       " '其余都是历史：厄尔是一只火鸡，文森特·维加赚得约翰·特拉伏塔获得奥斯卡提名。\\n',\n",
       " '沼泽地，18天长的鸡吗？\\n',\n",
       " '但“奖项电影”只是在某一特定类别中创造一部电影，这就使电影《奥斯卡》中的一部电影自动失去了资格——慢慢地消失了。\\n',\n",
       " '罗杰斯于2005年获得了《好莱坞步行街》上的明星。\\n',\n",
       " '新闻集团打算把《星期日泰晤士报》卖给七珀斯吗?\\n',\n",
       " '《 ABC 新闻》第24章。\\n',\n",
       " '我想知道如果她没有穿那条裙子会不会发生什么事？\\n',\n",
       " '1990年，史蒂芬·金的电视改编《它》的观众吓坏了，这两个部分就进入了大银幕。\\n',\n",
       " '「 EurostingSongs挑战赛」的杰出的、宏伟的《神韵》比赛\\n',\n",
       " '《斯德哥尔摩警察》网站上的一份声明写道，我们的犯罪相对较少，很少有考虑到服务的人被捕。\\n',\n",
       " '据《纽约每日新闻》报道， Ku Klux Klan 在 MLK 假日周末向位于阿拉巴马州的流动社区的居民分发了征聘传单。\\n',\n",
       " '代表 Daleiden 发表的一项声明说，他使用了几十年来调查记者的同样秘密技巧，行使了《第一修正案》的言论自由和新闻自由，并遵循所有适用的法律。\\n',\n",
       " '在桑德森的墙纸工厂工作后，他在接受鸡尾酒会之前是一个洗碗机。\\n',\n",
       " '猫头鹰\\n',\n",
       " '《镜报》没有透露他的工资。\\n',\n",
       " '哦，是的，还有 CB 收音机。\\n',\n",
       " '根据《福布斯》杂志的报道，他的财富估计超过了120亿美元，其中列在法国和委内瑞拉的总统之上。\\n',\n",
       " '唯一的例外是伊德里恩·艾巴（根据在非洲内战中作战的一名儿童士兵的经验）和拳击电影《 Creed 》（根据美国内战中一名儿童士兵的经验）的杰出表现。\\n',\n",
       " '\"《加利福尼亚州劳工法》中的规定，规定雇员有权\"\"在合理时间内使用机构设施，以便举行有关行使[代表权]权利的会议\"\"。\"\\n',\n",
       " '虽然他无疑是最有影响力的乐手之一——他的最新专辑《黑星》上周才发布——他也是一个时尚偶像。\\n',\n",
       " '2016年电影《欣赏电影》\\n',\n",
       " '令人惊讶的发现：林赛（ Lindsey ）在他的父母家维吉尼亚海滩康复的同时，得知她的体重在上升，他震惊地得知自己的体重在上升\\n',\n",
       " '\"他对147名新闻干事的需求显然没有什么明显的需求，即根据福格公司的要求，大约几个月前公布了一份《新闻公报》，也透露了一个通讯\"\"家庭\"\"的221名成员。\"\\n',\n",
       " '据《 JAMA 神经病学》杂志周一的报道，这一发现反映了最年轻的患者在死亡后被诊断为广泛的 CTE --退行性大脑变化，伴随着一系列与头部反复发作有关的神经性精神病症状。\\n',\n",
       " '她的最新《某些女人》，用 mails - Meloy 改编的短篇小说改编为 Reichthardt ，讲述了她以前的工作所确定的严肃气氛。\\n',\n",
       " '哦，上帝！\\n',\n",
       " '爸，《星球大战》是什么？\\n',\n",
       " '据《密尔沃基周刊》报道， Hamzeh 周二出庭并告诉法官说，对他的指控是不真实的。\\n',\n",
       " '他们在一份对检察官的陈述中补充说，这些活动家的律师在线张贴，他们租用了一个办公室学习、印刷和邮寄几本书，其中包括《战略非暴力冲突：关于基本要素的思考》。\\n',\n",
       " '杰布.布什穿着衬衣、领带和西装裤子，与参议员林西·格雷厄姆一起在木镶板房里工作。\\n',\n",
       " '报道说，雪停止后，挖掘的时间线还不清楚，华盛顿的应急管理主任克里斯?盖费斯斯于上周日告诉《哥伦比亚广播公司新闻》。\\n',\n",
       " '皮蛋也是这部电影的非官方顾问，《星球大战》巨兽经常与他的《星际迷航》导演 J . J . Abrams 分享他的想法。\\n',\n",
       " '如今的大都市，宽林荫大道和新古典主义的城堡，在那里有着豪华的殖民别墅和现代化的摩天大楼，是阿根廷国家的核心。\\n',\n",
       " '在6月的一次拍卖会上，在被拍卖的14件作品中，水彩景观、裸照和生活仍然是在《纽伦堡》的拍卖中的14块。\\n',\n",
       " '在《背风大战》是他的座右铭，并在《顿顿修道院》中度过了一辈子的社会观察，我认为他可能会根据人类的小型戏剧而写他想要的东西，并且人们相信他。\\n',\n",
       " '《全明星周》将以每年10月的世界系列中最佳的美国和全国联盟的主场优势为目标。\\n',\n",
       " '《信息自由法》之下的邮件在请求英格兰的区域救护车服务后获得了《信息自由法》中的呼叫数字，这些服务在2012-13、2013-14和2014-15财政年度的医疗紧急情况下，有多少人参加了监狱。\\n',\n",
       " '接着，弗兰克接着又继续下大雨落在德斯蒙德已经饱和的土地上。\\n',\n",
       " '在该照会中发现了两个名字，但警察无法找到其存在的任何痕迹，因为找到了新南威尔士州警察局的记录以及澳大利亚联邦警察局、维多利亚州和昆士兰警察局以及其他机构的《 Boumemans Bay Post 报》的记录。\\n',\n",
       " '《大矮子》，《在外面，艾美》在制作人协会的奖项中获奖\\n',\n",
       " '斑点猫头鹰开始使我们的社会垮台，然后（总统）克林顿把斯坦斯山脉变成了一片荒野之地，或者是什么。\\n',\n",
       " '新颖的蒸汽盒，完美的保存肉质和潮湿，是硅胶制成的，使它具有灵活性，使它可以适合不同尺寸的鸡。\\n',\n",
       " ...]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading as Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"text\", \n",
    "                       data_files={\n",
    "                           \"train\": [\"cc-matrix-enzh-0to30M.en\", \"cc-matrix-enzh-0to30M.zh\"]                           }\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 962336\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 240584\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted_data = dataset['train'].train_test_split(test_size=0.2)\n",
    "splitted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '壳牌公司的一位发言人对《星期日泰晤士报》表示，壳牌在支付税款方面是多么透明，并于2014年在英国造成损失。'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted_data[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking into the checkpoint issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From OMNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model', 'generator', 'vocab', 'opt'])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "omnt_checkpoint = torch.load('nllb-200-600M-onmt.pt', map_location=torch.device('cpu'))\n",
    "omnt_checkpoint.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(config='', save_config=None, data={}, skip_empty_level='silent', save_data='', overwrite=False, n_sample=0, dump_transforms=False, src_vocab='', tgt_vocab='', share_vocab=True, src_feats_vocab=None, src_vocab_size=256206, tgt_vocab_size=256206, vocab_size_multiple=1, src_words_min_frequency=1, tgt_words_min_frequency=1, src_seq_length_trunc=None, tgt_seq_length_trunc=None, both_embeddings=None, src_embeddings=None, tgt_embeddings=None, embeddings_type=None, switchout_temperature=1.0, tokendrop_temperature=1.0, tokenmask_temperature=1.0, reversible_tokenization='joiner', prior_tokenization=False, src_subword_model='', tgt_subword_model='', src_subword_nbest=1, tgt_subword_nbest=1, src_subword_alpha=0.0, tgt_subword_alpha=0.0, src_subword_vocab='', tgt_subword_vocab='', src_vocab_threshold=0, tgt_vocab_threshold=0, src_subword_type='none', tgt_subword_type='none', src_onmttok_kwargs=\"{'mode': 'none'}\", tgt_onmttok_kwargs=\"{'mode': 'none'}\", src_seq_length=150, tgt_seq_length=150, src_prefix='', tgt_prefix='', permute_sent_ratio=0.0, rotate_ratio=0.0, insert_ratio=0.0, random_ratio=0.0, mask_ratio=0.0, mask_length='subword', poisson_lambda=3.0, replace_length=-1, src_word_vec_size=1024, tgt_word_vec_size=1024, word_vec_size=1024, share_decoder_embeddings=True, share_embeddings=True, position_encoding=True, position_encoding_type='SinusoidalConcat', update_vocab=False, feat_merge='concat', feat_vec_size=-1, feat_vec_exponent=0.7, model_task='seq2seq', model_type='text', model_dtype='fp16', encoder_type='transformer', decoder_type='transformer', freeze_encoder=False, freeze_decoder=False, layers=-1, enc_layers=12, dec_layers=12, hidden_size=1024, enc_hid_size=1024, dec_hid_size=1024, cnn_kernel_width=3, pos_ffn_activation_fn='relu', input_feed=1, bridge=False, rnn_type='LSTM', context_gate=None, bridge_extra_node=True, bidir_edges=True, state_dim=512, n_edge_types=2, n_node=2, n_steps=2, src_ggnn_size=0, global_attention='general', global_attention_function='softmax', self_attn_type='scaled-dot', max_relative_positions=0, heads=16, transformer_ff=4096, aan_useffn=False, add_qkvbias=True, lambda_align=0.0, alignment_layer=-3, alignment_heads=0, full_context_alignment=False, copy_attn=False, copy_attn_type='general', generator_function='softmax', copy_attn_force=False, reuse_copy_attn=False, copy_loss_by_seqlength=False, coverage_attn=False, lambda_coverage=0.0, lm_prior_model=None, lm_prior_lambda=0.0, lm_prior_tau=1.0, loss_scale=0, apex_opt_level='', data_type='text', save_model='nllb', save_checkpoint_steps=5000, keep_checkpoint=50, gpu_ranks=[0], world_size=1, gpu_backend='nccl', gpu_verbose_level=0, master_ip='localhost', master_port=10000, seed=1234, param_init=0.0, param_init_glorot=True, train_from='', reset_optim='none', pre_word_vecs_enc=None, pre_word_vecs_dec=None, freeze_word_vecs_enc=False, freeze_word_vecs_dec=False, num_workers=4, batch_size=8192, batch_size_multiple=1, batch_type='tokens', normalization='tokens', accum_count=[4], accum_steps=[0], valid_steps=5000, valid_batch_size=4096, train_steps=100000, single_pass=False, early_stopping=0, early_stopping_criteria=None, optim='', adagrad_accumulator_init=0, max_grad_norm=0.0, dropout=[0.1], attention_dropout=[0.1], dropout_steps=[0], truncated_decoder=0, adam_beta1=0.9, adam_beta2=0.98, label_smoothing=0.1, average_decay=0.0, average_every=1, learning_rate=5e-05, learning_rate_decay=0.5, start_decay_steps=50000, decay_steps=10000, decay_method='none', warmup_steps=4000, log_file='', log_file_level='0', verbose=False, train_eval_steps=200, train_metrics=[], valid_metrics=[], scoring_debug=False, dump_preds=None, report_every=100, exp_host='', exp='', tensorboard=False, tensorboard_log_dir='runs/onmt', bucket_size=262144, bucket_size_init=-1, bucket_size_increment=0, prefetch_factor=400, brnn=False, data_task='seq2seq', decoder_start_token='</s>', _all_transform={'filtertoolong'})"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omnt_checkpoint['opt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['src', 'tgt', 'data_task', 'decoder_start_token'])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omnt_checkpoint['vocab'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256206\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '<blank>',\n",
       " '</s>',\n",
       " '<unk>',\n",
       " 'an',\n",
       " '▁n',\n",
       " '▁m',\n",
       " '▁t',\n",
       " '▁k',\n",
       " '▁a',\n",
       " '▁s',\n",
       " 'en',\n",
       " 'in',\n",
       " '▁d',\n",
       " 'er',\n",
       " 'la',\n",
       " '▁b',\n",
       " '▁p',\n",
       " 'on',\n",
       " 'ar',\n",
       " 'at',\n",
       " 'is',\n",
       " 'le',\n",
       " '▁e',\n",
       " 'un',\n",
       " 'li',\n",
       " '▁o',\n",
       " '▁v',\n",
       " '▁h',\n",
       " '▁c',\n",
       " '▁i',\n",
       " 'ak',\n",
       " 'as',\n",
       " 'es',\n",
       " 'am',\n",
       " '▁y',\n",
       " 'it',\n",
       " 'or',\n",
       " '▁g',\n",
       " 'et',\n",
       " '▁w',\n",
       " 'al',\n",
       " '▁f',\n",
       " 'AT',\n",
       " 'IN',\n",
       " '▁<',\n",
       " 'ED',\n",
       " 'ATA',\n",
       " 'MIN',\n",
       " 'DATA',\n",
       " 'MINED',\n",
       " '▁r',\n",
       " 'ir',\n",
       " 'ang',\n",
       " 'ti',\n",
       " '▁l',\n",
       " 'ad',\n",
       " 'os',\n",
       " 'om',\n",
       " 'ur',\n",
       " 'el',\n",
       " '▁j',\n",
       " '▁na',\n",
       " 'us',\n",
       " 'em',\n",
       " 'ch',\n",
       " 'ik',\n",
       " 'um',\n",
       " 'ga',\n",
       " '▁\"',\n",
       " '▁A',\n",
       " 'ta',\n",
       " '▁M',\n",
       " 'il',\n",
       " '▁ا',\n",
       " 'im',\n",
       " 'ed',\n",
       " 'ek',\n",
       " '▁u',\n",
       " '▁de',\n",
       " 'wa',\n",
       " '▁.',\n",
       " '▁la',\n",
       " '▁S',\n",
       " '▁N',\n",
       " 'id',\n",
       " 'te',\n",
       " 'ing',\n",
       " 'ay',\n",
       " '▁-',\n",
       " '▁ku',\n",
       " 'ok',\n",
       " 'ra',\n",
       " 'ol',\n",
       " '▁1',\n",
       " '▁ب',\n",
       " '▁le',\n",
       " 'ia',\n",
       " '▁K',\n",
       " '▁ka',\n",
       " '▁z',\n",
       " 'uk',\n",
       " 'ha',\n",
       " 'ba',\n",
       " '▁(',\n",
       " 'ut',\n",
       " 'на',\n",
       " 'da',\n",
       " '▁in',\n",
       " '▁T',\n",
       " 'he',\n",
       " '▁an',\n",
       " '▁ma',\n",
       " '▁م',\n",
       " '▁B',\n",
       " 'ya',\n",
       " 'ie',\n",
       " '▁I',\n",
       " 'ul',\n",
       " 're',\n",
       " '▁с',\n",
       " '▁P',\n",
       " '▁D',\n",
       " '▁क',\n",
       " 'ot',\n",
       " 'ana',\n",
       " '▁E',\n",
       " '▁sa',\n",
       " '▁б',\n",
       " 'od',\n",
       " '▁se',\n",
       " '▁pr',\n",
       " 'та',\n",
       " '▁ta',\n",
       " '▁ک',\n",
       " '▁q',\n",
       " 'ان',\n",
       " '▁к',\n",
       " 'ent',\n",
       " 'ov',\n",
       " '▁2',\n",
       " '▁و',\n",
       " '▁п',\n",
       " 'hi',\n",
       " 'да',\n",
       " 'di',\n",
       " '▁,',\n",
       " 'ag',\n",
       " 'st',\n",
       " 'ci',\n",
       " '▁di',\n",
       " 'ap',\n",
       " '▁C',\n",
       " '▁en',\n",
       " 'ра',\n",
       " '▁L',\n",
       " '▁J',\n",
       " 'de',\n",
       " '▁ni',\n",
       " '▁un',\n",
       " '▁ya',\n",
       " '▁ba',\n",
       " 'ka',\n",
       " 'ng',\n",
       " '▁د',\n",
       " 'ен',\n",
       " '▁H',\n",
       " '▁ne',\n",
       " 'ла',\n",
       " 'na',\n",
       " '▁da',\n",
       " 'si',\n",
       " '▁te',\n",
       " 'ho',\n",
       " 'ri',\n",
       " '▁ng',\n",
       " 'ong',\n",
       " '▁li',\n",
       " '▁ت',\n",
       " '▁м',\n",
       " 'eng',\n",
       " 'bi',\n",
       " 'og',\n",
       " 'we',\n",
       " 'ma',\n",
       " 'aka',\n",
       " 'tu',\n",
       " 'ye',\n",
       " 'ara',\n",
       " '..',\n",
       " 'and',\n",
       " '▁в',\n",
       " '{{',\n",
       " '▁O',\n",
       " '▁स',\n",
       " 'ار',\n",
       " '▁qu',\n",
       " 'ja',\n",
       " '▁Y',\n",
       " 'ig',\n",
       " '▁wa',\n",
       " 'se',\n",
       " '▁to',\n",
       " 'ku',\n",
       " 'za',\n",
       " 'то',\n",
       " '▁ti',\n",
       " 'zi',\n",
       " 'to',\n",
       " 'sa',\n",
       " 'eg',\n",
       " '▁{{',\n",
       " '▁G',\n",
       " '▁и',\n",
       " '\".',\n",
       " 'те',\n",
       " '▁mu',\n",
       " '▁о',\n",
       " '▁а',\n",
       " '▁si',\n",
       " 'op',\n",
       " '▁ह',\n",
       " 'be',\n",
       " 'ung',\n",
       " 'не',\n",
       " 'ou',\n",
       " 'va',\n",
       " '▁ه',\n",
       " '▁ch',\n",
       " '▁д',\n",
       " '▁at',\n",
       " 'ina',\n",
       " 'ва',\n",
       " '▁th',\n",
       " 'ни',\n",
       " '▁al',\n",
       " '▁س',\n",
       " '▁ل',\n",
       " \"▁'\",\n",
       " '▁me',\n",
       " 'ni',\n",
       " '▁ال',\n",
       " '▁ن',\n",
       " '▁з',\n",
       " 'ati',\n",
       " '▁ha',\n",
       " '▁W',\n",
       " 'ud',\n",
       " '▁is',\n",
       " 'ke',\n",
       " '▁R',\n",
       " '▁म',\n",
       " 'yo',\n",
       " 'ua',\n",
       " '▁no',\n",
       " '▁на',\n",
       " 'je',\n",
       " 'ти',\n",
       " 'ata',\n",
       " '▁am',\n",
       " '▁V',\n",
       " '▁U',\n",
       " '▁ko',\n",
       " '▁प',\n",
       " '▁ke',\n",
       " 'ро',\n",
       " 'ре',\n",
       " 'ob',\n",
       " 'ter',\n",
       " 'но',\n",
       " 'ani',\n",
       " 'ce',\n",
       " '▁re',\n",
       " 'pa',\n",
       " 'ne',\n",
       " 'ere',\n",
       " 'bu',\n",
       " '▁F',\n",
       " 'pe',\n",
       " 've',\n",
       " '▁be',\n",
       " '▁on',\n",
       " 'ال',\n",
       " '▁ar',\n",
       " '...',\n",
       " '▁ak',\n",
       " 'ка',\n",
       " '▁va',\n",
       " 'ли',\n",
       " 'ari',\n",
       " 'ist',\n",
       " 'lar',\n",
       " 'ama',\n",
       " 'ри',\n",
       " 'ini',\n",
       " '▁nga',\n",
       " '▁tr',\n",
       " '▁ئ',\n",
       " 'bo',\n",
       " '्य',\n",
       " 'hu',\n",
       " '▁ر',\n",
       " 'ant',\n",
       " '▁je',\n",
       " 'ona',\n",
       " 'ле',\n",
       " 'era',\n",
       " 'ki',\n",
       " '▁pa',\n",
       " 'ས་',\n",
       " '▁20',\n",
       " '▁per',\n",
       " 'ro',\n",
       " 'ны',\n",
       " 'ca',\n",
       " '▁3',\n",
       " '▁mo',\n",
       " '▁mi',\n",
       " '▁ny',\n",
       " 'ika',\n",
       " 'ji',\n",
       " 'ada',\n",
       " 'га',\n",
       " 'end',\n",
       " '▁so',\n",
       " '▁अ',\n",
       " 'ст',\n",
       " '▁su',\n",
       " '▁پ',\n",
       " '▁א',\n",
       " '▁bi',\n",
       " 'ма',\n",
       " 'ون',\n",
       " 'isa',\n",
       " '▁tu',\n",
       " '▁es',\n",
       " '▁el',\n",
       " '▁ki',\n",
       " 'me',\n",
       " '▁đ',\n",
       " '▁que',\n",
       " 'wo',\n",
       " '▁er',\n",
       " 'го',\n",
       " '▁st',\n",
       " 'ita',\n",
       " 'lo',\n",
       " 'wi',\n",
       " 'ug',\n",
       " '▁the',\n",
       " '▁ج',\n",
       " '▁for',\n",
       " '▁त',\n",
       " 'ko',\n",
       " 'est',\n",
       " '▁у',\n",
       " 'vi',\n",
       " '▁ब',\n",
       " 'ای',\n",
       " '▁do',\n",
       " 'ene',\n",
       " '▁ش',\n",
       " '▁ga',\n",
       " '▁con',\n",
       " '▁ж',\n",
       " '▁ف',\n",
       " '▁по',\n",
       " '▁ye',\n",
       " '▁ي',\n",
       " 'ny',\n",
       " 'ane',\n",
       " 'ི་',\n",
       " 'ze',\n",
       " '▁fa',\n",
       " '▁ق',\n",
       " '्र',\n",
       " 'ло',\n",
       " 'ira',\n",
       " 'vo',\n",
       " 'ست',\n",
       " 'ко',\n",
       " '▁ज',\n",
       " '▁vi',\n",
       " 'ند',\n",
       " 'up',\n",
       " '▁?',\n",
       " '▁आ',\n",
       " 'ང་',\n",
       " '▁as',\n",
       " 'uma',\n",
       " '▁po',\n",
       " 'ер',\n",
       " 'ine',\n",
       " '▁за',\n",
       " '▁أ',\n",
       " 'ور',\n",
       " '▁خ',\n",
       " '▁x',\n",
       " '▁т',\n",
       " '▁kan',\n",
       " 'ena',\n",
       " 'nd',\n",
       " '▁да',\n",
       " '▁bo',\n",
       " 'ic',\n",
       " '▁go',\n",
       " 'ver',\n",
       " 'eh',\n",
       " '▁آ',\n",
       " '▁ra',\n",
       " '▁et',\n",
       " '▁og',\n",
       " '▁é',\n",
       " '▁par',\n",
       " '▁ho',\n",
       " 'asi',\n",
       " 'asa',\n",
       " '▁ה',\n",
       " '▁ع',\n",
       " 'th',\n",
       " 'ार',\n",
       " 'ib',\n",
       " '▁न',\n",
       " '▁ja',\n",
       " 'qu',\n",
       " '▁그',\n",
       " 'isi',\n",
       " 'so',\n",
       " 'any',\n",
       " 'ova',\n",
       " 'eni',\n",
       " '▁ol',\n",
       " 'ele',\n",
       " 'во',\n",
       " 'ind',\n",
       " '▁hi',\n",
       " '▁4',\n",
       " '▁lo',\n",
       " '▁ба',\n",
       " '▁men',\n",
       " '▁ve',\n",
       " '▁19',\n",
       " 'oni',\n",
       " 'gi',\n",
       " 'ام',\n",
       " 'lam',\n",
       " 'pr',\n",
       " 'ры',\n",
       " '▁za',\n",
       " 'po',\n",
       " '▁com',\n",
       " '▁ك',\n",
       " '▁of',\n",
       " '▁та',\n",
       " '▁द',\n",
       " '▁ag',\n",
       " '▁im',\n",
       " 'ու',\n",
       " 'iri',\n",
       " 'olo',\n",
       " '00',\n",
       " '▁ক',\n",
       " '▁du',\n",
       " 'are',\n",
       " 'no',\n",
       " 'ین',\n",
       " 'ို',\n",
       " '▁не',\n",
       " 'ts',\n",
       " 'han',\n",
       " 'iti',\n",
       " 'ات',\n",
       " 'eri',\n",
       " 'лы',\n",
       " 'lu',\n",
       " '▁व',\n",
       " 'ab',\n",
       " '▁ang',\n",
       " 'ore',\n",
       " 'ge',\n",
       " 'tr',\n",
       " '▁5',\n",
       " 'pi',\n",
       " '▁के',\n",
       " '▁dan',\n",
       " '்க',\n",
       " 'yi',\n",
       " 'ди',\n",
       " 'ju',\n",
       " 'де',\n",
       " '▁nd',\n",
       " '▁τ',\n",
       " '▁man',\n",
       " 'ula',\n",
       " '▁ш',\n",
       " 'kan',\n",
       " 'iz',\n",
       " 'اد',\n",
       " 'fa',\n",
       " 'una',\n",
       " 'adi',\n",
       " '▁х',\n",
       " 'ate',\n",
       " '니다',\n",
       " '▁ल',\n",
       " '▁um',\n",
       " 'aya',\n",
       " '▁ко',\n",
       " '▁ی',\n",
       " 'ན་',\n",
       " 'য়',\n",
       " '▁گ',\n",
       " '▁yang',\n",
       " '▁ab',\n",
       " 'und',\n",
       " 'eb',\n",
       " 'ela',\n",
       " 'mi',\n",
       " '▁ч',\n",
       " 'ub',\n",
       " '▁र',\n",
       " 'င်',\n",
       " 'ды',\n",
       " '▁Ma',\n",
       " '▁Z',\n",
       " 'anga',\n",
       " 'sh',\n",
       " 'ond',\n",
       " '▁ir',\n",
       " '▁می',\n",
       " 'ан',\n",
       " '▁min',\n",
       " '▁이',\n",
       " '▁yo',\n",
       " '▁ad',\n",
       " 'र्',\n",
       " 'ന്',\n",
       " '▁я',\n",
       " '▁ap',\n",
       " '▁..',\n",
       " '▁and',\n",
       " 'one',\n",
       " '▁til',\n",
       " '▁om',\n",
       " '▁An',\n",
       " ':1',\n",
       " 'ول',\n",
       " 'eka',\n",
       " '▁bu',\n",
       " '▁،',\n",
       " '▁ver',\n",
       " '▁iz',\n",
       " '▁е',\n",
       " '▁ל',\n",
       " 'ད་',\n",
       " '▁sam',\n",
       " '▁چ',\n",
       " 'lah',\n",
       " 'alo',\n",
       " '▁kw',\n",
       " 'ru',\n",
       " '्या',\n",
       " 'go',\n",
       " '▁il',\n",
       " '▁ка',\n",
       " 'ви',\n",
       " '▁die',\n",
       " 'ки',\n",
       " 'ين',\n",
       " '▁به',\n",
       " '▁ri',\n",
       " '▁ہ',\n",
       " 'fi',\n",
       " 'ake',\n",
       " '▁em',\n",
       " '▁he',\n",
       " '▁ح',\n",
       " 'َّ',\n",
       " '▁van',\n",
       " 'ura',\n",
       " '▁kon',\n",
       " 'ме',\n",
       " 'てい',\n",
       " 'ako',\n",
       " 'uka',\n",
       " 'ens',\n",
       " 'ost',\n",
       " 'ान',\n",
       " 'ты',\n",
       " 'oko',\n",
       " 'ін',\n",
       " '▁les',\n",
       " '▁מ',\n",
       " 'uri',\n",
       " 'ան',\n",
       " 'lik',\n",
       " 'са',\n",
       " '▁del',\n",
       " 'ann',\n",
       " 'ite',\n",
       " '▁እ',\n",
       " '▁mat',\n",
       " 'ulu',\n",
       " '▁ber',\n",
       " '▁Na',\n",
       " '▁av',\n",
       " 'les',\n",
       " '▁kom',\n",
       " 'ano',\n",
       " 'ami',\n",
       " 'hat',\n",
       " 'ón',\n",
       " 'lem',\n",
       " '▁est',\n",
       " '▁pas',\n",
       " '▁।',\n",
       " 'ler',\n",
       " '▁6',\n",
       " '▁or',\n",
       " '▁ter',\n",
       " '▁ik',\n",
       " 'ans',\n",
       " '▁...',\n",
       " '▁af',\n",
       " '▁се',\n",
       " '▁আ',\n",
       " 'ें',\n",
       " 'yn',\n",
       " '▁pe',\n",
       " 'co',\n",
       " '▁있',\n",
       " '▁të',\n",
       " '▁vo',\n",
       " 'ire',\n",
       " 'ക്',\n",
       " '▁প',\n",
       " '▁op',\n",
       " 'ەر',\n",
       " 'jo',\n",
       " 'for',\n",
       " 'aku',\n",
       " 'ру',\n",
       " 'ala',\n",
       " '▁э',\n",
       " 'ör',\n",
       " '▁ai',\n",
       " '▁य',\n",
       " 'ui',\n",
       " '▁А',\n",
       " 'ου',\n",
       " '▁স',\n",
       " '▁den',\n",
       " '▁med',\n",
       " 'ima',\n",
       " '▁है',\n",
       " 'ete',\n",
       " '▁ɖ',\n",
       " '▁در',\n",
       " '▁gi',\n",
       " '▁we',\n",
       " 'ला',\n",
       " '▁Б',\n",
       " 'wan',\n",
       " 'ess',\n",
       " '▁ge',\n",
       " '▁ग',\n",
       " ').',\n",
       " 'aki',\n",
       " 'ora',\n",
       " 'ми',\n",
       " 'ber',\n",
       " '▁Al',\n",
       " 'نى',\n",
       " '▁до',\n",
       " '▁pro',\n",
       " '▁mar',\n",
       " '▁Т',\n",
       " '▁ব',\n",
       " '▁som',\n",
       " '▁à',\n",
       " 'ish',\n",
       " 'är',\n",
       " '▁det',\n",
       " 'ɔn',\n",
       " '▁201',\n",
       " 'ود',\n",
       " '▁भ',\n",
       " 'лә',\n",
       " '▁bil',\n",
       " '▁ex',\n",
       " '்த',\n",
       " 'all',\n",
       " '▁उ',\n",
       " '▁ક',\n",
       " 'ци',\n",
       " 'وا',\n",
       " 'мо',\n",
       " 'ག་',\n",
       " 'ten',\n",
       " '▁ש',\n",
       " '\",',\n",
       " 'yd',\n",
       " '▁des',\n",
       " 'ེ་',\n",
       " 'сы',\n",
       " 'ys',\n",
       " 'elo',\n",
       " '▁कर',\n",
       " 'zo',\n",
       " '▁[',\n",
       " 'és',\n",
       " 'amb',\n",
       " '्त',\n",
       " '▁nu',\n",
       " '▁sp',\n",
       " '▁sk',\n",
       " 'ente',\n",
       " '▁In',\n",
       " '▁бо',\n",
       " 'mo',\n",
       " 'ord',\n",
       " 'した',\n",
       " 'ста',\n",
       " 'لى',\n",
       " 'ın',\n",
       " 'atu',\n",
       " '▁π',\n",
       " '▁har',\n",
       " 'ell',\n",
       " 'ль',\n",
       " 'ato',\n",
       " '▁г',\n",
       " '▁mak',\n",
       " 'ம்',\n",
       " 'itu',\n",
       " '▁ב',\n",
       " 'ன்',\n",
       " 'ck',\n",
       " 'ар',\n",
       " 'ali',\n",
       " 'је',\n",
       " 'esu',\n",
       " '▁ek',\n",
       " '▁nh',\n",
       " 'ust',\n",
       " 'ве',\n",
       " '▁К',\n",
       " '▁М',\n",
       " '▁sh',\n",
       " 'inn',\n",
       " '▁ए',\n",
       " 'än',\n",
       " 'int',\n",
       " 'ུ་',\n",
       " 'ehova',\n",
       " 'ip',\n",
       " '▁mas',\n",
       " '▁7',\n",
       " 'gu',\n",
       " 'eke',\n",
       " '▁der',\n",
       " '▁ay',\n",
       " '▁ac',\n",
       " '▁би',\n",
       " 'lan',\n",
       " 'મા',\n",
       " 'ith',\n",
       " 'oku',\n",
       " 'ля',\n",
       " '▁са',\n",
       " 'ort',\n",
       " '▁ca',\n",
       " '▁10',\n",
       " 'usa',\n",
       " 'ने',\n",
       " '▁то',\n",
       " '▁С',\n",
       " 'ise',\n",
       " 'eta',\n",
       " 'رد',\n",
       " '▁pi',\n",
       " '▁kun',\n",
       " 'َّ',\n",
       " '▁با',\n",
       " 'ही',\n",
       " 'こと',\n",
       " '▁ز',\n",
       " '▁қ',\n",
       " 'ོ་',\n",
       " 'нда',\n",
       " 'ue',\n",
       " 'ino',\n",
       " '▁ọ',\n",
       " 'ার',\n",
       " 'се',\n",
       " 'ach',\n",
       " '▁it',\n",
       " '▁것',\n",
       " 'ast',\n",
       " '▁8',\n",
       " '▁мо',\n",
       " '▁tal',\n",
       " 'ར་',\n",
       " 'ons',\n",
       " '▁š',\n",
       " 'lig',\n",
       " 'ila',\n",
       " '▁nan',\n",
       " 'man',\n",
       " 'pan',\n",
       " '▁하',\n",
       " '▁by',\n",
       " '▁ph',\n",
       " '▁σ',\n",
       " 'до',\n",
       " 'اب',\n",
       " '▁এ',\n",
       " '▁که',\n",
       " '▁бу',\n",
       " 'лу',\n",
       " 'ော',\n",
       " 'uh',\n",
       " 'ris',\n",
       " 'ran',\n",
       " 'per',\n",
       " 'ता',\n",
       " 'ます',\n",
       " 'ida',\n",
       " 'ത്',\n",
       " 'να',\n",
       " '▁aw',\n",
       " 'las',\n",
       " 'ori',\n",
       " '▁mem',\n",
       " '▁oku',\n",
       " '▁!',\n",
       " '▁uk',\n",
       " 'ade',\n",
       " '▁od',\n",
       " 'եր',\n",
       " '▁ਕ',\n",
       " 'oma',\n",
       " 'ile',\n",
       " 'zu',\n",
       " 'iv',\n",
       " 'art',\n",
       " 'ban',\n",
       " 'න්',\n",
       " 'ende',\n",
       " 'ill',\n",
       " '▁ji',\n",
       " '▁ત',\n",
       " '▁Ch',\n",
       " '▁α',\n",
       " '▁che',\n",
       " '▁9',\n",
       " 'án',\n",
       " 'ים',\n",
       " '▁사',\n",
       " '▁ob',\n",
       " '▁kwa',\n",
       " 'ени',\n",
       " 'tt',\n",
       " '▁ts',\n",
       " 'imo',\n",
       " 'ید',\n",
       " '▁про',\n",
       " 'ות',\n",
       " 'ја',\n",
       " '▁mga',\n",
       " '▁ž',\n",
       " '▁kh',\n",
       " 'аб',\n",
       " '▁lu',\n",
       " 'ना',\n",
       " 'gan',\n",
       " 'िक',\n",
       " '▁ε',\n",
       " '▁і',\n",
       " '▁का',\n",
       " 'する',\n",
       " 'sk',\n",
       " '▁ਹ',\n",
       " 'ня',\n",
       " '▁200',\n",
       " 'ть',\n",
       " '▁κ',\n",
       " '▁କ',\n",
       " 'ning',\n",
       " '▁dis',\n",
       " 'ٱل',\n",
       " 'ero',\n",
       " 'ër',\n",
       " 'wu',\n",
       " 'ար',\n",
       " 'лі',\n",
       " 'cha',\n",
       " 'amp',\n",
       " 'ɛn',\n",
       " '▁wo',\n",
       " '▁ser',\n",
       " 'ct',\n",
       " '▁cu',\n",
       " 'ito',\n",
       " 'iko',\n",
       " 'ure',\n",
       " 'за',\n",
       " '▁П',\n",
       " 'lit',\n",
       " '▁Ne',\n",
       " 'ले',\n",
       " '▁ok',\n",
       " '▁bir',\n",
       " 'ai',\n",
       " 'لا',\n",
       " '▁fi',\n",
       " '▁ci',\n",
       " '▁pag',\n",
       " 'لار',\n",
       " '▁az',\n",
       " 'du',\n",
       " '▁Н',\n",
       " '▁co',\n",
       " 'fe',\n",
       " 'ly',\n",
       " '▁kar',\n",
       " 'oka',\n",
       " '▁μ',\n",
       " 'ont',\n",
       " '▁પ',\n",
       " 'che',\n",
       " 'лар',\n",
       " '▁kas',\n",
       " '▁ਦ',\n",
       " 'nh',\n",
       " '▁ত',\n",
       " 'ما',\n",
       " 'ку',\n",
       " '▁var',\n",
       " 'fu',\n",
       " 'ні',\n",
       " '▁ਸ',\n",
       " 'िय',\n",
       " '▁ম',\n",
       " '▁á',\n",
       " 'را',\n",
       " 'ча',\n",
       " 'iki',\n",
       " 'lim',\n",
       " 'der',\n",
       " '▁Is',\n",
       " '▁pre',\n",
       " 'ებ',\n",
       " '▁yi',\n",
       " 'ന്ന',\n",
       " 'ес',\n",
       " '▁por',\n",
       " '▁ө',\n",
       " 'ту',\n",
       " '▁हो',\n",
       " 'ца',\n",
       " '▁प्र',\n",
       " 'let',\n",
       " 'ah',\n",
       " '▁ndi',\n",
       " '▁च',\n",
       " '▁ра',\n",
       " '▁Q',\n",
       " 'ない',\n",
       " '▁sem',\n",
       " '▁را',\n",
       " 'ის',\n",
       " '▁إ',\n",
       " '▁het',\n",
       " '▁ah',\n",
       " '▁Se',\n",
       " 'ás',\n",
       " 'amo',\n",
       " '▁os',\n",
       " '▁من',\n",
       " '▁Sa',\n",
       " 'chi',\n",
       " '▁మ',\n",
       " 'ose',\n",
       " 'oto',\n",
       " 'inga',\n",
       " 'ει',\n",
       " '▁સ',\n",
       " 'vu',\n",
       " '▁ма',\n",
       " '▁us',\n",
       " '▁де',\n",
       " '▁ସ',\n",
       " '▁De',\n",
       " 'न्',\n",
       " 'った',\n",
       " 'по',\n",
       " 'ക്ക',\n",
       " 'esi',\n",
       " '▁ٱل',\n",
       " ...]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(omnt_checkpoint['vocab']['tgt']))\n",
    "omnt_checkpoint['vocab']['src']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoder.embeddings.make_embedding.emb_luts.0.weight': tensor([[-0.0321,  0.0348,  0.0181,  ...,  0.0312, -0.0099, -0.0133],\n",
       "         [-0.0039,  0.0104, -0.0156,  ...,  0.0290, -0.0138, -0.0134],\n",
       "         [-0.0245, -0.0283, -0.0295,  ...,  0.9712, -0.0255, -0.0273],\n",
       "         ...,\n",
       "         [-0.0123, -0.0031, -0.0089,  ...,  0.0645, -0.0182, -0.0740],\n",
       "         [ 0.0085, -0.0088, -0.0091,  ...,  0.0571, -0.0035, -0.1298],\n",
       "         [-0.0076, -0.0107, -0.0051,  ...,  1.0264, -0.0338, -0.1175]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.embeddings.make_embedding.pe.pe': tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  1.0000,  1.0000]],\n",
       " \n",
       "         [[ 0.8415,  0.8317,  0.8218,  ...,  1.0000,  1.0000,  1.0000]],\n",
       " \n",
       "         [[ 0.9093,  0.9236,  0.9365,  ...,  1.0000,  1.0000,  1.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.9563,  0.5417,  0.7653,  ...,  0.8688,  0.8733,  0.8777]],\n",
       " \n",
       "         [[ 0.2705,  0.9999,  0.9649,  ...,  0.8687,  0.8733,  0.8777]],\n",
       " \n",
       "         [[-0.6639,  0.5685,  0.3343,  ...,  0.8687,  0.8732,  0.8776]]]),\n",
       " 'encoder.transformer.0.self_attn.linear_keys.weight': tensor([[ 5.9570e-01,  6.5479e-01,  5.9863e-01,  ...,  1.2024e-01,\n",
       "          -1.4905e-01, -1.1023e-01],\n",
       "         [ 3.1909e-01, -1.8079e-01, -2.6294e-01,  ..., -2.0264e-01,\n",
       "           1.9861e-01,  4.2529e-01],\n",
       "         [ 5.2148e-01,  8.4229e-01,  9.9951e-01,  ...,  2.0679e-01,\n",
       "           2.0203e-01, -5.2261e-03],\n",
       "         ...,\n",
       "         [ 1.3794e-01, -1.1102e-01, -5.5176e-01,  ..., -3.5400e-03,\n",
       "           4.2267e-02, -3.9399e-05],\n",
       "         [-1.5588e-01,  6.3293e-02,  1.2901e-02,  ..., -3.8452e-01,\n",
       "           1.5308e-01,  2.8540e-01],\n",
       "         [-2.2595e-01, -9.3994e-02,  3.0981e-01,  ..., -1.6748e-01,\n",
       "          -7.5378e-02, -1.4185e-01]], dtype=torch.float16),\n",
       " 'encoder.transformer.0.self_attn.linear_keys.bias': tensor([ 0.0179,  0.0283,  0.0196,  ..., -0.0025, -0.0250,  0.0193],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.0.self_attn.linear_values.weight': tensor([[ 0.0630,  0.0343, -0.0231,  ..., -0.0668, -0.1157, -0.0081],\n",
       "         [ 0.0947,  0.0685,  0.0461,  ..., -0.0442, -0.1248, -0.0464],\n",
       "         [ 0.0397,  0.0829,  0.0338,  ...,  0.0314, -0.0782, -0.1476],\n",
       "         ...,\n",
       "         [-0.2808, -0.0279,  0.0878,  ...,  0.0007, -0.0660, -0.0214],\n",
       "         [ 0.0296,  0.0190, -0.0400,  ...,  0.0379, -0.0027, -0.2717],\n",
       "         [-0.0427, -0.0203, -0.0764,  ...,  0.0536, -0.0583,  0.1981]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.0.self_attn.linear_values.bias': tensor([-0.0758, -0.1368, -0.0085,  ..., -0.0319,  0.0159, -0.0054],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.0.self_attn.linear_query.weight': tensor([[ 0.1394, -0.0477, -0.5269,  ..., -0.0635,  0.2659,  0.1715],\n",
       "         [-0.2218, -0.3704, -0.5278,  ..., -0.2737, -0.0536, -0.3032],\n",
       "         [ 1.0117,  1.0215,  0.9995,  ..., -0.0361, -0.1837,  0.1039],\n",
       "         ...,\n",
       "         [-0.1182, -0.1311, -0.4141,  ...,  0.3547, -0.1899,  0.1781],\n",
       "         [-0.1536,  0.0272,  0.0038,  ...,  0.2732, -0.1216,  0.3218],\n",
       "         [-0.2279, -0.1813,  0.2534,  ...,  0.2791, -0.2869, -0.0817]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.0.self_attn.linear_query.bias': tensor([ 0.0445, -0.6504,  0.0458,  ..., -0.0200, -0.2649, -0.0813],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.0.self_attn.final_linear.weight': tensor([[-0.0432, -0.1025, -0.0201,  ...,  0.0115, -0.0321, -0.1892],\n",
       "         [-0.0038, -0.0169, -0.0073,  ...,  0.0214,  0.0050,  0.0173],\n",
       "         [ 0.0013, -0.0362,  0.0130,  ..., -0.0028, -0.0018,  0.0048],\n",
       "         ...,\n",
       "         [-0.0644,  0.3677,  0.1345,  ...,  0.2361, -0.3904,  0.1660],\n",
       "         [ 0.0199, -0.1136, -0.2612,  ..., -0.1603,  0.0699, -0.0349],\n",
       "         [-0.1918, -0.1236, -0.0550,  ...,  0.0294,  0.1755,  0.0011]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.0.self_attn.final_linear.bias': tensor([-1.2274e-01, -5.3802e-02, -8.2552e-05,  ..., -1.1182e-01,\n",
       "          1.1978e-02, -4.8340e-02], dtype=torch.float16),\n",
       " 'encoder.transformer.0.layer_norm.weight': tensor([0.1989, 0.7793, 1.0020,  ..., 0.0681, 0.1104, 0.1191],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.0.layer_norm.bias': tensor([ 0.0027,  0.0109,  0.0106,  ..., -0.0605,  0.0066,  0.0079],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.0.feed_forward.w_1.weight': tensor([[-0.0822, -0.0664, -0.0421,  ..., -0.2252, -0.0174, -0.0901],\n",
       "         [-0.2766,  0.0712, -0.0408,  ...,  0.0138,  0.1761,  0.0470],\n",
       "         [-0.0440,  0.0100, -0.1147,  ...,  0.3438, -0.1202, -0.3132],\n",
       "         ...,\n",
       "         [-0.0398, -0.0275, -0.0571,  ..., -0.1705, -0.0421,  0.1415],\n",
       "         [-0.0609, -0.7812, -1.0000,  ..., -0.1626,  0.0497,  0.0238],\n",
       "         [ 0.0217, -0.0949,  0.0983,  ...,  0.0842,  0.1940, -0.0874]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.0.feed_forward.w_1.bias': tensor([-0.3264, -0.1189, -0.1158,  ..., -0.1076, -0.3582,  0.1184],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.0.feed_forward.w_2.weight': tensor([[ 5.4474e-02, -3.5706e-02,  7.3608e-02,  ..., -6.0089e-02,\n",
       "          -4.7217e-01,  1.3208e-01],\n",
       "         [ 1.6308e-04,  2.6016e-02, -9.5901e-03,  ..., -2.8137e-02,\n",
       "          -1.6527e-03, -2.5024e-02],\n",
       "         [-5.9280e-03,  3.3844e-02, -2.1988e-02,  ...,  1.0384e-02,\n",
       "          -3.5498e-01,  1.5053e-02],\n",
       "         ...,\n",
       "         [-3.8116e-02, -2.1011e-02,  9.9854e-02,  ..., -1.0352e-01,\n",
       "           7.6103e-04, -2.3364e-01],\n",
       "         [ 2.1960e-01,  2.2803e-01, -4.5837e-02,  ..., -3.4943e-02,\n",
       "          -1.2585e-01,  8.1848e-02],\n",
       "         [-8.0322e-02, -2.6505e-02, -3.4448e-01,  ..., -8.9417e-02,\n",
       "          -2.1942e-02, -1.9287e-02]], dtype=torch.float16),\n",
       " 'encoder.transformer.0.feed_forward.w_2.bias': tensor([-0.0136, -0.3655,  0.3784,  ..., -0.4990, -0.1392, -0.7456],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.0.feed_forward.layer_norm.weight': tensor([0.3259, 1.1016, 1.6602,  ..., 0.3540, 0.1896, 0.2242],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.0.feed_forward.layer_norm.bias': tensor([ 0.0026, -0.0082, -0.0087,  ...,  0.1504,  0.0718,  0.1245],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.1.self_attn.linear_keys.weight': tensor([[ 0.3132,  0.0303,  0.0594,  ..., -0.1047,  0.2419, -0.0555],\n",
       "         [-0.0422,  0.0921,  0.0128,  ..., -0.0507,  0.2769,  0.0223],\n",
       "         [ 0.4136,  0.1260, -0.0191,  ...,  0.1329, -0.0108, -0.2230],\n",
       "         ...,\n",
       "         [-0.1825,  0.0161, -0.1770,  ..., -0.0454,  0.0595, -0.0764],\n",
       "         [-0.0319, -0.1340,  0.1174,  ...,  0.0326,  0.0575, -0.1683],\n",
       "         [-0.1371, -0.0249,  0.2296,  ..., -0.0196,  0.2529,  0.0146]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.1.self_attn.linear_keys.bias': tensor([ 0.0179,  0.0086, -0.0042,  ..., -0.0084, -0.0040, -0.0065],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.1.self_attn.linear_values.weight': tensor([[-0.0923, -0.0101, -0.0208,  ..., -0.2000,  0.0222,  0.2756],\n",
       "         [ 0.0516, -0.0055,  0.0196,  ...,  0.0775,  0.0753,  0.2461],\n",
       "         [-0.0670, -0.0357, -0.0576,  ...,  0.0538, -0.1516, -0.2198],\n",
       "         ...,\n",
       "         [-0.3870,  0.1213, -0.1592,  ...,  0.2500,  0.0792, -0.0464],\n",
       "         [ 0.0688,  0.0403,  0.0655,  ...,  0.2209,  0.2373,  0.0286],\n",
       "         [-0.0528, -0.0808,  0.0141,  ...,  0.0310,  0.0302,  0.0488]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.1.self_attn.linear_values.bias': tensor([ 0.0793, -0.0319,  0.0125,  ...,  0.0307, -0.0509, -0.0137],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.1.self_attn.linear_query.weight': tensor([[ 0.2812,  0.1462,  0.0362,  ..., -0.0667,  0.0091,  0.1052],\n",
       "         [-0.1653,  0.0444, -0.0217,  ..., -0.1318, -0.0861, -0.2842],\n",
       "         [ 0.2281, -0.0016,  0.1289,  ...,  0.1704, -0.0197,  0.5918],\n",
       "         ...,\n",
       "         [-0.2090,  0.0235, -0.0010,  ..., -0.0251, -0.0866,  0.5020],\n",
       "         [-0.0695,  0.1296, -0.1885,  ..., -0.3889, -0.0210,  0.4143],\n",
       "         [ 0.3230, -0.0293, -0.0364,  ..., -0.0184,  0.3271,  0.4307]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.1.self_attn.linear_query.bias': tensor([ 0.1562, -0.1987,  0.2474,  ..., -0.0505, -0.3267,  0.2563],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.1.self_attn.final_linear.weight': tensor([[-0.1606,  0.1558, -0.1584,  ..., -0.0085,  0.0523, -0.3730],\n",
       "         [-0.0123,  0.0967,  0.0272,  ...,  0.0414,  0.1367,  0.1249],\n",
       "         [ 0.0454, -0.0834, -0.0182,  ...,  0.0917, -0.2466,  0.0072],\n",
       "         ...,\n",
       "         [-0.1824, -0.0740,  0.1194,  ...,  0.0551, -0.1820, -0.1041],\n",
       "         [ 0.2206, -0.1924,  0.2886,  ..., -0.0296, -0.3135, -0.0388],\n",
       "         [ 0.0279,  0.1779, -0.1296,  ..., -0.4524,  0.0889, -0.1515]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.1.self_attn.final_linear.bias': tensor([-0.5601, -0.4797,  0.5430,  ..., -0.7354, -0.2346, -0.3242],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.1.layer_norm.weight': tensor([0.2800, 0.6963, 1.0000,  ..., 0.1310, 0.1289, 0.0534],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.1.layer_norm.bias': tensor([-0.0067, -0.0167,  0.0038,  ..., -0.0364,  0.0032,  0.0068],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.1.feed_forward.w_1.weight': tensor([[ 0.0288,  0.0228, -0.0608,  ..., -0.0198, -0.1741,  0.0707],\n",
       "         [ 0.1438,  0.0527,  0.0828,  ..., -0.2949, -0.1907, -0.1910],\n",
       "         [ 0.0360,  0.2744,  0.0928,  ..., -0.1534,  0.1022, -0.0169],\n",
       "         ...,\n",
       "         [ 0.1482,  0.1733, -0.1611,  ..., -0.0764,  0.1055, -0.1361],\n",
       "         [ 0.2598,  0.2203,  0.1808,  ..., -0.0300,  0.2542,  0.1102],\n",
       "         [ 0.2976,  0.0646,  0.2065,  ..., -0.0663, -0.1564, -0.0345]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.1.feed_forward.w_1.bias': tensor([-0.0112, -0.2332, -0.1604,  ..., -0.1494, -0.0759, -0.0137],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.1.feed_forward.w_2.weight': tensor([[ 0.0429,  0.0976,  0.2073,  ..., -0.2070,  0.0483, -0.0598],\n",
       "         [ 0.0475,  0.0685, -0.1361,  ..., -0.0488,  0.0395,  0.2761],\n",
       "         [-0.0568, -0.0254,  0.1608,  ...,  0.1666,  0.0410,  0.0114],\n",
       "         ...,\n",
       "         [ 0.1724,  0.2063,  0.0209,  ...,  0.3467,  0.2316,  0.0891],\n",
       "         [ 0.1193,  0.0363, -0.2162,  ..., -0.0956,  0.0388,  0.0544],\n",
       "         [-0.1222,  0.2642, -0.2076,  ..., -0.0162, -0.0597, -0.1100]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.1.feed_forward.w_2.bias': tensor([-0.4980, -0.6235,  0.3506,  ..., -0.8696,  0.0837,  0.0706],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.1.feed_forward.layer_norm.weight': tensor([0.3640, 0.5825, 0.7876,  ..., 0.3674, 0.2639, 0.7192],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.1.feed_forward.layer_norm.bias': tensor([-0.0160, -0.1505,  0.0006,  ...,  0.2961,  0.1514,  0.3875],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.2.self_attn.linear_keys.weight': tensor([[-0.3447, -0.4392, -0.2729,  ...,  0.1851,  0.1113, -0.1281],\n",
       "         [-0.5283, -0.3757, -0.2126,  ...,  0.0692, -0.0856,  0.1705],\n",
       "         [ 0.0643, -0.1009,  0.0264,  ..., -0.1879, -0.1941, -0.0309],\n",
       "         ...,\n",
       "         [ 0.1146, -0.1388,  0.0802,  ...,  0.0249, -0.2125, -0.0556],\n",
       "         [ 0.0025,  0.0030,  0.4084,  ...,  0.2128,  0.1897,  0.1888],\n",
       "         [-0.2020, -0.0406, -0.2024,  ...,  0.0584,  0.1917,  0.2167]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.2.self_attn.linear_keys.bias': tensor([-0.0259, -0.0300,  0.0098,  ...,  0.0023,  0.0161, -0.0312],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.2.self_attn.linear_values.weight': tensor([[ 0.0021,  0.0291,  0.2048,  ...,  0.3103, -0.0507, -0.1099],\n",
       "         [ 0.0806,  0.0523, -0.0200,  ...,  0.1108, -0.0761,  0.0262],\n",
       "         [-0.0192,  0.1956,  0.0440,  ..., -0.2407, -0.1000, -0.0016],\n",
       "         ...,\n",
       "         [ 0.0179,  0.0394, -0.0177,  ...,  0.1243,  0.0283,  0.0198],\n",
       "         [ 0.0754, -0.0627, -0.0371,  ...,  0.2119, -0.3503,  0.0305],\n",
       "         [ 0.1018,  0.2954,  0.0267,  ...,  0.0017,  0.0053, -0.0506]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.2.self_attn.linear_values.bias': tensor([ 0.1757, -0.1736,  0.0048,  ...,  0.0027, -0.0123,  0.0012],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.2.self_attn.linear_query.weight': tensor([[-0.2394, -0.2335, -0.0950,  ...,  0.1547,  0.1436,  0.1914],\n",
       "         [-0.0340, -0.1176, -0.3926,  ...,  0.1010, -0.0010, -0.0776],\n",
       "         [ 0.1042, -0.2820,  0.0054,  ..., -0.1914,  0.0817,  0.1759],\n",
       "         ...,\n",
       "         [-0.0563, -0.0400,  0.0502,  ..., -0.0404,  0.2847,  0.0385],\n",
       "         [ 0.1459, -0.0163,  0.0189,  ...,  0.0794,  0.2352,  0.1910],\n",
       "         [ 0.1403,  0.0652,  0.1050,  ..., -0.0582, -0.0510, -0.1772]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.2.self_attn.linear_query.bias': tensor([-0.1339,  0.7612,  0.0226,  ...,  0.0240,  0.1281,  0.1191],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.2.self_attn.final_linear.weight': tensor([[ 0.3418, -0.0677,  0.2137,  ...,  0.0499,  0.2649,  0.1776],\n",
       "         [-0.3020, -0.0951,  0.0147,  ...,  0.0165,  0.0594,  0.0526],\n",
       "         [-0.2974,  0.2010,  0.0136,  ...,  0.1761,  0.2007, -0.0710],\n",
       "         ...,\n",
       "         [ 0.0853,  0.2075,  0.0961,  ..., -0.2235, -0.2292,  0.0688],\n",
       "         [-0.0068, -0.0393, -0.1675,  ..., -0.1058, -0.2266, -0.1720],\n",
       "         [ 0.2888,  0.1907,  0.2939,  ...,  0.1801,  0.4055,  0.1396]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.2.self_attn.final_linear.bias': tensor([-0.3291,  0.1842, -0.0803,  ..., -0.4951, -0.2440, -0.1320],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.2.layer_norm.weight': tensor([0.2551, 0.3916, 0.4587,  ..., 0.1593, 0.1455, 0.0640],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.2.layer_norm.bias': tensor([ 0.0016, -0.0094,  0.0044,  ..., -0.0253,  0.0004, -0.1724],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.2.feed_forward.w_1.weight': tensor([[-0.0267, -0.2546, -0.1114,  ...,  0.0208, -0.4014, -0.0889],\n",
       "         [ 0.1504,  0.1133, -0.3638,  ..., -0.0380, -0.0839, -0.0189],\n",
       "         [-0.3149,  0.2098, -0.0369,  ..., -0.0750,  0.1387, -0.1759],\n",
       "         ...,\n",
       "         [ 0.1409,  0.0983,  0.0416,  ...,  0.1759, -0.0855,  0.1470],\n",
       "         [-0.1570,  0.0397, -0.2314,  ..., -0.1272,  0.0359,  0.0133],\n",
       "         [-0.1584, -0.0536, -0.2089,  ..., -0.1642, -0.5425, -0.1512]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.2.feed_forward.w_1.bias': tensor([-0.0205, -0.1761, -0.1024,  ..., -0.1119, -0.2397, -0.1015],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.2.feed_forward.w_2.weight': tensor([[ 0.2343,  0.1005, -0.0427,  ..., -0.0775, -0.1626, -0.0025],\n",
       "         [-0.2034, -0.2035, -0.0301,  ..., -0.2466,  0.1137,  0.2327],\n",
       "         [ 0.0221,  0.0497,  0.0061,  ...,  0.0156,  0.4207,  0.0813],\n",
       "         ...,\n",
       "         [ 0.0395,  0.2274,  0.0329,  ...,  0.0589,  0.1022, -0.2118],\n",
       "         [-0.0251, -0.1895,  0.0290,  ...,  0.5098,  0.1008, -0.3828],\n",
       "         [ 0.3801,  0.1416, -0.0853,  ...,  0.0865,  0.0880, -0.0945]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.2.feed_forward.w_2.bias': tensor([-0.2042, -0.3538,  0.0340,  ..., -0.5020, -0.2240, -0.4961],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.2.feed_forward.layer_norm.weight': tensor([0.4373, 0.5439, 0.6646,  ..., 0.4412, 0.3557, 0.5312],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.2.feed_forward.layer_norm.bias': tensor([ 0.0157, -0.1758,  0.0136,  ...,  0.3103,  0.2247,  0.3567],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.3.self_attn.linear_keys.weight': tensor([[ 0.1273,  0.2241, -0.0561,  ..., -0.1205,  0.2671,  0.1118],\n",
       "         [-0.1213,  0.0095, -0.0512,  ..., -0.0240,  0.2441,  0.0936],\n",
       "         [-0.1697, -0.1659,  0.1774,  ...,  0.1174, -0.1907,  0.1812],\n",
       "         ...,\n",
       "         [-0.2524, -0.0491, -0.2900,  ..., -0.0951,  0.3584,  0.0298],\n",
       "         [ 0.3196, -0.3335,  0.0198,  ..., -0.2542,  0.2593, -0.2198],\n",
       "         [-0.2167, -0.0964, -0.0109,  ...,  0.1262,  0.2217,  0.2588]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.3.self_attn.linear_keys.bias': tensor([ 0.0045, -0.0249,  0.0055,  ...,  0.0129,  0.0192, -0.0107],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.3.self_attn.linear_values.weight': tensor([[-0.1000, -0.0194, -0.0435,  ..., -0.2358,  0.0946,  0.0695],\n",
       "         [ 0.0120,  0.0917, -0.1650,  ...,  0.1097, -0.0242, -0.0247],\n",
       "         [ 0.0066, -0.0302,  0.0870,  ..., -0.0209,  0.4578, -0.0189],\n",
       "         ...,\n",
       "         [ 0.3220, -0.0787, -0.2520,  ...,  0.1698,  0.2107,  0.0212],\n",
       "         [ 0.0778,  0.0969,  0.0474,  ...,  0.1864, -0.0052, -0.0772],\n",
       "         [-0.0798,  0.2871,  0.1097,  ..., -0.0649,  0.0545,  0.0087]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.3.self_attn.linear_values.bias': tensor([-0.0052,  0.1276, -0.0572,  ..., -0.1461, -0.2104, -0.0482],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.3.self_attn.linear_query.weight': tensor([[ 0.1786, -0.3074, -0.1509,  ...,  0.1709, -0.3638, -0.1603],\n",
       "         [-0.1456, -0.0075, -0.0852,  ..., -0.1757,  0.4004,  0.3643],\n",
       "         [ 0.1526,  0.0436, -0.1224,  ...,  0.0580,  0.4180, -0.2462],\n",
       "         ...,\n",
       "         [-0.2957, -0.0009, -0.4045,  ...,  0.2405, -0.2418,  0.0562],\n",
       "         [-0.1401, -0.1481, -0.1549,  ..., -0.3357,  0.0062, -0.2097],\n",
       "         [ 0.1744, -0.2991, -0.4023,  ...,  0.0531, -0.3079,  0.1447]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.3.self_attn.linear_query.bias': tensor([ 0.3250,  0.2773,  0.2637,  ...,  0.4526, -0.1826,  0.5928],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.3.self_attn.final_linear.weight': tensor([[ 0.2375,  0.4453, -0.1666,  ..., -0.0097, -0.2749,  0.2230],\n",
       "         [-0.1320,  0.1287, -0.1155,  ..., -0.0981, -0.1682,  0.0881],\n",
       "         [ 0.2158, -0.1592,  0.2389,  ..., -0.2021,  0.0601,  0.0741],\n",
       "         ...,\n",
       "         [-0.2094,  0.2705,  0.3247,  ..., -0.3774, -0.0038,  0.0504],\n",
       "         [ 0.1333, -0.0093,  0.0922,  ..., -0.2559, -0.2120, -0.0821],\n",
       "         [ 0.1238, -0.1958, -0.2073,  ...,  0.0613,  0.0382, -0.0040]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.3.self_attn.final_linear.bias': tensor([-0.1768,  0.0324, -0.4902,  ..., -0.1864, -0.1381, -0.0352],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.3.layer_norm.weight': tensor([0.2632, 0.3484, 0.3823,  ..., 0.1754, 0.1694, 0.0759],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.3.layer_norm.bias': tensor([-0.0017, -0.0178,  0.0143,  ..., -0.0205, -0.0016, -0.1876],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.3.feed_forward.w_1.weight': tensor([[ 0.2700,  0.1938,  0.2998,  ...,  0.1562,  0.1998,  0.3589],\n",
       "         [ 0.1078,  0.3862,  0.1381,  ..., -0.1604, -0.1088,  0.1871],\n",
       "         [-0.0421,  0.3840, -0.1287,  ...,  0.0122,  0.1149, -0.0363],\n",
       "         ...,\n",
       "         [ 0.0747,  0.2203, -0.2174,  ..., -0.3176, -0.0036, -0.1372],\n",
       "         [ 0.1874,  0.1069, -0.1216,  ..., -0.2939,  0.4365,  0.1703],\n",
       "         [ 0.1702,  0.2842,  0.0290,  ...,  0.0687, -0.0826,  0.0793]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.3.feed_forward.w_1.bias': tensor([-0.0987, -0.1757, -0.0022,  ..., -0.2773, -0.2390, -0.1774],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.3.feed_forward.w_2.weight': tensor([[-0.0603, -0.1033,  0.0816,  ..., -0.3188, -0.2788, -0.1556],\n",
       "         [ 0.0388,  0.0523, -0.2107,  ..., -0.2145,  0.0859,  0.1127],\n",
       "         [-0.0334, -0.1051, -0.0261,  ...,  0.1476, -0.2554, -0.1295],\n",
       "         ...,\n",
       "         [-0.0571, -0.1379, -0.1825,  ..., -0.0071, -0.1080, -0.2607],\n",
       "         [-0.2651, -0.1130,  0.1184,  ..., -0.3130,  0.2365, -0.1627],\n",
       "         [-0.2324, -0.0607,  0.2191,  ...,  0.4524,  0.0908,  0.0570]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.3.feed_forward.w_2.bias': tensor([-0.1322, -0.2524, -0.2625,  ..., -0.6270, -0.4980,  0.1598],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.3.feed_forward.layer_norm.weight': tensor([0.5571, 0.6401, 0.6841,  ..., 0.5356, 0.4375, 0.5571],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.3.feed_forward.layer_norm.bias': tensor([ 0.0726, -0.2047,  0.0892,  ...,  0.3430,  0.1608,  0.3596],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.4.self_attn.linear_keys.weight': tensor([[-0.1092, -0.0028, -0.1350,  ..., -0.1427,  0.2861, -0.0529],\n",
       "         [ 0.1046,  0.1780, -0.0298,  ..., -0.0937,  0.0679, -0.0216],\n",
       "         [-0.1721, -0.1276,  0.2137,  ...,  0.0411, -0.0165, -0.1064],\n",
       "         ...,\n",
       "         [ 0.1192,  0.1671,  0.2939,  ...,  0.1614, -0.0975,  0.0159],\n",
       "         [-0.1638, -0.1135, -0.0382,  ...,  0.0856, -0.1583, -0.0257],\n",
       "         [-0.1689,  0.3257, -0.1021,  ...,  0.0256,  0.0517,  0.1899]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.4.self_attn.linear_keys.bias': tensor([ 0.0186, -0.0253, -0.0241,  ...,  0.0240,  0.0197,  0.0040],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.4.self_attn.linear_values.weight': tensor([[-0.0698, -0.0300,  0.1226,  ..., -0.2991,  0.0958,  0.0717],\n",
       "         [ 0.1055,  0.0383, -0.1597,  ..., -0.2573,  0.1906,  0.0244],\n",
       "         [ 0.0713, -0.3877, -0.1547,  ...,  0.0464, -0.0274,  0.0415],\n",
       "         ...,\n",
       "         [ 0.0169,  0.1136, -0.2030,  ..., -0.1438,  0.3379,  0.1071],\n",
       "         [-0.2815,  0.1594,  0.4478,  ...,  0.1702, -0.4883,  0.0009],\n",
       "         [-0.2661, -0.0815,  0.4202,  ..., -0.1316, -0.3354,  0.0138]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.4.self_attn.linear_values.bias': tensor([-0.0147, -0.1171,  0.1478,  ..., -0.0172, -0.0030, -0.1357],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.4.self_attn.linear_query.weight': tensor([[-0.3167,  0.2019,  0.2131,  ...,  0.0332, -0.3059, -0.3101],\n",
       "         [ 0.0149, -0.0750,  0.2404,  ..., -0.1279, -0.1794,  0.1234],\n",
       "         [-0.0178, -0.0139, -0.2239,  ..., -0.1698, -0.0556, -0.1077],\n",
       "         ...,\n",
       "         [-0.1376,  0.0729, -0.2188,  ...,  0.0415, -0.1272,  0.3601],\n",
       "         [ 0.0909,  0.0670, -0.0636,  ..., -0.1163, -0.2096, -0.0964],\n",
       "         [ 0.2123, -0.3875, -0.1823,  ..., -0.0737, -0.0089, -0.1017]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.4.self_attn.linear_query.bias': tensor([-0.1918,  0.3306,  0.0953,  ..., -0.0160, -0.1255,  0.0032],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.4.self_attn.final_linear.weight': tensor([[ 0.3052,  0.0142, -0.2898,  ...,  0.0723,  0.0241,  0.1895],\n",
       "         [-0.2295, -0.2798, -0.0404,  ...,  0.1388,  0.2332,  0.2834],\n",
       "         [ 0.1124,  0.0247,  0.2905,  ...,  0.0372, -0.3989, -0.0923],\n",
       "         ...,\n",
       "         [ 0.0914,  0.4182,  0.0561,  ..., -0.3076, -0.0513,  0.0178],\n",
       "         [-0.0081, -0.1631,  0.3223,  ...,  0.0817, -0.0063,  0.0272],\n",
       "         [-0.0464,  0.0386, -0.3821,  ..., -0.2686,  0.2832, -0.3052]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.4.self_attn.final_linear.bias': tensor([-0.2148, -0.2712, -0.4692,  ..., -0.4463, -0.2773, -0.1874],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.4.layer_norm.weight': tensor([0.2480, 0.2932, 0.3071,  ..., 0.1970, 0.1952, 0.0856],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.4.layer_norm.bias': tensor([ 0.0061, -0.0186,  0.0067,  ..., -0.0136,  0.0044, -0.1420],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.4.feed_forward.w_1.weight': tensor([[-0.0947, -0.3777,  0.1453,  ...,  0.2079,  0.1239, -0.1075],\n",
       "         [-0.0224, -0.1260,  0.0084,  ..., -0.1088, -0.1128,  0.4102],\n",
       "         [-0.1868,  0.2651, -0.2267,  ..., -0.2732,  0.2051,  0.1484],\n",
       "         ...,\n",
       "         [-0.1461, -0.2017,  0.1912,  ..., -0.0526, -0.1860,  0.0266],\n",
       "         [ 0.3555,  0.1292, -0.0576,  ..., -0.0812, -0.0305, -0.1492],\n",
       "         [ 0.0394, -0.0345,  0.0431,  ...,  0.0626,  0.0320,  0.5259]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.4.feed_forward.w_1.bias': tensor([-0.1158, -0.1146, -0.0953,  ..., -0.0958, -0.1493,  0.1250],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.4.feed_forward.w_2.weight': tensor([[ 9.1736e-02, -3.4851e-02,  6.6711e-02,  ...,  1.7236e-01,\n",
       "          -2.4426e-01, -1.8814e-02],\n",
       "         [ 2.3706e-01,  1.6125e-01, -1.7261e-01,  ...,  1.8347e-01,\n",
       "           3.2921e-03,  2.4902e-02],\n",
       "         [ 1.3171e-01,  1.8689e-01, -5.2910e-03,  ..., -8.6365e-02,\n",
       "          -4.3640e-02,  3.5248e-03],\n",
       "         ...,\n",
       "         [-3.4094e-04, -2.9468e-01,  1.6211e-01,  ...,  2.6520e-02,\n",
       "          -4.6265e-02,  5.0537e-02],\n",
       "         [-2.1143e-01, -1.3879e-01,  4.9731e-01,  ...,  7.4158e-03,\n",
       "           2.5854e-01,  2.4811e-02],\n",
       "         [-8.2092e-02,  2.2675e-02, -2.8735e-01,  ...,  6.6528e-02,\n",
       "           2.5195e-01,  3.7183e-01]], dtype=torch.float16),\n",
       " 'encoder.transformer.4.feed_forward.w_2.bias': tensor([-0.0294, -0.0543, -0.1670,  ..., -0.4087, -0.3076, -0.2484],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.4.feed_forward.layer_norm.weight': tensor([0.8389, 0.8896, 0.9126,  ..., 0.8003, 0.6929, 0.9141],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.4.feed_forward.layer_norm.bias': tensor([ 0.0629, -0.1814,  0.1497,  ...,  0.3884,  0.2186,  0.6089],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.5.self_attn.linear_keys.weight': tensor([[-0.1027,  0.1290,  0.2034,  ...,  0.0239,  0.0739,  0.1147],\n",
       "         [ 0.0134,  0.0109,  0.0040,  ..., -0.1880, -0.0925,  0.1240],\n",
       "         [ 0.1074, -0.2524,  0.1588,  ..., -0.0675, -0.1399, -0.1630],\n",
       "         ...,\n",
       "         [-0.1376,  0.0674, -0.1263,  ..., -0.0240, -0.1779,  0.2888],\n",
       "         [-0.0420, -0.0428, -0.0071,  ...,  0.1560,  0.2296, -0.0860],\n",
       "         [ 0.2198,  0.2279, -0.0904,  ..., -0.0740,  0.1239, -0.1846]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.5.self_attn.linear_keys.bias': tensor([-0.0248, -0.0112,  0.0246,  ...,  0.0078,  0.0279, -0.0154],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.5.self_attn.linear_values.weight': tensor([[-6.2683e-02,  4.0723e-01,  3.0078e-01,  ...,  2.5342e-01,\n",
       "           3.8208e-01, -1.4209e-01],\n",
       "         [ 2.1057e-01, -4.9731e-01, -5.0195e-01,  ...,  4.3604e-01,\n",
       "          -2.5562e-01,  7.9468e-02],\n",
       "         [ 2.2192e-01, -4.8315e-01, -4.9951e-01,  ...,  2.4561e-01,\n",
       "          -1.2524e-01, -8.5876e-02],\n",
       "         ...,\n",
       "         [ 2.9297e-03,  1.6528e-01,  1.7249e-01,  ...,  2.3157e-01,\n",
       "          -8.5205e-02, -1.6647e-02],\n",
       "         [ 1.7163e-01, -5.5176e-01,  1.1542e-01,  ..., -2.9545e-03,\n",
       "           3.3618e-01,  3.1757e-04],\n",
       "         [-1.7358e-01,  1.0901e-01,  3.8971e-02,  ..., -3.2129e-01,\n",
       "          -6.4148e-02,  9.6985e-02]], dtype=torch.float16),\n",
       " 'encoder.transformer.5.self_attn.linear_values.bias': tensor([ 0.2517,  0.0800, -0.1105,  ...,  0.1067,  0.0918, -0.2163],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.5.self_attn.linear_query.weight': tensor([[-0.0835, -0.2556,  0.1780,  ..., -0.0013,  0.0685,  0.1277],\n",
       "         [-0.0474,  0.0945,  0.0859,  ..., -0.0839, -0.2522,  0.3069],\n",
       "         [-0.1220,  0.1346,  0.0013,  ..., -0.1627,  0.1812, -0.3811],\n",
       "         ...,\n",
       "         [-0.5020, -0.2435, -0.0367,  ..., -0.1248, -0.0159,  0.0864],\n",
       "         [-0.0185, -0.1132, -0.1614,  ...,  0.0128, -0.2505,  0.0236],\n",
       "         [ 0.0471, -0.0351,  0.2712,  ..., -0.0532, -0.0487,  0.1573]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.5.self_attn.linear_query.bias': tensor([-0.1298,  0.3081, -0.0649,  ...,  0.3333, -0.2089, -0.3911],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.5.self_attn.final_linear.weight': tensor([[-0.1759, -0.2527, -0.4905,  ..., -0.1455,  0.0436,  0.0316],\n",
       "         [-0.0808,  0.3103, -0.0531,  ...,  0.2908, -0.4807, -0.0984],\n",
       "         [-0.2214, -0.2119,  0.0950,  ...,  0.3025,  0.3235,  0.0836],\n",
       "         ...,\n",
       "         [-0.1042, -0.2352, -0.2393,  ..., -0.0567, -0.1390, -0.5044],\n",
       "         [ 0.1320, -0.2820,  0.1831,  ..., -0.0705,  0.1049,  0.4670],\n",
       "         [ 0.1141, -0.1040,  0.2449,  ...,  0.2822, -0.1600,  0.0797]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.5.self_attn.final_linear.bias': tensor([-0.2974,  0.1377, -0.3240,  ..., -0.3740, -0.3955, -0.0625],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.5.layer_norm.weight': tensor([0.2593, 0.2888, 0.2932,  ..., 0.2134, 0.1947, 0.1024],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.5.layer_norm.bias': tensor([ 0.0130, -0.0358,  0.0096,  ..., -0.0243, -0.0059, -0.1379],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.5.feed_forward.w_1.weight': tensor([[-0.1627,  0.4009,  0.3484,  ..., -0.4468,  0.2238,  0.0247],\n",
       "         [-0.2316, -0.2040,  0.2120,  ...,  0.0018,  0.2175, -0.0315],\n",
       "         [ 0.0982,  0.2788, -0.5264,  ...,  0.2260, -0.1183, -0.1161],\n",
       "         ...,\n",
       "         [-0.1323, -0.2255,  0.0363,  ..., -0.2161, -0.0649, -0.0839],\n",
       "         [ 0.0827,  0.3855,  0.0304,  ..., -0.1301,  0.0207,  0.0511],\n",
       "         [ 0.1125,  0.0268,  0.0286,  ..., -0.0140, -0.2603,  0.0757]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.5.feed_forward.w_1.bias': tensor([-0.3113, -0.2347, -0.2471,  ...,  0.0296, -0.0106, -0.1477],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.5.feed_forward.w_2.weight': tensor([[ 0.1118, -0.0317,  0.3906,  ...,  0.1901,  0.0196,  0.1421],\n",
       "         [ 0.2644, -0.2418, -0.0213,  ...,  0.2583, -0.1804,  0.0911],\n",
       "         [ 0.1860,  0.1185, -0.1493,  ..., -0.0756, -0.0252, -0.0974],\n",
       "         ...,\n",
       "         [-0.0878,  0.0639,  0.0248,  ...,  0.1627, -0.0352,  0.0266],\n",
       "         [-0.2057,  0.3218,  0.3306,  ...,  0.0302, -0.1030, -0.1403],\n",
       "         [-0.3589,  0.0693,  0.3821,  ...,  0.0552,  0.0178,  0.1909]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.5.feed_forward.w_2.bias': tensor([-0.1635,  0.1593,  0.2888,  ..., -0.3391, -0.1981,  0.2345],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.5.feed_forward.layer_norm.weight': tensor([1.0498, 1.0732, 1.0986,  ..., 1.0576, 0.9355, 1.1182],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.5.feed_forward.layer_norm.bias': tensor([ 0.0972, -0.2222,  0.1204,  ...,  0.2439, -0.0014,  0.7427],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.6.self_attn.linear_keys.weight': tensor([[-0.0706, -0.0345,  0.2610,  ..., -0.1284,  0.0302, -0.1357],\n",
       "         [ 0.1028,  0.0043, -0.1156,  ..., -0.1282, -0.1825, -0.1663],\n",
       "         [-0.2213, -0.1600,  0.1575,  ...,  0.2169, -0.0125,  0.1002],\n",
       "         ...,\n",
       "         [-0.0800, -0.0073, -0.0764,  ..., -0.1770, -0.0122, -0.1235],\n",
       "         [ 0.1169,  0.0127, -0.1982,  ..., -0.0235,  0.0922,  0.0699],\n",
       "         [-0.4729, -0.2825,  0.0516,  ...,  0.0944,  0.0304,  0.3552]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.6.self_attn.linear_keys.bias': tensor([-0.0212,  0.0140,  0.0011,  ...,  0.0077,  0.0287, -0.0167],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.6.self_attn.linear_values.weight': tensor([[-0.1652,  0.3330,  0.3162,  ..., -0.2389,  0.2849, -0.0800],\n",
       "         [-0.1203,  0.2966, -0.1333,  ..., -0.2632,  0.3467,  0.0620],\n",
       "         [ 0.1744, -0.0900,  0.2561,  ..., -0.0182,  0.3352, -0.1104],\n",
       "         ...,\n",
       "         [-0.5029, -0.0768,  0.3154,  ..., -0.1250, -0.3794,  0.0287],\n",
       "         [ 0.2097,  0.2666, -0.5264,  ..., -0.3022,  0.2620, -0.2422],\n",
       "         [-0.3618,  0.1602,  0.1276,  ...,  0.1013, -0.4070, -0.0555]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.6.self_attn.linear_values.bias': tensor([-0.1273,  0.0925, -0.0681,  ...,  0.1183,  0.1335,  0.1152],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.6.self_attn.linear_query.weight': tensor([[ 5.8380e-02,  1.7212e-01,  1.0687e-01,  ...,  7.8491e-02,\n",
       "           2.8854e-02,  3.4326e-01],\n",
       "         [ 3.1348e-01,  1.0724e-01, -2.2888e-03,  ..., -3.3398e-03,\n",
       "          -2.1375e-01,  2.3218e-01],\n",
       "         [-1.6968e-01, -1.7670e-02,  1.2512e-01,  ...,  4.8615e-02,\n",
       "          -2.1106e-01, -2.4805e-01],\n",
       "         ...,\n",
       "         [-4.3365e-02,  4.7638e-02,  7.3303e-02,  ..., -3.2837e-02,\n",
       "           5.4840e-02, -1.5967e-01],\n",
       "         [ 7.6599e-02,  1.3269e-01, -4.7241e-02,  ...,  1.3574e-01,\n",
       "          -1.0687e-01, -2.5854e-01],\n",
       "         [ 1.8738e-02,  1.0598e-04,  3.7262e-02,  ...,  1.2688e-02,\n",
       "           3.1647e-02, -2.9468e-01]], dtype=torch.float16),\n",
       " 'encoder.transformer.6.self_attn.linear_query.bias': tensor([-0.4375,  0.0213,  0.2203,  ..., -0.4126, -0.1324,  0.9414],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.6.self_attn.final_linear.weight': tensor([[ 0.4932, -0.2959, -0.2666,  ..., -0.0205,  0.1765, -0.2189],\n",
       "         [ 0.2407,  0.2262,  0.1125,  ..., -0.0656,  0.1926,  0.0654],\n",
       "         [ 0.2922,  0.1119,  0.3481,  ...,  0.2129,  0.0788,  0.0352],\n",
       "         ...,\n",
       "         [-0.1580,  0.1183,  0.0581,  ..., -0.2288, -0.0887, -0.0232],\n",
       "         [-0.0381,  0.1404, -0.0415,  ..., -0.2959,  0.0758, -0.3286],\n",
       "         [-0.5059,  0.4998, -0.0927,  ...,  0.3774, -0.2629, -0.5054]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.6.self_attn.final_linear.bias': tensor([-0.0626,  0.2250, -0.1521,  ..., -0.4531, -0.2175,  0.0847],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.6.layer_norm.weight': tensor([0.2781, 0.3113, 0.3044,  ..., 0.2424, 0.2245, 0.1198],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.6.layer_norm.bias': tensor([ 0.0091, -0.0322,  0.0044,  ..., -0.0353, -0.0171, -0.1396],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.6.feed_forward.w_1.weight': tensor([[ 0.5049, -0.2141,  0.1185,  ...,  0.0346, -0.0762, -0.2939],\n",
       "         [-0.1039,  0.0520, -0.2483,  ..., -0.2549,  0.0668,  0.0578],\n",
       "         [-0.1609, -0.0087,  0.0487,  ...,  0.0201, -0.0370, -0.1423],\n",
       "         ...,\n",
       "         [-0.2238,  0.2118,  0.5020,  ..., -0.2603, -0.5088, -0.1318],\n",
       "         [ 0.0547, -0.1289, -0.1384,  ..., -0.3176, -0.0857,  0.4788],\n",
       "         [-0.1636, -0.1747,  0.0696,  ...,  0.1959, -0.3271, -0.2164]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.6.feed_forward.w_1.bias': tensor([-0.2440, -0.1483, -0.1898,  ..., -0.2588, -0.2106, -0.1549],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.6.feed_forward.w_2.weight': tensor([[-0.1581, -0.0368,  0.2334,  ..., -0.3040,  0.1361,  0.1009],\n",
       "         [ 0.2061,  0.0373, -0.1328,  ...,  0.4993, -0.0421, -0.2350],\n",
       "         [ 0.0025,  0.0223, -0.1836,  ...,  0.1327, -0.1272,  0.0468],\n",
       "         ...,\n",
       "         [-0.1857,  0.1321, -0.2494,  ..., -0.3591, -0.0569,  0.1892],\n",
       "         [-0.3350, -0.0670,  0.2271,  ...,  0.2461, -0.0009, -0.2045],\n",
       "         [ 0.0830,  0.0142,  0.1000,  ..., -0.2190,  0.0870,  0.2712]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.6.feed_forward.w_2.bias': tensor([-0.2479, -0.1766,  0.3108,  ..., -0.2715,  0.1544, -0.0607],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.6.feed_forward.layer_norm.weight': tensor([1.2949, 1.2705, 1.2734,  ..., 1.2637, 1.1494, 1.0869],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.6.feed_forward.layer_norm.bias': tensor([ 0.1816, -0.1693,  0.0043,  ...,  0.1831, -0.1348,  0.7734],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.7.self_attn.linear_keys.weight': tensor([[ 0.0403,  0.2040, -0.1902,  ...,  0.1509, -0.1453, -0.1630],\n",
       "         [-0.0783, -0.0748, -0.1115,  ...,  0.1044, -0.0614, -0.2024],\n",
       "         [-0.0826, -0.0220,  0.1033,  ...,  0.1572, -0.1219,  0.0381],\n",
       "         ...,\n",
       "         [-0.1497, -0.0581, -0.2620,  ..., -0.0068, -0.0917,  0.1129],\n",
       "         [-0.3176,  0.1254,  0.0963,  ...,  0.0521, -0.0949, -0.0911],\n",
       "         [ 0.0673, -0.1017, -0.2588,  ...,  0.0936,  0.1602,  0.0665]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.7.self_attn.linear_keys.bias': tensor([ 0.0198,  0.0307,  0.0190,  ...,  0.0079, -0.0173, -0.0075],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.7.self_attn.linear_values.weight': tensor([[ 0.6250,  0.3130, -0.0906,  ...,  0.1709,  0.1100,  0.0266],\n",
       "         [-0.3528,  0.0912, -0.4282,  ...,  0.2196, -0.1589, -0.0099],\n",
       "         [-0.0517, -0.0556, -0.1302,  ...,  0.4043, -0.1920,  0.2158],\n",
       "         ...,\n",
       "         [-0.0071, -0.0645, -0.3860,  ..., -0.0900,  0.0782, -0.0291],\n",
       "         [ 0.0308,  0.2062, -0.3958,  ...,  0.0598,  0.4902, -0.1539],\n",
       "         [ 0.4363, -0.0987, -0.4602,  ...,  0.1437,  0.0404,  0.0533]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.7.self_attn.linear_values.bias': tensor([ 0.0676,  0.4995, -0.2866,  ..., -0.1379, -0.1763,  0.2477],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.7.self_attn.linear_query.weight': tensor([[ 0.0831,  0.0398,  0.1328,  ...,  0.0081, -0.0090,  0.0434],\n",
       "         [-0.1038,  0.0436, -0.0704,  ..., -0.0676,  0.1190, -0.1210],\n",
       "         [ 0.0927,  0.0960,  0.0286,  ...,  0.0106,  0.1908,  0.1411],\n",
       "         ...,\n",
       "         [ 0.0671,  0.2081,  0.0379,  ...,  0.1769,  0.0137,  0.0703],\n",
       "         [-0.0284,  0.0137,  0.2231,  ..., -0.0323,  0.2351,  0.2407],\n",
       "         [ 0.0638,  0.0581, -0.0652,  ...,  0.0352,  0.0883,  0.0115]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.7.self_attn.linear_query.bias': tensor([-0.0459,  0.5107,  0.0079,  ...,  0.2482,  0.0854,  0.0585],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.7.self_attn.final_linear.weight': tensor([[-0.3962,  0.1968,  0.2440,  ..., -0.2708,  0.0613,  0.5044],\n",
       "         [-0.1608, -0.2025,  0.0812,  ...,  0.4180,  0.1942,  0.0641],\n",
       "         [ 0.1642, -0.0959,  0.0797,  ...,  0.0237, -0.0937,  0.5010],\n",
       "         ...,\n",
       "         [-0.1365, -0.1866,  0.0842,  ..., -0.2437, -0.0931,  0.2573],\n",
       "         [-0.1290,  0.1565,  0.2512,  ..., -0.3945,  0.0866, -0.0850],\n",
       "         [-0.3518,  0.4900,  0.2866,  ...,  0.2408,  0.2727, -0.0621]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.7.self_attn.final_linear.bias': tensor([-0.1945, -0.0603, -0.1410,  ..., -0.2971, -0.3428, -0.5000],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.7.layer_norm.weight': tensor([0.2864, 0.2981, 0.3025,  ..., 0.2600, 0.2637, 0.1532],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.7.layer_norm.bias': tensor([ 0.0018, -0.0353,  0.0016,  ..., -0.0442, -0.0206, -0.0556],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.7.feed_forward.w_1.weight': tensor([[ 0.3796, -0.2915,  0.0097,  ..., -0.1077,  0.2246,  0.0941],\n",
       "         [ 0.2150,  0.0715, -0.2764,  ..., -0.3174, -0.0606, -0.1829],\n",
       "         [-0.1274,  0.3252,  0.2388,  ..., -0.0192, -0.5010,  0.2556],\n",
       "         ...,\n",
       "         [-0.1538,  0.1974,  0.0007,  ...,  0.4119,  0.2036, -0.1060],\n",
       "         [ 0.1875, -0.0032, -0.2629,  ..., -0.0845,  0.0578,  0.1394],\n",
       "         [ 0.0598, -0.2800,  0.1261,  ..., -0.0494,  0.1498, -0.3682]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.7.feed_forward.w_1.bias': tensor([-0.1786,  0.0657, -0.2452,  ..., -0.1400, -0.1942, -0.2534],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.7.feed_forward.w_2.weight': tensor([[-0.0225,  0.0469,  0.5161,  ..., -0.0067, -0.4446,  0.1309],\n",
       "         [-0.1541,  0.0912, -0.1689,  ..., -0.0529, -0.1357, -0.1484],\n",
       "         [-0.1372,  0.0805, -0.4978,  ..., -0.1472,  0.0931,  0.0875],\n",
       "         ...,\n",
       "         [ 0.3457,  0.2556,  0.1699,  ...,  0.0536,  0.0854,  0.0499],\n",
       "         [ 0.2449, -0.0298,  0.0054,  ...,  0.1077, -0.1932, -0.1685],\n",
       "         [-0.1469, -0.0487, -0.0607,  ...,  0.0502, -0.5054,  0.0828]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.7.feed_forward.w_2.bias': tensor([-0.1750, -0.2800,  0.2668,  ..., -0.0279,  0.1831,  0.1775],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.7.feed_forward.layer_norm.weight': tensor([1.4658, 1.5234, 1.4697,  ..., 1.4619, 1.3975, 1.0264],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.7.feed_forward.layer_norm.bias': tensor([ 0.2140, -0.1531, -0.1310,  ...,  0.1624, -0.3196,  0.5806],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.8.self_attn.linear_keys.weight': tensor([[-0.0688, -0.0091, -0.1277,  ..., -0.0792, -0.1315,  0.2549],\n",
       "         [ 0.0327, -0.0327,  0.1985,  ..., -0.1526, -0.1306, -0.0013],\n",
       "         [ 0.0291, -0.2593,  0.0817,  ..., -0.0741,  0.0684, -0.0975],\n",
       "         ...,\n",
       "         [ 0.1205,  0.0697, -0.0167,  ...,  0.0344, -0.0850,  0.0958],\n",
       "         [-0.0356, -0.0033,  0.0279,  ...,  0.2449,  0.3352,  0.3486],\n",
       "         [-0.0198, -0.0017, -0.0009,  ..., -0.1373,  0.0296,  0.0921]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.8.self_attn.linear_keys.bias': tensor([ 0.0190,  0.0186,  0.0075,  ..., -0.0027,  0.0239, -0.0230],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.8.self_attn.linear_values.weight': tensor([[ 2.1698e-02,  5.0391e-01, -6.3599e-02,  ...,  3.4375e-01,\n",
       "          -5.6732e-02, -2.5439e-01],\n",
       "         [-5.0098e-01,  4.8828e-01, -3.9917e-01,  ..., -6.4163e-03,\n",
       "           2.7271e-01,  5.1239e-02],\n",
       "         [ 5.0342e-01, -2.5781e-01, -9.2224e-02,  ..., -2.5854e-01,\n",
       "           3.8013e-01,  2.5391e-01],\n",
       "         ...,\n",
       "         [-2.3806e-04, -2.4878e-01, -3.6865e-01,  ...,  1.6638e-01,\n",
       "           4.3030e-02,  1.0706e-01],\n",
       "         [-7.1472e-02,  5.3406e-02,  3.3667e-01,  ..., -4.8535e-01,\n",
       "          -4.1321e-02, -2.5244e-01],\n",
       "         [ 5.5518e-01, -6.1310e-02,  4.2041e-01,  ...,  2.2180e-01,\n",
       "           1.3086e-01, -1.9629e-01]], dtype=torch.float16),\n",
       " 'encoder.transformer.8.self_attn.linear_values.bias': tensor([ 0.0462, -0.1036, -0.0646,  ..., -0.1625,  0.0988,  0.0123],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.8.self_attn.linear_query.weight': tensor([[ 0.0249, -0.0852, -0.0412,  ...,  0.1812,  0.1111,  0.2671],\n",
       "         [-0.0025,  0.0505, -0.0931,  ...,  0.1137, -0.0060, -0.0775],\n",
       "         [ 0.2181,  0.0528,  0.0793,  ...,  0.2654,  0.0399, -0.0110],\n",
       "         ...,\n",
       "         [-0.0602,  0.0107, -0.1671,  ...,  0.1301,  0.2366,  0.0963],\n",
       "         [-0.0008,  0.1672,  0.2468,  ..., -0.1220, -0.1705, -0.0371],\n",
       "         [-0.1020,  0.0135,  0.0702,  ..., -0.0245,  0.1068, -0.0165]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.8.self_attn.linear_query.bias': tensor([ 0.4907, -0.1603, -0.2605,  ...,  0.0511,  0.5312,  0.1677],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.8.self_attn.final_linear.weight': tensor([[ 0.5000, -0.0681,  0.1069,  ..., -0.0848, -0.1550,  0.4285],\n",
       "         [-0.2615,  0.5898,  0.2981,  ..., -0.0019, -0.3152, -0.5073],\n",
       "         [-0.1832,  0.0960,  0.3252,  ..., -0.1443, -0.3098,  0.2847],\n",
       "         ...,\n",
       "         [-0.0236,  0.0312,  0.1584,  ..., -0.2113, -0.5024,  0.0351],\n",
       "         [ 0.4995,  0.0998,  0.1969,  ...,  0.3953,  0.0782, -0.1827],\n",
       "         [ 0.4517, -0.3076, -0.0215,  ...,  0.5000, -0.2598,  0.3438]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.8.self_attn.final_linear.bias': tensor([ 0.1201,  0.2529, -0.2769,  ..., -0.3010,  0.0786, -0.0475],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.8.layer_norm.weight': tensor([0.3359, 0.3503, 0.3406,  ..., 0.3118, 0.3076, 0.1715],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.8.layer_norm.bias': tensor([ 0.0137, -0.0487, -0.0163,  ..., -0.0402, -0.0260, -0.1025],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.8.feed_forward.w_1.weight': tensor([[-0.3015, -0.1304,  0.0669,  ..., -0.0325, -0.2152, -0.0944],\n",
       "         [-0.2681, -0.3765,  0.2029,  ...,  0.0684, -0.3738,  0.5010],\n",
       "         [ 0.1047, -0.2003, -0.0580,  ...,  0.0778, -0.3823,  0.1543],\n",
       "         ...,\n",
       "         [ 0.2312, -0.0673, -0.2435,  ..., -0.1077,  0.1111,  0.4194],\n",
       "         [-0.0049, -0.4768,  0.1390,  ...,  0.1024,  0.1753,  0.2595],\n",
       "         [-0.2419, -0.3213, -0.1992,  ..., -0.2561, -0.0931,  0.1797]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.8.feed_forward.w_1.bias': tensor([ 0.0402, -0.1921,  0.0009,  ..., -0.1022, -0.2727, -0.0250],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.8.feed_forward.w_2.weight': tensor([[ 2.2571e-01,  5.2917e-02,  3.8025e-02,  ...,  1.0181e-01,\n",
       "          -2.2070e-01,  8.1055e-02],\n",
       "         [-2.0561e-03, -7.6782e-02,  1.0541e-01,  ...,  3.5132e-01,\n",
       "          -4.7266e-01,  1.1688e-01],\n",
       "         [ 7.1564e-03, -2.0728e-01, -2.0337e-01,  ...,  1.1371e-01,\n",
       "          -2.3010e-01,  1.0876e-01],\n",
       "         ...,\n",
       "         [-2.3773e-02,  1.3578e-04, -8.6914e-02,  ...,  7.0740e-02,\n",
       "          -2.6953e-01,  2.6047e-02],\n",
       "         [ 9.0515e-02, -1.7542e-01,  3.3740e-01,  ...,  2.4524e-01,\n",
       "           1.2427e-01, -1.3501e-01],\n",
       "         [-9.7717e-02, -6.4453e-02,  9.1858e-02,  ...,  1.1902e-02,\n",
       "          -2.8101e-01,  2.2546e-01]], dtype=torch.float16),\n",
       " 'encoder.transformer.8.feed_forward.w_2.bias': tensor([-0.2666, -0.3450,  0.4014,  ..., -0.3533,  0.4529,  0.5000],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.8.feed_forward.layer_norm.weight': tensor([1.7168, 1.6738, 1.7178,  ..., 1.6064, 1.6738, 1.0039],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.8.feed_forward.layer_norm.bias': tensor([ 0.3455, -0.1840, -0.2661,  ...,  0.2267, -0.2754,  0.4849],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.9.self_attn.linear_keys.weight': tensor([[-0.0934,  0.0262,  0.0886,  ...,  0.1398,  0.0416,  0.0706],\n",
       "         [ 0.0422, -0.0273,  0.0023,  ...,  0.0563, -0.0426, -0.0468],\n",
       "         [ 0.0401,  0.0974, -0.2075,  ...,  0.0203, -0.0175, -0.0652],\n",
       "         ...,\n",
       "         [-0.1060,  0.0012, -0.0681,  ...,  0.2020, -0.1016, -0.1328],\n",
       "         [-0.0526,  0.1748, -0.0415,  ...,  0.1202,  0.0574, -0.2179],\n",
       "         [-0.1506, -0.1725,  0.0459,  ..., -0.1196, -0.1444, -0.0065]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.9.self_attn.linear_keys.bias': tensor([ 0.0083,  0.0081,  0.0163,  ..., -0.0311,  0.0125, -0.0013],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.9.self_attn.linear_values.weight': tensor([[-0.2930, -0.3538,  0.0164,  ...,  0.4600,  0.1323, -0.1270],\n",
       "         [ 0.0593, -0.1552, -0.1039,  ..., -0.1021, -0.5010, -0.3408],\n",
       "         [ 0.5088, -0.0402, -0.3328,  ...,  0.3240,  0.4419, -0.1626],\n",
       "         ...,\n",
       "         [ 0.1453,  0.3970, -0.5234,  ..., -0.2471, -0.4275, -0.0396],\n",
       "         [ 0.2164,  0.4973, -0.1378,  ...,  0.0124, -0.0421,  0.1594],\n",
       "         [-0.1237, -0.2634, -0.2815,  ...,  0.0323,  0.0985, -0.0444]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.9.self_attn.linear_values.bias': tensor([-0.0606,  0.2057,  0.1285,  ...,  0.0414, -0.0179, -0.1126],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.9.self_attn.linear_query.weight': tensor([[ 0.0533, -0.0367, -0.0298,  ..., -0.1113, -0.1151,  0.0941],\n",
       "         [-0.1364,  0.1113, -0.1127,  ..., -0.2859,  0.1389,  0.1482],\n",
       "         [-0.0801, -0.3044, -0.0587,  ...,  0.1190,  0.0527, -0.0526],\n",
       "         ...,\n",
       "         [ 0.0027,  0.0098,  0.0071,  ..., -0.0561, -0.0421,  0.0130],\n",
       "         [-0.0845, -0.0067,  0.0457,  ..., -0.0438,  0.0351,  0.0575],\n",
       "         [ 0.0757,  0.1078,  0.1119,  ...,  0.0146, -0.0102,  0.1037]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.9.self_attn.linear_query.bias': tensor([-0.0395,  0.2544,  0.0701,  ..., -0.0873, -0.4170,  0.1514],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.9.self_attn.final_linear.weight': tensor([[ 0.1595,  0.1053,  0.3035,  ..., -0.3784, -0.0549, -0.2336],\n",
       "         [-0.2150, -0.3525,  0.2207,  ..., -0.0771, -0.3467,  0.3674],\n",
       "         [ 0.0105,  0.2457,  0.1207,  ...,  0.3616, -0.4292,  0.0353],\n",
       "         ...,\n",
       "         [ 0.5269, -0.0899, -0.2500,  ...,  0.1394, -0.0237,  0.4280],\n",
       "         [ 0.0251, -0.3645,  0.2576,  ..., -0.2942, -0.0091, -0.1046],\n",
       "         [-0.3721,  0.2549, -0.1851,  ...,  0.2076, -0.0285, -0.1213]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.9.self_attn.final_linear.bias': tensor([ 0.1194,  0.1234, -0.3315,  ..., -0.1862, -0.2507, -0.3210],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.9.layer_norm.weight': tensor([0.3367, 0.3257, 0.3416,  ..., 0.3267, 0.3186, 0.2015],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.9.layer_norm.bias': tensor([-0.0108, -0.0378, -0.0120,  ..., -0.0397, -0.0296,  0.0007],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.9.feed_forward.w_1.weight': tensor([[-0.0850, -0.0334,  0.3296,  ..., -0.1797,  0.3176, -0.1475],\n",
       "         [ 0.2062,  0.0526,  0.4924,  ...,  0.3694,  0.2479, -0.2455],\n",
       "         [-0.1028, -0.2615, -0.3381,  ...,  0.1091, -0.3706,  0.1146],\n",
       "         ...,\n",
       "         [ 0.1309, -0.0182, -0.2957,  ...,  0.1622,  0.2084,  0.1324],\n",
       "         [-0.0830, -0.2047, -0.0750,  ..., -0.1594, -0.1129, -0.0185],\n",
       "         [ 0.2568,  0.1365, -0.0729,  ...,  0.1531,  0.0869,  0.1686]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.9.feed_forward.w_1.bias': tensor([-0.0652, -0.2603, -0.2491,  ..., -0.2477, -0.1270,  0.0648],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.9.feed_forward.w_2.weight': tensor([[ 0.0651, -0.1250,  0.2908,  ..., -0.2876,  0.0086, -0.3081],\n",
       "         [-0.4285,  0.4309, -0.1884,  ..., -0.0551, -0.2028, -0.0470],\n",
       "         [ 0.1276, -0.3735, -0.4778,  ..., -0.2922,  0.0392,  0.0829],\n",
       "         ...,\n",
       "         [-0.3789,  0.1226,  0.0482,  ...,  0.4937,  0.3669, -0.0059],\n",
       "         [-0.2620,  0.1331, -0.2130,  ..., -0.1647, -0.0382,  0.0402],\n",
       "         [-0.1763, -0.3599,  0.0088,  ..., -0.2683, -0.1316,  0.0857]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.9.feed_forward.w_2.bias': tensor([-0.2073, -0.3306,  0.3516,  ..., -0.1648,  0.4053,  0.5000],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.9.feed_forward.layer_norm.weight': tensor([1.8896, 1.8975, 1.8369,  ..., 1.7158, 1.8867, 1.0020],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.9.feed_forward.layer_norm.bias': tensor([ 0.3481, -0.0704, -0.3877,  ...,  0.0132, -0.2754,  0.4976],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.10.self_attn.linear_keys.weight': tensor([[ 0.0754, -0.0750, -0.0309,  ..., -0.0340,  0.1871, -0.0750],\n",
       "         [-0.1177,  0.2014,  0.1215,  ...,  0.3188, -0.0710, -0.0375],\n",
       "         [ 0.1743, -0.0444,  0.1678,  ..., -0.1851,  0.1493,  0.0254],\n",
       "         ...,\n",
       "         [-0.1746, -0.0400,  0.0757,  ...,  0.1132, -0.1665,  0.1760],\n",
       "         [ 0.0428, -0.0995, -0.0690,  ...,  0.0299, -0.1648,  0.1179],\n",
       "         [-0.0603,  0.0314, -0.0255,  ...,  0.0553,  0.0923,  0.2301]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.10.self_attn.linear_keys.bias': tensor([ 0.0078, -0.0056, -0.0106,  ..., -0.0212, -0.0186, -0.0229],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.10.self_attn.linear_values.weight': tensor([[-0.1522,  0.2383,  0.1865,  ...,  0.2905,  0.0820,  0.0166],\n",
       "         [ 0.2666,  0.1146, -0.0781,  ...,  0.1699, -0.3335,  0.1304],\n",
       "         [-0.4468, -0.3083,  0.0360,  ..., -0.0930, -0.3240, -0.0488],\n",
       "         ...,\n",
       "         [ 0.0994, -0.2683,  0.5088,  ..., -0.0464, -0.1176,  0.3032],\n",
       "         [ 0.2908,  0.2324, -0.0009,  ..., -0.1620,  0.3689,  0.2500],\n",
       "         [ 0.0656, -0.1532, -0.0412,  ...,  0.4946, -0.2261, -0.2732]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.10.self_attn.linear_values.bias': tensor([-0.0977, -0.1240,  0.0131,  ..., -0.0121,  0.0525, -0.0734],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.10.self_attn.linear_query.weight': tensor([[-0.1930,  0.0522,  0.0082,  ...,  0.0521,  0.0998, -0.0263],\n",
       "         [-0.0575,  0.1549, -0.1658,  ..., -0.1860,  0.2107, -0.1044],\n",
       "         [-0.0764,  0.0324,  0.0944,  ...,  0.1270, -0.0540, -0.0314],\n",
       "         ...,\n",
       "         [-0.0464, -0.0764,  0.0583,  ...,  0.2264,  0.1029,  0.0735],\n",
       "         [ 0.0351, -0.0892, -0.0684,  ...,  0.1586,  0.0099, -0.1727],\n",
       "         [-0.0319, -0.0603, -0.1011,  ..., -0.3140,  0.0933, -0.0415]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.10.self_attn.linear_query.bias': tensor([-0.3320,  0.1873,  0.0091,  ...,  0.1371,  0.0784, -0.0020],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.10.self_attn.final_linear.weight': tensor([[-0.5034, -0.2983,  0.0629,  ...,  0.0390, -0.5015, -0.2040],\n",
       "         [ 0.1003,  0.1798,  0.1031,  ..., -0.3835,  0.0084,  0.5010],\n",
       "         [ 0.1758,  0.1630, -0.2917,  ...,  0.0962,  0.4963,  0.5000],\n",
       "         ...,\n",
       "         [ 0.1755,  0.3667,  0.3611,  ..., -0.2289,  0.1060, -0.2825],\n",
       "         [-0.2671, -0.4214, -0.3074,  ..., -0.2625, -0.3059,  0.0285],\n",
       "         [ 0.4990,  0.3005, -0.4709,  ...,  0.2185, -0.4902,  0.2612]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.10.self_attn.final_linear.bias': tensor([-0.0365,  0.1382, -0.2482,  ..., -0.3245, -0.1206, -0.3130],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.10.layer_norm.weight': tensor([0.3611, 0.3506, 0.3416,  ..., 0.3398, 0.3438, 0.2307],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.10.layer_norm.bias': tensor([-0.0012, -0.0452, -0.0125,  ..., -0.0408, -0.0147,  0.0466],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.10.feed_forward.w_1.weight': tensor([[ 0.1156, -0.0344,  0.1548,  ..., -0.2179,  0.3853, -0.0242],\n",
       "         [ 0.3975, -0.1139,  0.0091,  ..., -0.0827,  0.4434, -0.1761],\n",
       "         [ 0.3960,  0.0216, -0.0314,  ...,  0.0225, -0.2169, -0.0543],\n",
       "         ...,\n",
       "         [ 0.1395, -0.3721,  0.1024,  ..., -0.0687,  0.1179, -0.2325],\n",
       "         [-0.1383, -0.0200,  0.1689,  ...,  0.2786, -0.1664, -0.4736],\n",
       "         [-0.0500, -0.1879,  0.0883,  ...,  0.0870,  0.1624, -0.0334]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.10.feed_forward.w_1.bias': tensor([-0.1774, -0.2493,  0.0089,  ...,  0.0812, -0.1042, -0.0112],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.10.feed_forward.w_2.weight': tensor([[ 0.0633,  0.3196, -0.3215,  ..., -0.1116,  0.1827,  0.2673],\n",
       "         [ 0.0570, -0.2937, -0.0856,  ...,  0.1217, -0.1844,  0.2048],\n",
       "         [-0.4844, -0.1295, -0.0923,  ...,  0.0325, -0.0368, -0.0098],\n",
       "         ...,\n",
       "         [-0.0031,  0.2913, -0.1672,  ...,  0.0862,  0.0590, -0.1099],\n",
       "         [-0.2598,  0.1019,  0.2947,  ..., -0.1364, -0.1061,  0.0092],\n",
       "         [ 0.1287, -0.1024,  0.1221,  ..., -0.0401,  0.0828, -0.0964]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.10.feed_forward.w_2.bias': tensor([-0.1252, -0.2593,  0.1232,  ...,  0.0485,  0.2494,  0.5000],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.10.feed_forward.layer_norm.weight': tensor([1.7891, 1.7773, 1.8184,  ..., 1.7373, 1.7686, 1.0000],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.10.feed_forward.layer_norm.bias': tensor([ 0.3123, -0.1324, -0.2803,  ..., -0.2485, -0.1224,  0.3059],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.11.self_attn.linear_keys.weight': tensor([[ 0.0086, -0.0414, -0.3499,  ...,  0.1313,  0.1610,  0.0051],\n",
       "         [ 0.1420,  0.0161, -0.0805,  ...,  0.0367,  0.0750, -0.0147],\n",
       "         [ 0.1501,  0.0788, -0.0873,  ..., -0.0138,  0.0100, -0.0401],\n",
       "         ...,\n",
       "         [-0.2258,  0.1349, -0.0797,  ...,  0.0210, -0.0510, -0.0725],\n",
       "         [ 0.0133, -0.2283, -0.1971,  ...,  0.2808, -0.1448,  0.1022],\n",
       "         [-0.2223,  0.0643, -0.2065,  ..., -0.0892,  0.0763,  0.0452]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.11.self_attn.linear_keys.bias': tensor([-0.0247, -0.0124,  0.0158,  ...,  0.0298,  0.0184, -0.0053],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.11.self_attn.linear_values.weight': tensor([[-0.3228, -0.2213, -0.4314,  ..., -0.1243,  0.0023, -0.1805],\n",
       "         [-0.2544,  0.2683,  0.0088,  ..., -0.3044,  0.3425, -0.0644],\n",
       "         [ 0.0420,  0.0554, -0.0193,  ..., -0.2462,  0.1820, -0.2064],\n",
       "         ...,\n",
       "         [-0.3894,  0.0691, -0.2698,  ...,  0.0479,  0.0766,  0.0311],\n",
       "         [-0.5674, -0.0828, -0.1783,  ..., -0.1588,  0.3806, -0.0439],\n",
       "         [ 0.2032,  0.0678, -0.4326,  ..., -0.1093, -0.0641,  0.1697]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.11.self_attn.linear_values.bias': tensor([ 0.0423,  0.0190, -0.0619,  ..., -0.0843,  0.0345, -0.0243],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.11.self_attn.linear_query.weight': tensor([[ 0.1095, -0.1726,  0.1010,  ..., -0.0748, -0.0682, -0.0528],\n",
       "         [ 0.0183, -0.0203,  0.2111,  ...,  0.5200,  0.1798, -0.0073],\n",
       "         [ 0.1667,  0.0603, -0.1227,  ...,  0.1139,  0.0238,  0.0292],\n",
       "         ...,\n",
       "         [-0.1715,  0.1740,  0.0278,  ..., -0.0693,  0.1809, -0.0255],\n",
       "         [-0.0011, -0.0814,  0.0205,  ...,  0.0806, -0.0358,  0.1978],\n",
       "         [ 0.0307, -0.0151,  0.0194,  ..., -0.1848, -0.0357,  0.0113]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.11.self_attn.linear_query.bias': tensor([-0.0655,  0.1114, -0.0804,  ..., -0.0082,  0.4026, -0.2406],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.11.self_attn.final_linear.weight': tensor([[-0.1425, -0.1304,  0.2463,  ...,  0.2605,  0.0704,  0.5000],\n",
       "         [-0.3113,  0.3472,  0.5015,  ...,  0.2778,  0.4353,  0.2678],\n",
       "         [-0.1364, -0.0914, -0.2534,  ..., -0.2903, -0.2021,  0.0350],\n",
       "         ...,\n",
       "         [-0.3306, -0.3472,  0.4177,  ...,  0.0071, -0.5371, -0.0557],\n",
       "         [ 0.4880, -0.2247, -0.1022,  ...,  0.0623, -0.1614, -0.0088],\n",
       "         [-0.0360, -0.0154,  0.0797,  ...,  0.0604, -0.0828,  0.0997]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.11.self_attn.final_linear.bias': tensor([-0.0739, -0.0063, -0.0346,  ..., -0.2489, -0.1971, -0.2751],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.11.layer_norm.weight': tensor([0.3723, 0.3530, 0.3521,  ..., 0.3430, 0.3640, 0.5117],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.11.layer_norm.bias': tensor([ 0.0076, -0.0373, -0.0152,  ..., -0.0352,  0.0002,  0.1772],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.11.feed_forward.w_1.weight': tensor([[ 2.5732e-01, -1.6187e-01, -4.4678e-01,  ...,  5.9845e-02,\n",
       "          -3.4668e-01, -6.9824e-02],\n",
       "         [ 1.7688e-01, -1.4697e-01,  3.6548e-01,  ..., -4.4287e-01,\n",
       "          -1.0376e-01, -8.9600e-02],\n",
       "         [-2.4670e-01,  2.3022e-01,  3.3618e-01,  ..., -1.1487e-01,\n",
       "          -3.8867e-01,  1.1646e-01],\n",
       "         ...,\n",
       "         [-1.4368e-01, -6.0242e-02,  3.1592e-01,  ...,  7.5500e-02,\n",
       "           1.4355e-01,  8.1177e-02],\n",
       "         [ 4.7302e-02,  3.1885e-01,  1.1621e-01,  ..., -6.1920e-02,\n",
       "          -9.5032e-02,  5.9395e-03],\n",
       "         [-3.4888e-01, -3.6890e-01, -3.6548e-01,  ...,  2.2339e-01,\n",
       "           2.6584e-04,  1.1902e-01]], dtype=torch.float16),\n",
       " 'encoder.transformer.11.feed_forward.w_1.bias': tensor([-0.2379, -0.0254, -0.0665,  ..., -0.1392, -0.1793, -0.1711],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.11.feed_forward.w_2.weight': tensor([[-0.0247, -0.5405,  0.3447,  ...,  0.0279,  0.1028,  0.2759],\n",
       "         [ 0.1445,  0.2554,  0.0915,  ..., -0.1010, -0.0536,  0.1388],\n",
       "         [-0.0830, -0.0148, -0.2334,  ..., -0.0434, -0.1500,  0.2351],\n",
       "         ...,\n",
       "         [-0.4978,  0.0639, -0.3618,  ..., -0.0822,  0.0499,  0.0111],\n",
       "         [-0.1675, -0.2073, -0.1583,  ..., -0.1167, -0.0303,  0.3142],\n",
       "         [-0.0075,  0.0165,  0.0419,  ..., -0.0137,  0.0292, -0.0189]],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.11.feed_forward.w_2.bias': tensor([-0.0811, -0.1230,  0.0258,  ...,  0.1039,  0.1157,  0.2913],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.11.feed_forward.layer_norm.weight': tensor([1.3672, 1.2451, 1.1787,  ..., 1.4297, 1.2148, 1.0000],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.transformer.11.feed_forward.layer_norm.bias': tensor([ 0.1785, -0.0873, -0.0390,  ..., -0.2549,  0.0047, -0.2499],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.layer_norm.weight': tensor([0.4553, 0.4626, 0.4673,  ..., 0.4241, 0.4702, 0.7456],\n",
       "        dtype=torch.float16),\n",
       " 'encoder.layer_norm.bias': tensor([ 0.0014, -0.0053,  0.0018,  ..., -0.0308,  0.0012, -0.5000],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.embeddings.make_embedding.emb_luts.0.weight': tensor([[-0.0321,  0.0348,  0.0181,  ...,  0.0312, -0.0099, -0.0133],\n",
       "         [-0.0039,  0.0104, -0.0156,  ...,  0.0290, -0.0138, -0.0134],\n",
       "         [-0.0245, -0.0283, -0.0295,  ...,  0.9712, -0.0255, -0.0273],\n",
       "         ...,\n",
       "         [-0.0123, -0.0031, -0.0089,  ...,  0.0645, -0.0182, -0.0740],\n",
       "         [ 0.0085, -0.0088, -0.0091,  ...,  0.0571, -0.0035, -0.1298],\n",
       "         [-0.0076, -0.0107, -0.0051,  ...,  1.0264, -0.0338, -0.1175]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.embeddings.make_embedding.pe.pe': tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  1.0000,  1.0000]],\n",
       " \n",
       "         [[ 0.8415,  0.8317,  0.8218,  ...,  1.0000,  1.0000,  1.0000]],\n",
       " \n",
       "         [[ 0.9093,  0.9236,  0.9365,  ...,  1.0000,  1.0000,  1.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.9563,  0.5417,  0.7653,  ...,  0.8688,  0.8733,  0.8777]],\n",
       " \n",
       "         [[ 0.2705,  0.9999,  0.9649,  ...,  0.8687,  0.8733,  0.8777]],\n",
       " \n",
       "         [[-0.6639,  0.5685,  0.3343,  ...,  0.8687,  0.8732,  0.8776]]]),\n",
       " 'decoder.transformer_layers.0.self_attn.linear_keys.weight': tensor([[ 0.2527, -0.0136, -0.0460,  ...,  0.0115,  0.2996, -0.2917],\n",
       "         [-0.2238, -0.0130,  0.0161,  ..., -0.0285, -0.1180, -0.1647],\n",
       "         [-0.0419,  0.1165,  0.0529,  ..., -0.1235, -0.1420, -0.0377],\n",
       "         ...,\n",
       "         [-0.1418, -0.2262, -0.2668,  ..., -0.4019,  0.1725, -0.4397],\n",
       "         [ 0.8843,  0.7051, -0.0199,  ...,  0.0214, -0.0945, -0.0924],\n",
       "         [ 0.5151,  0.7905,  0.5708,  ..., -0.1842,  0.1663, -0.4202]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.self_attn.linear_keys.bias': tensor([ 0.0171,  0.0190, -0.0028,  ...,  0.0214,  0.0051,  0.0162],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.self_attn.linear_values.weight': tensor([[ 0.1139, -0.0122,  0.0055,  ..., -0.0195, -0.0603, -0.2316],\n",
       "         [-0.0013,  0.0484, -0.0739,  ...,  0.0040,  0.0352,  0.0255],\n",
       "         [ 0.0599, -0.0119, -0.0344,  ...,  0.0804,  0.0247, -0.1047],\n",
       "         ...,\n",
       "         [-0.0121, -0.1064, -0.1040,  ...,  0.1565, -0.1312, -0.0582],\n",
       "         [ 0.0189, -0.0357, -0.0127,  ...,  0.1639, -0.0505,  0.2944],\n",
       "         [ 0.0603,  0.0012, -0.0043,  ...,  0.0229, -0.0510, -0.0253]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.self_attn.linear_values.bias': tensor([-0.1753, -0.0671,  0.0499,  ...,  0.0657, -0.1055,  0.1422],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.self_attn.linear_query.weight': tensor([[-0.0608, -0.0219, -0.0292,  ..., -0.2030,  0.2261,  0.2169],\n",
       "         [ 0.1031,  0.2079, -0.1537,  ...,  0.4563, -0.2065, -0.2493],\n",
       "         [-0.0536, -0.1057,  0.1284,  ...,  0.4861,  0.1749,  0.3396],\n",
       "         ...,\n",
       "         [-0.5005, -0.5000, -0.5034,  ...,  0.0190,  0.0634,  0.2839],\n",
       "         [ 0.7227,  0.9077,  0.5024,  ..., -0.3882,  0.0387, -0.2537],\n",
       "         [ 0.4541,  0.6118,  0.9834,  ...,  0.1675, -0.1345,  0.0667]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.self_attn.linear_query.bias': tensor([ 0.1714, -0.2122,  0.0087,  ..., -0.0495, -0.3044, -0.4622],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.self_attn.final_linear.weight': tensor([[-0.0922, -0.0627,  0.0594,  ..., -0.0709, -0.0629,  0.0347],\n",
       "         [-0.0212,  0.0095,  0.0910,  ..., -0.0070,  0.0226,  0.1028],\n",
       "         [ 0.0100, -0.0276,  0.0167,  ...,  0.0674,  0.0024, -0.0916],\n",
       "         ...,\n",
       "         [-0.0880, -0.1554, -0.0174,  ...,  0.0215, -0.1620,  0.0854],\n",
       "         [-0.1394,  0.1870,  0.1448,  ...,  0.1100, -0.1603,  0.1107],\n",
       "         [-0.1072,  0.2668,  0.0040,  ..., -0.1271, -0.0542,  0.1467]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.self_attn.final_linear.bias': tensor([-0.1847, -0.5151, -0.2632,  ...,  0.0610,  0.0335,  0.0851],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.layer_norm_1.weight': tensor([0.2671, 1.0840, 1.7480,  ..., 0.0716, 0.1120, 0.1166],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.layer_norm_1.bias': tensor([-0.0013,  0.0142,  0.0267,  ...,  0.0082,  0.0058,  0.0043],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.context_attn.linear_keys.weight': tensor([[ 2.7686e-01,  1.1139e-01,  1.2846e-03,  ...,  1.1462e-01,\n",
       "          -3.6523e-01,  2.1698e-02],\n",
       "         [ 3.3984e-01,  5.4596e-02,  1.8262e-01,  ..., -1.6614e-01,\n",
       "           5.4962e-02,  9.5703e-02],\n",
       "         [-3.4485e-02,  3.1982e-02,  2.4927e-01,  ...,  1.3464e-01,\n",
       "          -1.5210e-01,  2.5131e-02],\n",
       "         ...,\n",
       "         [ 1.2830e-01, -9.8999e-02, -4.3060e-02,  ..., -1.1230e-01,\n",
       "           4.0527e-02,  3.7445e-02],\n",
       "         [-6.1572e-01,  3.0398e-05, -5.9692e-02,  ..., -2.1881e-02,\n",
       "           9.4528e-03,  9.6970e-03],\n",
       "         [-4.3845e-04, -4.5288e-02,  7.2693e-02,  ..., -2.0020e-02,\n",
       "          -1.5234e-01, -4.1809e-02]], dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.context_attn.linear_keys.bias': tensor([-0.0035,  0.0259,  0.0131,  ...,  0.0161, -0.0030, -0.0134],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.context_attn.linear_values.weight': tensor([[-2.2168e-01,  1.9531e-01, -2.2302e-01,  ..., -1.5430e-01,\n",
       "          -1.2484e-03, -3.5038e-03],\n",
       "         [-3.2806e-02, -1.0162e-01, -2.2998e-01,  ...,  1.2439e-01,\n",
       "           1.1713e-01,  2.1912e-02],\n",
       "         [ 8.3557e-02, -7.8430e-03,  4.4479e-03,  ..., -7.7515e-02,\n",
       "           1.2612e-04,  6.9237e-03],\n",
       "         ...,\n",
       "         [-1.2396e-01, -4.8584e-02,  6.2744e-02,  ...,  1.0675e-01,\n",
       "          -1.0156e-01, -3.1647e-02],\n",
       "         [ 1.4001e-01, -7.8613e-02, -2.6871e-02,  ..., -1.4417e-01,\n",
       "           1.5289e-02,  5.5725e-02],\n",
       "         [ 7.7400e-03,  1.3008e-02,  1.4258e-01,  ..., -1.0828e-01,\n",
       "           1.8225e-01, -2.6154e-02]], dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.context_attn.linear_values.bias': tensor([ 0.0178, -0.0514, -0.0856,  ..., -0.0892, -0.1333,  0.0561],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.context_attn.linear_query.weight': tensor([[ 0.0181,  0.1831, -0.3489,  ...,  0.0055,  0.0179,  0.3159],\n",
       "         [-0.0963,  0.0248, -0.2091,  ..., -0.0937, -0.1661,  0.0223],\n",
       "         [-0.3262,  0.1173,  0.0812,  ..., -0.0140, -0.0066,  0.2837],\n",
       "         ...,\n",
       "         [ 0.1371, -0.1003,  0.0622,  ...,  0.0039, -0.0370, -0.1047],\n",
       "         [-0.5054,  0.0831, -0.1569,  ..., -0.0121,  0.0830, -0.0902],\n",
       "         [-0.2988, -0.0426,  0.0012,  ...,  0.2725,  0.2629, -0.3853]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.context_attn.linear_query.bias': tensor([-0.0820,  0.0203,  0.0393,  ...,  0.0010, -0.1102, -0.0185],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.context_attn.final_linear.weight': tensor([[ 5.9479e-02,  5.2246e-02,  1.3879e-01,  ..., -9.1675e-02,\n",
       "          -2.8214e-02,  1.6602e-02],\n",
       "         [ 4.7424e-02,  9.0332e-02, -1.6830e-02,  ..., -4.2999e-02,\n",
       "           5.8899e-02, -5.5199e-03],\n",
       "         [ 5.9357e-02, -1.4062e-01,  1.0791e-01,  ...,  7.4707e-02,\n",
       "          -3.4363e-02, -7.2876e-02],\n",
       "         ...,\n",
       "         [-1.1938e-01, -1.7102e-01, -6.7566e-02,  ...,  5.9052e-02,\n",
       "           5.6091e-02,  7.9163e-02],\n",
       "         [ 2.0984e-01,  8.2825e-02,  1.6260e-01,  ...,  4.7180e-02,\n",
       "          -1.4214e-02,  1.7029e-01],\n",
       "         [ 4.0222e-02,  1.0779e-01, -3.9624e-01,  ...,  9.2896e-02,\n",
       "          -4.1656e-02, -9.8169e-05]], dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.context_attn.final_linear.bias': tensor([-0.2084,  0.1163, -0.0021,  ..., -0.0227,  0.0979,  0.2081],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.layer_norm_2.weight': tensor([0.1486, 0.3196, 0.3987,  ..., 0.1199, 0.0912, 0.0984],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.layer_norm_2.bias': tensor([-0.0154, -0.0182, -0.0321,  ..., -0.0105, -0.0070, -0.0171],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.feed_forward.w_1.weight': tensor([[ 0.1344,  0.7622,  0.2412,  ...,  0.2900, -0.2639, -0.3789],\n",
       "         [-0.0172,  0.1448, -0.2893,  ...,  0.1109, -0.2581, -0.0345],\n",
       "         [ 0.1434, -0.1132,  0.1077,  ..., -0.1946,  0.0080, -0.4224],\n",
       "         ...,\n",
       "         [-0.0646,  0.4250, -0.0431,  ..., -0.1595, -0.1165,  0.3054],\n",
       "         [ 0.2461, -0.0331,  0.0623,  ...,  0.1078,  0.2034, -0.1764],\n",
       "         [ 0.3833, -0.1958, -0.3770,  ...,  0.0027, -0.2030, -0.2375]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.feed_forward.w_1.bias': tensor([-0.2781, -0.0742, -0.0874,  ..., -0.0566, -0.0241, -0.2001],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.feed_forward.w_2.weight': tensor([[-0.1371, -0.1132,  0.0466,  ...,  0.2681, -0.0298, -0.1131],\n",
       "         [ 0.2164, -0.0201, -0.0924,  ...,  0.1089, -0.1300, -0.1272],\n",
       "         [ 0.0881,  0.0721, -0.0802,  ..., -0.0063,  0.0746,  0.1890],\n",
       "         ...,\n",
       "         [ 0.1101,  0.0833, -0.1298,  ..., -0.0008, -0.0935, -0.0485],\n",
       "         [-0.3896,  0.1687, -0.1404,  ...,  0.0498,  0.0757,  0.1858],\n",
       "         [-0.2925, -0.0252,  0.2664,  ...,  0.1076,  0.4316,  0.0273]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.feed_forward.w_2.bias': tensor([ 9.9304e-02,  5.1483e-02,  2.4277e-02,  ..., -4.9146e-01,\n",
       "         -1.7102e-01, -5.7220e-06], dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.feed_forward.layer_norm.weight': tensor([0.2993, 0.6899, 0.8115,  ..., 0.2859, 0.1858, 0.2041],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.0.feed_forward.layer_norm.bias': tensor([-0.0114, -0.0121, -0.0416,  ...,  0.0494,  0.0188,  0.0542],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.self_attn.linear_keys.weight': tensor([[ 0.1534, -0.1125, -0.1119,  ..., -0.1871,  0.2374,  0.0232],\n",
       "         [ 0.3218, -0.1428,  0.0255,  ..., -0.1315, -0.0137,  0.1196],\n",
       "         [-0.1437, -0.1470, -0.0161,  ..., -0.2886,  0.1324, -0.1108],\n",
       "         ...,\n",
       "         [-0.1249,  0.1697,  0.4082,  ..., -0.3542, -0.2754, -0.1086],\n",
       "         [ 0.4036, -0.0794,  0.0101,  ..., -0.0826,  0.5649, -0.0558],\n",
       "         [-0.0320,  0.0214, -0.0146,  ...,  0.2869, -0.0469, -0.2600]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.self_attn.linear_keys.bias': tensor([-0.0115,  0.0259, -0.0176,  ...,  0.0260,  0.0236,  0.0175],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.self_attn.linear_values.weight': tensor([[ 0.1611,  0.2081,  0.1771,  ..., -0.2059, -0.0025,  0.0425],\n",
       "         [ 0.0439, -0.1763, -0.0055,  ..., -0.1410, -0.0632, -0.1220],\n",
       "         [ 0.1693,  0.2229,  0.4146,  ...,  0.0634,  0.0104,  0.1014],\n",
       "         ...,\n",
       "         [ 0.0663, -0.0892, -0.1144,  ..., -0.2166, -0.0717,  0.0168],\n",
       "         [ 0.0919,  0.2917, -0.1696,  ..., -0.0205, -0.2695,  0.1384],\n",
       "         [-0.1870,  0.1490, -0.2499,  ..., -0.0961, -0.0006,  0.1333]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.self_attn.linear_values.bias': tensor([ 0.0769, -0.0881, -0.1488,  ..., -0.0063, -0.0072,  0.0170],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.self_attn.linear_query.weight': tensor([[-0.0667,  0.2040,  0.0103,  ..., -0.0268, -0.1711,  0.0939],\n",
       "         [-0.2396,  0.3296, -0.2190,  ...,  0.1805,  0.2666, -0.0803],\n",
       "         [ 0.0565,  0.0851, -0.0085,  ...,  0.4153,  0.1097, -0.0129],\n",
       "         ...,\n",
       "         [-0.0325, -0.0953,  0.1277,  ..., -0.4856, -0.5068,  0.2008],\n",
       "         [ 0.0311, -0.1217, -0.0499,  ...,  0.2189, -0.4949,  0.3477],\n",
       "         [-0.2493,  0.1967,  0.4160,  ..., -0.4136, -0.1923,  0.0126]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.self_attn.linear_query.bias': tensor([ 0.0458, -0.0001,  0.0027,  ..., -0.0262, -0.0651,  0.0208],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.self_attn.final_linear.weight': tensor([[-0.1395,  0.1007, -0.2021,  ..., -0.2854,  0.2642,  0.0026],\n",
       "         [-0.2791,  0.3120, -0.3350,  ...,  0.0471, -0.1516,  0.1459],\n",
       "         [ 0.2573, -0.1997,  0.0403,  ...,  0.0028,  0.0232, -0.1825],\n",
       "         ...,\n",
       "         [ 0.4160,  0.3020, -0.0767,  ...,  0.2146, -0.3484,  0.1781],\n",
       "         [-0.0596, -0.2515,  0.1478,  ...,  0.0984, -0.0452,  0.0968],\n",
       "         [-0.3391,  0.4890, -0.1302,  ...,  0.3733, -0.3181, -0.0583]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.self_attn.final_linear.bias': tensor([-0.1322, -0.5068, -0.3179,  ..., -0.4104,  0.5020,  0.0186],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.layer_norm_1.weight': tensor([0.2335, 0.3899, 0.4363,  ..., 0.1646, 0.1715, 0.1766],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.layer_norm_1.bias': tensor([ 0.0122,  0.0031,  0.0142,  ..., -0.0172,  0.0160,  0.0054],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.context_attn.linear_keys.weight': tensor([[ 3.3398e-01,  3.1158e-02,  4.3152e-02,  ..., -1.9287e-01,\n",
       "          -1.6858e-01,  4.2175e-02],\n",
       "         [ 3.4515e-02, -1.7725e-01, -7.6660e-02,  ..., -5.5206e-02,\n",
       "          -6.0150e-02, -2.6245e-01],\n",
       "         [-1.0291e-01, -2.0947e-01,  4.8615e-02,  ..., -1.3843e-01,\n",
       "           1.7810e-01, -4.2267e-02],\n",
       "         ...,\n",
       "         [ 3.3081e-02,  8.9050e-02, -9.6863e-02,  ..., -4.3579e-02,\n",
       "          -4.2603e-02,  7.5928e-02],\n",
       "         [ 1.0266e-01, -3.2788e-01, -1.7285e-01,  ..., -2.2192e-01,\n",
       "           2.8801e-04,  2.7344e-01],\n",
       "         [ 3.6304e-01, -2.3499e-01, -4.5776e-01,  ...,  1.2268e-01,\n",
       "           1.1481e-01, -2.8976e-02]], dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.context_attn.linear_keys.bias': tensor([-0.0274, -0.0099, -0.0295,  ...,  0.0261, -0.0028, -0.0228],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.context_attn.linear_values.weight': tensor([[-0.0637, -0.1584,  0.0154,  ..., -0.1114, -0.0318, -0.0013],\n",
       "         [ 0.2046,  0.1764, -0.1729,  ..., -0.3098,  0.1407, -0.0028],\n",
       "         [ 0.0202,  0.1407,  0.1124,  ...,  0.0605, -0.0785, -0.0263],\n",
       "         ...,\n",
       "         [ 0.1542,  0.1243,  0.1514,  ...,  0.1077, -0.0696,  0.0235],\n",
       "         [-0.1639, -0.0312,  0.2416,  ...,  0.0569,  0.0289,  0.0510],\n",
       "         [-0.0209,  0.0379,  0.0607,  ...,  0.0742,  0.1700, -0.0098]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.context_attn.linear_values.bias': tensor([ 0.0453, -0.0273, -0.0125,  ..., -0.0927, -0.0215,  0.1722],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.context_attn.linear_query.weight': tensor([[-0.0729, -0.0311, -0.1705,  ...,  0.1267,  0.0778, -0.0111],\n",
       "         [ 0.0575,  0.2003,  0.1246,  ...,  0.1231, -0.0115,  0.0741],\n",
       "         [ 0.1191,  0.1030, -0.2544,  ...,  0.0974,  0.0105, -0.0771],\n",
       "         ...,\n",
       "         [-0.5625, -0.1671,  0.2000,  ..., -0.2129,  0.0068,  0.2164],\n",
       "         [ 0.0772, -0.0503,  0.0390,  ..., -0.3298, -0.1742,  0.3379],\n",
       "         [ 0.0915,  0.0277, -0.4065,  ..., -0.1199,  0.4858, -0.0520]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.context_attn.linear_query.bias': tensor([-0.2184, -0.5171, -0.0284,  ..., -0.0183,  0.0548,  0.1344],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.context_attn.final_linear.weight': tensor([[-0.0327,  0.3237, -0.1772,  ...,  0.2573, -0.1348,  0.1152],\n",
       "         [ 0.0838,  0.0265, -0.2900,  ...,  0.2081,  0.1089,  0.0315],\n",
       "         [ 0.1299,  0.1479,  0.1301,  ..., -0.1229, -0.1685,  0.0691],\n",
       "         ...,\n",
       "         [-0.1422,  0.1223,  0.1954,  ..., -0.1614, -0.1797,  0.1772],\n",
       "         [ 0.0510,  0.0167, -0.0936,  ..., -0.1155, -0.2397,  0.1880],\n",
       "         [-0.3721, -0.2397,  0.0360,  ...,  0.0610, -0.0598,  0.0428]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.context_attn.final_linear.bias': tensor([-0.0711,  0.3042, -0.3445,  ..., -0.2491,  0.4126,  0.2225],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.layer_norm_2.weight': tensor([0.1310, 0.2273, 0.2188,  ..., 0.1075, 0.1068, 0.0997],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.layer_norm_2.bias': tensor([-0.0150, -0.0313, -0.0118,  ...,  0.0090,  0.0072, -0.0125],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.feed_forward.w_1.weight': tensor([[ 0.0250,  0.2991, -0.1091,  ..., -0.1277, -0.1576,  0.0674],\n",
       "         [ 0.2976, -0.1415,  0.4507,  ..., -0.1866,  0.1283,  0.0190],\n",
       "         [ 0.3267, -0.0324,  0.1790,  ..., -0.1440, -0.3320, -0.0600],\n",
       "         ...,\n",
       "         [ 0.1567,  0.0956,  0.1346,  ..., -0.3232, -0.1976, -0.3655],\n",
       "         [ 0.4209,  0.2257,  0.0028,  ..., -0.1316, -0.3711,  0.0569],\n",
       "         [ 0.1774, -0.2188,  0.1565,  ..., -0.2905,  0.0203, -0.2708]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.feed_forward.w_1.bias': tensor([-0.1600, -0.0608, -0.1709,  ..., -0.2771, -0.2356, -0.0980],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.feed_forward.w_2.weight': tensor([[-0.2366, -0.0439, -0.2294,  ...,  0.1348,  0.0668, -0.1799],\n",
       "         [ 0.1781,  0.1968,  0.0236,  ..., -0.1300, -0.2268,  0.2125],\n",
       "         [-0.0632,  0.1316,  0.0245,  ...,  0.0402,  0.0947, -0.1364],\n",
       "         ...,\n",
       "         [ 0.0862,  0.2155, -0.0999,  ..., -0.3252, -0.0651,  0.0750],\n",
       "         [ 0.1539,  0.5024,  0.3135,  ..., -0.2551,  0.0806, -0.0781],\n",
       "         [ 0.0167,  0.2075,  0.0334,  ..., -0.1337,  0.0619,  0.1277]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.feed_forward.w_2.bias': tensor([ 0.0667,  0.1331,  0.1181,  ..., -0.2947, -0.1676, -0.1841],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.feed_forward.layer_norm.weight': tensor([0.3525, 0.4556, 0.4563,  ..., 0.3020, 0.2478, 0.2815],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.1.feed_forward.layer_norm.bias': tensor([-0.0249, -0.0007, -0.0149,  ...,  0.0699,  0.0163,  0.0864],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.self_attn.linear_keys.weight': tensor([[-0.3308,  0.3665,  0.0386,  ...,  0.1205, -0.4648,  0.2186],\n",
       "         [ 0.0590,  0.1039,  0.0496,  ...,  0.1836, -0.3901,  0.3811],\n",
       "         [-0.4600, -0.1686,  0.0210,  ...,  0.0736,  0.1978,  0.0778],\n",
       "         ...,\n",
       "         [-0.5742, -0.1058, -0.0801,  ...,  0.3740, -0.0912, -0.0032],\n",
       "         [ 0.2705, -0.2336,  0.1692,  ..., -0.0341, -0.2332,  0.3018],\n",
       "         [ 0.0272, -0.0501,  0.1230,  ..., -0.1545, -0.0927, -0.0757]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.self_attn.linear_keys.bias': tensor([ 0.0152,  0.0089,  0.0074,  ...,  0.0129, -0.0011,  0.0167],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.self_attn.linear_values.weight': tensor([[-0.0995, -0.1671, -0.5259,  ..., -0.2869, -0.1268,  0.0750],\n",
       "         [ 0.0398, -0.0916, -0.0912,  ..., -0.1322, -0.2208,  0.1377],\n",
       "         [ 0.3433,  0.0499,  0.0323,  ...,  0.1874, -0.1367,  0.1506],\n",
       "         ...,\n",
       "         [ 0.0250,  0.0933,  0.0583,  ...,  0.0499, -0.1655, -0.2617],\n",
       "         [ 0.1227, -0.1143,  0.0930,  ...,  0.0697,  0.1770,  0.0898],\n",
       "         [ 0.0276,  0.1170, -0.1428,  ..., -0.0865,  0.0061, -0.1729]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.self_attn.linear_values.bias': tensor([-0.0004,  0.0150,  0.0520,  ...,  0.0121,  0.0303, -0.0188],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.self_attn.linear_query.weight': tensor([[-0.3401,  0.2352,  0.4128,  ...,  0.2554, -0.1636, -0.0297],\n",
       "         [ 0.1205, -0.1771, -0.1298,  ..., -0.0942,  0.0591, -0.1454],\n",
       "         [-0.0718, -0.0007,  0.0856,  ..., -0.1609,  0.2598, -0.0106],\n",
       "         ...,\n",
       "         [-0.0469, -0.1349,  0.0050,  ...,  0.1782,  0.1202, -0.1854],\n",
       "         [ 0.0954,  0.0739, -0.3987,  ...,  0.0170,  0.0279,  0.0166],\n",
       "         [-0.0461,  0.4138, -0.0368,  ..., -0.1155, -0.2113, -0.0650]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.self_attn.linear_query.bias': tensor([-0.0418, -0.0768,  0.0216,  ..., -0.0235,  0.2318, -0.0897],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.self_attn.final_linear.weight': tensor([[-1.5540e-01, -1.6052e-01,  9.6558e-02,  ...,  9.5215e-02,\n",
       "          -5.2490e-01, -1.0329e-04],\n",
       "         [-1.7261e-01,  1.2408e-01,  8.1604e-02,  ..., -2.1411e-01,\n",
       "          -1.5759e-01,  1.6846e-01],\n",
       "         [-2.4548e-01, -2.9126e-01, -7.3509e-03,  ..., -1.0956e-01,\n",
       "          -1.0693e-01, -7.5806e-02],\n",
       "         ...,\n",
       "         [ 2.4243e-01, -2.2388e-01,  3.2446e-01,  ..., -3.1104e-01,\n",
       "           1.0699e-01, -3.1201e-01],\n",
       "         [-4.9121e-01,  1.8164e-01,  1.7285e-01,  ..., -5.4102e-01,\n",
       "           1.8896e-01, -1.7456e-01],\n",
       "         [-1.4868e-01, -4.2686e-03, -1.5247e-01,  ..., -2.5244e-01,\n",
       "           2.0667e-01, -1.2164e-01]], dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.self_attn.final_linear.bias': tensor([-0.4189,  0.2380, -0.4163,  ..., -0.5039, -0.1493,  0.4475],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.layer_norm_1.weight': tensor([0.2588, 0.3323, 0.3691,  ..., 0.1847, 0.1929, 0.1981],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.layer_norm_1.bias': tensor([ 0.0080,  0.0052,  0.0182,  ..., -0.0058,  0.0125, -0.0036],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.context_attn.linear_keys.weight': tensor([[ 3.9368e-03,  1.2466e-02, -1.2500e-01,  ..., -4.4922e-01,\n",
       "           1.3086e-01, -1.2421e-01],\n",
       "         [-4.1553e-01, -2.4854e-01,  6.1249e-02,  ..., -1.4575e-01,\n",
       "           1.7358e-01, -3.6469e-02],\n",
       "         [ 2.8638e-01,  1.9629e-01, -1.6016e-01,  ...,  2.6993e-02,\n",
       "           1.7017e-01, -1.0522e-01],\n",
       "         ...,\n",
       "         [ 4.3457e-01, -1.6064e-01,  2.1777e-01,  ...,  7.3486e-02,\n",
       "          -2.4500e-01,  2.2827e-02],\n",
       "         [ 8.1360e-02,  3.6682e-02, -1.1883e-03,  ..., -8.8074e-02,\n",
       "          -5.0568e-02, -5.5603e-02],\n",
       "         [ 3.4475e-04,  4.4116e-01, -6.1188e-02,  ...,  6.7322e-02,\n",
       "           1.3672e-01, -5.3192e-02]], dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.context_attn.linear_keys.bias': tensor([ 0.0274,  0.0169, -0.0277,  ..., -0.0282, -0.0058, -0.0312],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.context_attn.linear_values.weight': tensor([[ 0.0640, -0.0147, -0.0991,  ..., -0.4907, -0.1158,  0.0133],\n",
       "         [-0.0900, -0.2211, -0.1050,  ..., -0.0369,  0.0284, -0.0867],\n",
       "         [ 0.2096, -0.0718, -0.1810,  ..., -0.0245,  0.1345,  0.0412],\n",
       "         ...,\n",
       "         [-0.1543, -0.3372, -0.2124,  ...,  0.0370, -0.2200, -0.0192],\n",
       "         [-0.0036,  0.1351, -0.0414,  ..., -0.2722, -0.1978,  0.0154],\n",
       "         [-0.0208,  0.1348, -0.2732,  ...,  0.0633,  0.1826,  0.0693]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.context_attn.linear_values.bias': tensor([-0.0737,  0.0847,  0.0364,  ...,  0.0214,  0.0005, -0.0131],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.context_attn.linear_query.weight': tensor([[-0.4275,  0.0265,  0.0817,  ..., -0.0469,  0.2341,  0.2118],\n",
       "         [ 0.3743,  0.1727,  0.0540,  ..., -0.3494,  0.1705, -0.0107],\n",
       "         [-0.1183, -0.0934,  0.1401,  ..., -0.1702,  0.0417, -0.0448],\n",
       "         ...,\n",
       "         [ 0.1122,  0.2097,  0.0602,  ...,  0.1998, -0.2957,  0.0191],\n",
       "         [ 0.2045, -0.2030, -0.1022,  ..., -0.0297, -0.2910,  0.0159],\n",
       "         [-0.4851,  0.4463,  0.0319,  ...,  0.0139, -0.0826, -0.0349]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.context_attn.linear_query.bias': tensor([-0.0117,  0.0652, -0.1204,  ...,  0.0750,  0.0415, -0.0578],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.context_attn.final_linear.weight': tensor([[-5.6915e-02,  1.0187e-01, -2.7832e-01,  ...,  2.2229e-01,\n",
       "          -3.0542e-01, -7.3204e-03],\n",
       "         [ 7.6111e-02,  1.6968e-01, -1.3794e-01,  ..., -8.0750e-02,\n",
       "          -4.1077e-02,  4.9683e-02],\n",
       "         [-4.5776e-02, -2.0325e-01, -1.5405e-01,  ..., -2.6611e-01,\n",
       "           2.3071e-01, -2.0093e-01],\n",
       "         ...,\n",
       "         [ 3.4741e-01,  2.8296e-01,  3.5187e-02,  ...,  3.4546e-02,\n",
       "           7.2754e-02,  1.6708e-02],\n",
       "         [-2.9907e-01,  3.6499e-02, -2.6807e-01,  ..., -3.5181e-01,\n",
       "          -2.1741e-01, -3.6224e-02],\n",
       "         [-2.3279e-01,  3.2013e-02,  2.1243e-04,  ..., -2.9346e-01,\n",
       "           4.3140e-01,  1.1359e-01]], dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.context_attn.final_linear.bias': tensor([-0.1670,  0.2517,  0.0768,  ..., -0.3137, -0.0340, -0.1135],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.layer_norm_2.weight': tensor([0.1266, 0.1838, 0.1929,  ..., 0.1061, 0.0986, 0.1010],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.layer_norm_2.bias': tensor([-0.0076, -0.0356, -0.0011,  ..., -0.0028, -0.0035, -0.0181],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.feed_forward.w_1.weight': tensor([[-0.1963, -0.0854,  0.0271,  ..., -0.0640, -0.1954,  0.2534],\n",
       "         [-0.3438,  0.1814, -0.1447,  ..., -0.3826, -0.0184, -0.3777],\n",
       "         [ 0.0417,  0.0775,  0.2869,  ...,  0.1899,  0.0681, -0.3806],\n",
       "         ...,\n",
       "         [-0.2186, -0.0697, -0.2954,  ..., -0.0013, -0.0532, -0.4646],\n",
       "         [-0.1605,  0.4458,  0.0753,  ...,  0.0030, -0.1979,  0.0666],\n",
       "         [ 0.1459,  0.1836,  0.0529,  ...,  0.0804, -0.0881,  0.0609]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.feed_forward.w_1.bias': tensor([-0.1088, -0.2505, -0.1223,  ..., -0.2303, -0.1230, -0.0989],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.feed_forward.w_2.weight': tensor([[-0.1610, -0.3735, -0.0555,  ...,  0.2761, -0.0305, -0.1532],\n",
       "         [-0.0477, -0.2759,  0.1680,  ...,  0.0407, -0.1489, -0.1984],\n",
       "         [-0.0375, -0.1848, -0.0933,  ...,  0.1871,  0.2727, -0.1232],\n",
       "         ...,\n",
       "         [-0.0792, -0.1058, -0.3218,  ...,  0.0746,  0.1451,  0.1779],\n",
       "         [-0.0492, -0.1247,  0.2181,  ...,  0.3079, -0.0853,  0.2299],\n",
       "         [ 0.4763,  0.1882, -0.0437,  ..., -0.0560, -0.4338, -0.0142]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.feed_forward.w_2.bias': tensor([-0.1108, -0.2070, -0.0164,  ..., -0.2815, -0.0843, -0.2876],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.feed_forward.layer_norm.weight': tensor([0.3970, 0.4524, 0.4319,  ..., 0.3435, 0.3228, 0.3274],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.2.feed_forward.layer_norm.bias': tensor([-0.0156,  0.0081,  0.0066,  ...,  0.0600, -0.0039,  0.0792],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.self_attn.linear_keys.weight': tensor([[-0.0989,  0.3494,  0.1242,  ..., -0.0166, -0.2554, -0.0117],\n",
       "         [-0.0774, -0.2800,  0.0182,  ..., -0.2566, -0.0605, -0.3606],\n",
       "         [ 0.1135, -0.0656,  0.0745,  ...,  0.1853, -0.1089, -0.1870],\n",
       "         ...,\n",
       "         [-0.1294, -0.1048, -0.0658,  ...,  0.0122, -0.0033, -0.1639],\n",
       "         [-0.0522,  0.1150, -0.0699,  ..., -0.0789,  0.0958,  0.0314],\n",
       "         [-0.1689,  0.0449,  0.0771,  ..., -0.0376, -0.2283, -0.1467]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.self_attn.linear_keys.bias': tensor([-0.0116, -0.0057, -0.0070,  ...,  0.0291, -0.0091, -0.0291],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.self_attn.linear_values.weight': tensor([[-0.1167, -0.2267, -0.1385,  ..., -0.1125,  0.1224, -0.4749],\n",
       "         [-0.1150, -0.0827, -0.2052,  ...,  0.1290, -0.1851,  0.3667],\n",
       "         [ 0.0052,  0.1235,  0.1295,  ..., -0.2764,  0.4204,  0.4370],\n",
       "         ...,\n",
       "         [ 0.3010, -0.2048, -0.3379,  ..., -0.2324,  0.0846,  0.2581],\n",
       "         [-0.4292, -0.1766,  0.1385,  ...,  0.1300,  0.1146,  0.2145],\n",
       "         [ 0.2388, -0.5005, -0.0915,  ..., -0.0520, -0.3730,  0.1678]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.self_attn.linear_values.bias': tensor([-0.0255,  0.0488,  0.0688,  ...,  0.0421, -0.0326,  0.0906],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.self_attn.linear_query.weight': tensor([[ 0.2737, -0.1266,  0.1499,  ...,  0.0596,  0.1345,  0.2371],\n",
       "         [-0.0415,  0.2070,  0.0330,  ..., -0.1792,  0.1012,  0.2859],\n",
       "         [-0.0612,  0.1016,  0.1646,  ..., -0.2700, -0.0338, -0.0159],\n",
       "         ...,\n",
       "         [ 0.1676, -0.0497,  0.4077,  ..., -0.0945,  0.0942, -0.0088],\n",
       "         [-0.1548, -0.1628, -0.3193,  ...,  0.2554, -0.0501,  0.0714],\n",
       "         [ 0.2166,  0.1407, -0.0704,  ...,  0.0569,  0.3152, -0.0768]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.self_attn.linear_query.bias': tensor([ 0.0457,  0.1059,  0.3831,  ...,  0.3333,  0.0374, -0.0630],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.self_attn.final_linear.weight': tensor([[-0.1353, -0.2498,  0.0072,  ...,  0.0026,  0.3059,  0.0142],\n",
       "         [ 0.0606, -0.2023,  0.1234,  ...,  0.5112, -0.1735,  0.4958],\n",
       "         [ 0.0301,  0.1292,  0.4243,  ..., -0.1786, -0.1874,  0.2764],\n",
       "         ...,\n",
       "         [ 0.1820,  0.1312,  0.3752,  ..., -0.4602,  0.6631, -0.1891],\n",
       "         [ 0.1621, -0.3711,  0.2749,  ...,  0.4641,  0.3682, -0.2791],\n",
       "         [-0.3870,  0.2444,  0.1794,  ..., -0.2311,  0.4253, -0.3052]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.self_attn.final_linear.bias': tensor([-0.3191, -0.0130, -0.3569,  ..., -0.4990, -0.2812,  0.3171],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.layer_norm_1.weight': tensor([0.2649, 0.2991, 0.3367,  ..., 0.1940, 0.2025, 0.2090],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.layer_norm_1.bias': tensor([ 0.0098,  0.0048,  0.0200,  ..., -0.0096,  0.0063, -0.0139],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.context_attn.linear_keys.weight': tensor([[-0.1865, -0.0974,  0.1832,  ..., -0.2744, -0.0507, -0.1809],\n",
       "         [ 0.3071, -0.1432,  0.2676,  ...,  0.0955, -0.1697,  0.0543],\n",
       "         [ 0.0473,  0.2034,  0.2032,  ..., -0.0693, -0.2028,  0.1306],\n",
       "         ...,\n",
       "         [-0.2595,  0.1549, -0.1284,  ..., -0.0772, -0.1320,  0.1205],\n",
       "         [-0.1119,  0.0875,  0.0643,  ...,  0.0091, -0.0217, -0.0189],\n",
       "         [ 0.0364,  0.2268,  0.0769,  ..., -0.1428,  0.0320,  0.0672]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.context_attn.linear_keys.bias': tensor([-0.0310,  0.0280,  0.0024,  ...,  0.0099, -0.0284, -0.0260],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.context_attn.linear_values.weight': tensor([[ 2.4078e-02,  2.6733e-01,  2.0782e-02,  ...,  1.9263e-01,\n",
       "           1.7105e-02,  2.2621e-03],\n",
       "         [ 1.1975e-01,  6.5063e-02, -1.6510e-02,  ..., -3.9642e-02,\n",
       "           1.5393e-01,  4.3671e-02],\n",
       "         [ 2.0248e-02,  1.1452e-02,  8.4961e-02,  ..., -2.9495e-02,\n",
       "          -1.4844e-01,  1.0025e-02],\n",
       "         ...,\n",
       "         [ 2.6025e-01,  3.3112e-02,  6.1096e-02,  ...,  1.0437e-01,\n",
       "           3.6987e-01, -4.9896e-03],\n",
       "         [-7.1680e-01, -2.0828e-02, -1.8994e-01,  ...,  4.0741e-02,\n",
       "           2.2595e-01,  1.6713e-04],\n",
       "         [ 2.0004e-02,  2.5366e-01,  2.1960e-01,  ..., -1.0065e-01,\n",
       "           6.7871e-02,  1.7929e-02]], dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.context_attn.linear_values.bias': tensor([-0.0029,  0.0220, -0.1296,  ...,  0.0403,  0.0458,  0.0118],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.context_attn.linear_query.weight': tensor([[-0.0173,  0.0173,  0.1548,  ...,  0.4448,  0.2622, -0.3184],\n",
       "         [ 0.0507,  0.5000,  0.1766,  ...,  0.2209, -0.3264, -0.1075],\n",
       "         [ 0.0124, -0.0648,  0.1494,  ..., -0.2532,  0.1809, -0.1466],\n",
       "         ...,\n",
       "         [ 0.2498, -0.0932, -0.2915,  ..., -0.2399, -0.0685,  0.0255],\n",
       "         [ 0.0212,  0.2756,  0.0865,  ...,  0.0206, -0.1403,  0.0765],\n",
       "         [ 0.0407,  0.2849, -0.2264,  ..., -0.1996,  0.0529,  0.0208]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.context_attn.linear_query.bias': tensor([-0.1206,  0.1025,  0.0757,  ..., -0.0092, -0.0883, -0.1151],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.context_attn.final_linear.weight': tensor([[ 0.0876, -0.1027,  0.1193,  ...,  0.2145,  0.0585, -0.1265],\n",
       "         [ 0.0244,  0.0413, -0.1685,  ...,  0.0888,  0.1541, -0.0012],\n",
       "         [-0.1416, -0.0668, -0.3159,  ..., -0.0815,  0.0106, -0.0503],\n",
       "         ...,\n",
       "         [ 0.0349, -0.1460, -0.0229,  ...,  0.1868, -0.0360,  0.0837],\n",
       "         [ 0.1722,  0.0825, -0.0028,  ...,  0.3721,  0.1023,  0.1249],\n",
       "         [-0.0881,  0.0850, -0.1405,  ..., -0.2732,  0.2292, -0.1927]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.context_attn.final_linear.bias': tensor([ 0.1139,  0.2313,  0.3328,  ...,  0.1598,  0.1838, -0.0592],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.layer_norm_2.weight': tensor([0.1249, 0.1664, 0.1544,  ..., 0.0951, 0.1037, 0.1024],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.layer_norm_2.bias': tensor([-0.0096, -0.0279, -0.0147,  ..., -0.0196,  0.0010, -0.0201],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.feed_forward.w_1.weight': tensor([[ 0.2854, -0.1564,  0.2632,  ...,  0.3538,  0.1858, -0.2240],\n",
       "         [ 0.1320, -0.0382,  0.1373,  ...,  0.0072, -0.1225,  0.2688],\n",
       "         [-0.0849, -0.1731,  0.4026,  ..., -0.0120,  0.2983, -0.0650],\n",
       "         ...,\n",
       "         [-0.2664, -0.1324,  0.2439,  ...,  0.1598,  0.0373,  0.1060],\n",
       "         [-0.0652,  0.0218,  0.0721,  ..., -0.0628, -0.5562, -0.1991],\n",
       "         [ 0.0721,  0.0939, -0.2037,  ..., -0.0305, -0.1075, -0.0715]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.feed_forward.w_1.bias': tensor([-0.1403, -0.0063, -0.0494,  ...,  0.0057, -0.2385, -0.1196],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.feed_forward.w_2.weight': tensor([[-3.0322e-01, -6.6833e-02,  1.0114e-01,  ..., -5.2452e-04,\n",
       "           4.5197e-02,  2.4658e-02],\n",
       "         [ 7.6111e-02, -1.4819e-01,  2.0898e-01,  ..., -1.3293e-01,\n",
       "           1.6663e-02,  5.0812e-02],\n",
       "         [ 1.4941e-01, -1.0266e-01, -1.2354e-01,  ..., -6.5327e-04,\n",
       "          -8.2458e-02, -1.0616e-04],\n",
       "         ...,\n",
       "         [ 8.2153e-02,  1.2646e-01, -1.3855e-01,  ...,  7.0679e-02,\n",
       "          -1.0352e-01, -8.3435e-02],\n",
       "         [-2.7199e-03,  3.4009e-01, -2.1057e-01,  ..., -4.7485e-01,\n",
       "           7.7454e-02, -3.9380e-01],\n",
       "         [ 2.1704e-01,  3.9139e-03,  8.0872e-02,  ..., -1.1017e-01,\n",
       "          -1.6699e-01, -2.3010e-02]], dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.feed_forward.w_2.bias': tensor([-0.1926, -0.1788,  0.1004,  ..., -0.1218,  0.0177, -0.1982],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.feed_forward.layer_norm.weight': tensor([0.4517, 0.4846, 0.4919,  ..., 0.3994, 0.3762, 0.3960],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.3.feed_forward.layer_norm.bias': tensor([0.0141, 0.0581, 0.0347,  ..., 0.0001, 0.0165, 0.0414],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.self_attn.linear_keys.weight': tensor([[ 0.2207,  0.1823, -0.0239,  ..., -0.0892, -0.2250, -0.0438],\n",
       "         [-0.1675,  0.0461, -0.2908,  ..., -0.0573,  0.2861, -0.1185],\n",
       "         [-0.2162,  0.2639,  0.0948,  ...,  0.0665,  0.1389,  0.2520],\n",
       "         ...,\n",
       "         [-0.0280, -0.0919,  0.0092,  ...,  0.2295, -0.2079,  0.0559],\n",
       "         [ 0.1720,  0.1978, -0.1338,  ..., -0.3010,  0.5542,  0.1371],\n",
       "         [ 0.1345,  0.1317, -0.1661,  ...,  0.0614, -0.0829, -0.2491]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.self_attn.linear_keys.bias': tensor([ 0.0078,  0.0124, -0.0312,  ..., -0.0085, -0.0288, -0.0269],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.self_attn.linear_values.weight': tensor([[-0.2673,  0.0440,  0.2004,  ...,  0.0255, -0.0587,  0.0551],\n",
       "         [ 0.0173, -0.0415, -0.1914,  ...,  0.2194, -0.2268,  0.0632],\n",
       "         [-0.3674, -0.1038,  0.1384,  ...,  0.1515,  0.1569, -0.2007],\n",
       "         ...,\n",
       "         [ 0.0292,  0.3396, -0.3240,  ...,  0.2029,  0.3286,  0.3315],\n",
       "         [ 0.1886, -0.2532,  0.0946,  ..., -0.0008,  0.2737,  0.4102],\n",
       "         [ 0.4709, -0.1427, -0.1335,  ..., -0.3120, -0.1081,  0.0392]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.self_attn.linear_values.bias': tensor([ 0.0049,  0.0786,  0.0156,  ..., -0.0225, -0.0117, -0.1213],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.self_attn.linear_query.weight': tensor([[-0.1050, -0.0892, -0.0248,  ..., -0.2117,  0.2351, -0.1338],\n",
       "         [-0.0142, -0.0568, -0.2524,  ...,  0.1010, -0.2367,  0.1089],\n",
       "         [ 0.0411, -0.0777,  0.0359,  ..., -0.0947,  0.0923,  0.1312],\n",
       "         ...,\n",
       "         [-0.0166,  0.0511, -0.0418,  ...,  0.2347,  0.0373,  0.0219],\n",
       "         [-0.3071, -0.1300,  0.0977,  ..., -0.3276, -0.2328, -0.2103],\n",
       "         [-0.0814, -0.2233,  0.1093,  ..., -0.1879, -0.1793, -0.1509]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.self_attn.linear_query.bias': tensor([-0.1097, -0.2244,  0.1980,  ..., -0.0424,  0.0051, -0.0490],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.self_attn.final_linear.weight': tensor([[-0.2305, -0.0163,  0.1232,  ...,  0.3679,  0.2944, -0.0201],\n",
       "         [ 0.2825, -0.3196,  0.0299,  ...,  0.3411,  0.2332, -0.2466],\n",
       "         [ 0.1153, -0.1301, -0.1711,  ...,  0.0217, -0.2856, -0.2861],\n",
       "         ...,\n",
       "         [-0.2355,  0.1714,  0.0613,  ..., -0.0398, -0.2347,  0.0804],\n",
       "         [ 0.1771, -0.1948,  0.1324,  ...,  0.2856, -0.0511, -0.0304],\n",
       "         [ 0.0372,  0.3025, -0.4363,  ...,  0.2578, -0.0452, -0.1451]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.self_attn.final_linear.bias': tensor([-0.2571, -0.0980, -0.3325,  ..., -0.5020, -0.4917,  0.4910],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.layer_norm_1.weight': tensor([0.2859, 0.3113, 0.3257,  ..., 0.2101, 0.2292, 0.2230],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.layer_norm_1.bias': tensor([ 0.0132,  0.0039,  0.0213,  ..., -0.0128,  0.0103, -0.0153],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.context_attn.linear_keys.weight': tensor([[-0.3931,  0.0401,  0.1293,  ..., -0.1195, -0.0222,  0.0602],\n",
       "         [ 0.2490, -0.2781, -0.0809,  ..., -0.0480,  0.1019, -0.0571],\n",
       "         [-0.1403,  0.0654, -0.0033,  ...,  0.0448,  0.0354,  0.1317],\n",
       "         ...,\n",
       "         [-0.1971, -0.1921, -0.2170,  ...,  0.2629,  0.1705, -0.0448],\n",
       "         [ 0.0312, -0.1581,  0.1361,  ..., -0.0624, -0.0419,  0.0023],\n",
       "         [-0.0662,  0.0148, -0.1705,  ..., -0.0999, -0.0140, -0.0345]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.context_attn.linear_keys.bias': tensor([ 0.0095,  0.0270,  0.0019,  ...,  0.0180, -0.0116,  0.0144],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.context_attn.linear_values.weight': tensor([[ 0.0465,  0.1771, -0.3411,  ..., -0.0360, -0.1864,  0.0047],\n",
       "         [ 0.0253, -0.0935,  0.0439,  ...,  0.2808,  0.1083, -0.0020],\n",
       "         [ 0.0543, -0.3330, -0.0420,  ..., -0.2228,  0.1534,  0.0454],\n",
       "         ...,\n",
       "         [-0.1678, -0.2131, -0.0310,  ..., -0.1007,  0.1016, -0.0550],\n",
       "         [-0.1109,  0.0102,  0.1556,  ...,  0.1377, -0.0242,  0.0895],\n",
       "         [ 0.0603,  0.3479,  0.2795,  ...,  0.0861, -0.2308, -0.0095]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.context_attn.linear_values.bias': tensor([-0.0964, -0.0339,  0.0177,  ..., -0.0941, -0.0497, -0.0724],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.context_attn.linear_query.weight': tensor([[-0.1450, -0.3315, -0.1952,  ...,  0.0352,  0.0111,  0.0752],\n",
       "         [-0.0774, -0.0184, -0.0053,  ...,  0.1453,  0.0826,  0.3909],\n",
       "         [-0.2399, -0.0374, -0.0986,  ..., -0.2700,  0.3972,  0.3582],\n",
       "         ...,\n",
       "         [-0.0827, -0.0493, -0.0366,  ...,  0.0909, -0.2639,  0.0032],\n",
       "         [ 0.1049,  0.1342,  0.3726,  ..., -0.0033,  0.0166,  0.1332],\n",
       "         [-0.2563, -0.3057, -0.1438,  ..., -0.1454, -0.0888, -0.0361]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.context_attn.linear_query.bias': tensor([ 0.0404,  0.1339, -0.0085,  ...,  0.1522,  0.0819,  0.0841],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.context_attn.final_linear.weight': tensor([[ 0.0031, -0.0746,  0.2087,  ..., -0.1304, -0.2308, -0.2993],\n",
       "         [-0.0189, -0.2008, -0.2449,  ..., -0.1711, -0.0311,  0.1272],\n",
       "         [-0.2607,  0.1437, -0.4104,  ...,  0.0939, -0.0751,  0.2810],\n",
       "         ...,\n",
       "         [ 0.0663, -0.4783,  0.0988,  ..., -0.0980,  0.1279, -0.1688],\n",
       "         [-0.1671, -0.1387,  0.1354,  ..., -0.2666, -0.3020, -0.0603],\n",
       "         [ 0.0801, -0.0495,  0.4460,  ...,  0.1710, -0.0508, -0.0991]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.context_attn.final_linear.bias': tensor([ 0.1298,  0.2185,  0.3086,  ...,  0.0321, -0.1702, -0.3853],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.layer_norm_2.weight': tensor([0.1193, 0.1481, 0.1348,  ..., 0.0941, 0.1001, 0.0947],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.layer_norm_2.bias': tensor([-0.0073, -0.0116, -0.0107,  ..., -0.0408, -0.0026, -0.0193],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.feed_forward.w_1.weight': tensor([[-0.4097, -0.2700,  0.0737,  ...,  0.0936,  0.0958,  0.2720],\n",
       "         [ 0.0781, -0.1074,  0.0117,  ...,  0.2820,  0.2788, -0.1580],\n",
       "         [-0.2754, -0.2556,  0.0128,  ...,  0.2820,  0.0628,  0.0275],\n",
       "         ...,\n",
       "         [ 0.1747,  0.2019,  0.3281,  ...,  0.3142,  0.0674,  0.1089],\n",
       "         [-0.1899, -0.1687,  0.3840,  ..., -0.1477, -0.3643,  0.2465],\n",
       "         [ 0.4243,  0.2018, -0.3101,  ...,  0.2864, -0.1592,  0.0157]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.feed_forward.w_1.bias': tensor([-0.2307,  0.0841, -0.3652,  ..., -0.2961, -0.1600, -0.1461],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.feed_forward.w_2.weight': tensor([[-0.0831, -0.1288,  0.0689,  ..., -0.2583, -0.0450,  0.1154],\n",
       "         [-0.3525, -0.0558, -0.0667,  ..., -0.2041,  0.1604, -0.0671],\n",
       "         [-0.1544, -0.0462, -0.3406,  ...,  0.0112,  0.2053,  0.0386],\n",
       "         ...,\n",
       "         [ 0.2124, -0.1754, -0.0749,  ...,  0.2812,  0.3555,  0.1595],\n",
       "         [-0.0262, -0.1567,  0.1155,  ...,  0.2015, -0.0365,  0.1342],\n",
       "         [ 0.1613,  0.0448,  0.0573,  ...,  0.3638, -0.1046,  0.0436]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.feed_forward.w_2.bias': tensor([-0.1475, -0.1466,  0.2227,  ..., -0.0585,  0.1493,  0.0556],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.feed_forward.layer_norm.weight': tensor([0.5186, 0.5400, 0.5186,  ..., 0.4551, 0.4565, 0.4644],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.4.feed_forward.layer_norm.bias': tensor([ 0.0312,  0.1107,  0.0245,  ..., -0.1113, -0.0250,  0.0216],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.self_attn.linear_keys.weight': tensor([[-6.5857e-02,  2.0251e-01, -5.7465e-02,  ...,  4.4647e-02,\n",
       "          -1.0632e-01,  7.2449e-02],\n",
       "         [ 1.3733e-02, -1.0150e-01, -2.9434e-02,  ...,  1.1426e-01,\n",
       "           1.4832e-01,  8.0338e-03],\n",
       "         [ 1.1176e-01, -4.4739e-02, -8.9355e-02,  ..., -5.9605e-06,\n",
       "          -1.0858e-01, -2.6672e-02],\n",
       "         ...,\n",
       "         [-2.2217e-01, -1.2128e-01, -5.0537e-02,  ...,  1.6589e-01,\n",
       "          -1.8567e-01, -5.2338e-02],\n",
       "         [ 5.4565e-02,  7.6027e-03, -1.1421e-02,  ..., -1.1682e-01,\n",
       "           5.4169e-02,  2.6636e-01],\n",
       "         [-1.3696e-01, -3.6450e-01,  2.5366e-01,  ..., -3.2410e-02,\n",
       "          -1.6187e-01,  9.6558e-02]], dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.self_attn.linear_keys.bias': tensor([-0.0155,  0.0081, -0.0183,  ..., -0.0101, -0.0080,  0.0053],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.self_attn.linear_values.weight': tensor([[ 0.3101, -0.0659, -0.4985,  ..., -0.0261,  0.1904, -0.0820],\n",
       "         [ 0.1340, -0.1665, -0.0553,  ..., -0.0634,  0.3303, -0.4929],\n",
       "         [-0.5005, -0.4053,  0.0614,  ...,  0.1809, -0.4783,  0.2988],\n",
       "         ...,\n",
       "         [-0.0119, -0.3416, -0.1237,  ...,  0.0163, -0.1481, -0.0235],\n",
       "         [-0.2915,  0.0356, -0.1187,  ..., -0.1050, -0.2200,  0.3716],\n",
       "         [-0.2551,  0.0391,  0.1863,  ..., -0.0909, -0.0381,  0.0756]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.self_attn.linear_values.bias': tensor([-0.0251, -0.0740,  0.1063,  ..., -0.0436,  0.0135,  0.0956],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.self_attn.linear_query.weight': tensor([[-0.0420, -0.3079, -0.0490,  ...,  0.1382,  0.0266,  0.1229],\n",
       "         [-0.0640,  0.0458,  0.1144,  ...,  0.1064, -0.1210, -0.0217],\n",
       "         [ 0.1219, -0.0167,  0.0353,  ..., -0.0523, -0.1027,  0.1458],\n",
       "         ...,\n",
       "         [ 0.3435, -0.0014,  0.2062,  ...,  0.0873, -0.1273, -0.0268],\n",
       "         [-0.1941,  0.1359, -0.2434,  ...,  0.2717,  0.1084,  0.2108],\n",
       "         [-0.1007,  0.1511, -0.1353,  ...,  0.1998, -0.2651, -0.3638]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.self_attn.linear_query.bias': tensor([-0.0583, -0.0863,  0.0507,  ..., -0.0739,  0.1298,  0.1917],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.self_attn.final_linear.weight': tensor([[ 0.2006, -0.1216,  0.1897,  ...,  0.0042,  0.0221, -0.0220],\n",
       "         [-0.2788,  0.2749,  0.3147,  ..., -0.3538, -0.1953,  0.1487],\n",
       "         [-0.1451,  0.3015, -0.4993,  ..., -0.1499,  0.0264, -0.0385],\n",
       "         ...,\n",
       "         [-0.3020,  0.2366, -0.0396,  ..., -0.0085, -0.3369, -0.0049],\n",
       "         [ 0.2009, -0.1084, -0.2537,  ..., -0.3242, -0.2581,  0.1196],\n",
       "         [ 0.4978, -0.2510,  0.3047,  ..., -0.0637,  0.0760, -0.1024]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.self_attn.final_linear.bias': tensor([-0.2795,  0.4678, -0.2693,  ..., -0.4160, -0.0284,  0.2915],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.layer_norm_1.weight': tensor([0.2969, 0.3320, 0.3101,  ..., 0.2297, 0.2537, 0.2499],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.layer_norm_1.bias': tensor([ 0.0145,  0.0063,  0.0268,  ..., -0.0158,  0.0065, -0.0284],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.context_attn.linear_keys.weight': tensor([[ 0.0375, -0.0067,  0.0670,  ..., -0.3262, -0.0666,  0.0624],\n",
       "         [-0.0308,  0.0286, -0.1774,  ..., -0.1131, -0.0634,  0.1097],\n",
       "         [-0.0727,  0.0724, -0.2861,  ...,  0.2018,  0.0775, -0.0070],\n",
       "         ...,\n",
       "         [-0.1735, -0.0476, -0.1813,  ..., -0.2632, -0.2131, -0.0457],\n",
       "         [ 0.2214, -0.1711,  0.0067,  ..., -0.3740,  0.2871, -0.0119],\n",
       "         [-0.2520, -0.1650,  0.0536,  ..., -0.2666, -0.4377,  0.1949]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.context_attn.linear_keys.bias': tensor([ 0.0144, -0.0133,  0.0258,  ...,  0.0226, -0.0057, -0.0240],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.context_attn.linear_values.weight': tensor([[ 0.1630, -0.0317,  0.1660,  ..., -0.4785, -0.2400, -0.0245],\n",
       "         [ 0.0836, -0.0155,  0.2700,  ...,  0.0586, -0.1124,  0.0049],\n",
       "         [-0.0585,  0.0667, -0.0399,  ..., -0.2175, -0.0422, -0.0768],\n",
       "         ...,\n",
       "         [ 0.2810,  0.2720, -0.0113,  ..., -0.2407, -0.2625,  0.0165],\n",
       "         [ 0.1631,  0.1704,  0.0071,  ..., -0.2157, -0.0864,  0.0110],\n",
       "         [ 0.0294,  0.1296,  0.1190,  ...,  0.1326, -0.0536,  0.0583]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.context_attn.linear_values.bias': tensor([ 0.0158, -0.1880,  0.0825,  ..., -0.0160, -0.0419, -0.0264],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.context_attn.linear_query.weight': tensor([[ 0.0022,  0.1277, -0.0131,  ...,  0.0833, -0.0323, -0.1841],\n",
       "         [ 0.1743,  0.0327, -0.2566,  ..., -0.0372, -0.0149,  0.1061],\n",
       "         [ 0.2786,  0.0632,  0.1578,  ...,  0.1000,  0.0400,  0.3135],\n",
       "         ...,\n",
       "         [-0.1490, -0.1283, -0.1852,  ...,  0.0140, -0.1982,  0.0712],\n",
       "         [-0.0038,  0.1048, -0.1748,  ...,  0.1429,  0.2014, -0.0426],\n",
       "         [ 0.2123,  0.0413, -0.0383,  ...,  0.0410, -0.1842,  0.3259]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.context_attn.linear_query.bias': tensor([-0.0406, -0.0256,  0.0312,  ..., -0.0875,  0.0524,  0.0250],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.context_attn.final_linear.weight': tensor([[ 0.0500, -0.2297, -0.2686,  ...,  0.1371, -0.0784,  0.0501],\n",
       "         [ 0.3943,  0.1156,  0.1155,  ...,  0.1086, -0.0457,  0.1461],\n",
       "         [-0.0682, -0.1912, -0.1614,  ...,  0.2607,  0.1146, -0.2332],\n",
       "         ...,\n",
       "         [-0.2773, -0.2771,  0.3865,  ..., -0.0358,  0.0470, -0.2032],\n",
       "         [-0.0219,  0.0157, -0.1818,  ..., -0.1792, -0.1313,  0.1989],\n",
       "         [ 0.0125,  0.2400, -0.2211,  ..., -0.0269, -0.1218, -0.4712]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.context_attn.final_linear.bias': tensor([ 0.1956,  0.0657,  0.3936,  ...,  0.0127, -0.3669, -0.3601],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.layer_norm_2.weight': tensor([0.1075, 0.1220, 0.1219,  ..., 0.0916, 0.0882, 0.0904],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.layer_norm_2.bias': tensor([-0.0060, -0.0189, -0.0099,  ..., -0.0571, -0.0138, -0.0278],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.feed_forward.w_1.weight': tensor([[ 0.0246, -0.1316,  0.0693,  ...,  0.0710,  0.0610, -0.1390],\n",
       "         [-0.0105,  0.1313, -0.1672,  ...,  0.0849,  0.3064, -0.0118],\n",
       "         [-0.0682,  0.0808,  0.1024,  ..., -0.0484,  0.1665,  0.0039],\n",
       "         ...,\n",
       "         [ 0.3459,  0.1885,  0.3340,  ...,  0.1039, -0.0782, -0.0128],\n",
       "         [ 0.0859, -0.0403,  0.0482,  ..., -0.0431,  0.1514,  0.0355],\n",
       "         [ 0.1876,  0.1115, -0.5215,  ..., -0.3174, -0.3076,  0.0912]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.feed_forward.w_1.bias': tensor([-0.1182, -0.0240, -0.1111,  ..., -0.0903,  0.0151, -0.2751],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.feed_forward.w_2.weight': tensor([[-0.0988, -0.2712,  0.0759,  ..., -0.0606,  0.0951,  0.0548],\n",
       "         [-0.0896,  0.0380, -0.1500,  ...,  0.1359,  0.1032, -0.2303],\n",
       "         [-0.2039,  0.2030, -0.0993,  ...,  0.0145, -0.0025,  0.0596],\n",
       "         ...,\n",
       "         [-0.2715,  0.0426,  0.1118,  ..., -0.2710,  0.0401,  0.3145],\n",
       "         [ 0.3484, -0.2305, -0.2507,  ...,  0.0446, -0.0930,  0.0582],\n",
       "         [ 0.0522,  0.2581,  0.1652,  ...,  0.1074, -0.0864,  0.0522]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.feed_forward.w_2.bias': tensor([-0.0361,  0.0535,  0.2190,  ..., -0.0262,  0.0795, -0.0141],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.feed_forward.layer_norm.weight': tensor([0.5527, 0.5947, 0.5669,  ..., 0.5171, 0.5190, 0.5498],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.5.feed_forward.layer_norm.bias': tensor([-0.0238,  0.1709,  0.0458,  ..., -0.1510, -0.0305, -0.0568],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.self_attn.linear_keys.weight': tensor([[ 0.0374,  0.2539,  0.0807,  ..., -0.2534, -0.1730,  0.1367],\n",
       "         [ 0.1406,  0.1671, -0.1053,  ..., -0.0083, -0.1061,  0.0356],\n",
       "         [ 0.1313,  0.2104,  0.0596,  ..., -0.2021,  0.1768,  0.1305],\n",
       "         ...,\n",
       "         [-0.0445,  0.3679,  0.3291,  ..., -0.1658,  0.0309,  0.0033],\n",
       "         [ 0.1257,  0.0351,  0.0739,  ..., -0.0812, -0.0751,  0.0815],\n",
       "         [-0.3774, -0.2363, -0.0306,  ...,  0.2869, -0.1887,  0.2236]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.self_attn.linear_keys.bias': tensor([-0.0245,  0.0265,  0.0033,  ...,  0.0219, -0.0003, -0.0004],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.self_attn.linear_values.weight': tensor([[ 0.1777,  0.1213, -0.0913,  ..., -0.0241, -0.2084,  0.0174],\n",
       "         [ 0.5625,  0.0407,  0.4060,  ...,  0.0525, -0.2460, -0.0887],\n",
       "         [-0.0046, -0.1355,  0.1956,  ..., -0.2605,  0.1372, -0.1455],\n",
       "         ...,\n",
       "         [ 0.3633, -0.1598,  0.4990,  ...,  0.0370, -0.4541,  0.3074],\n",
       "         [-0.6509,  0.4268,  0.5879,  ...,  0.2170, -0.2910,  0.5000],\n",
       "         [ 0.0174, -0.2291,  0.1272,  ...,  0.3635,  0.0128, -0.2561]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.self_attn.linear_values.bias': tensor([-0.0603,  0.0184,  0.1882,  ..., -0.0049,  0.0327, -0.1523],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.self_attn.linear_query.weight': tensor([[-0.0482, -0.3528, -0.1902,  ...,  0.0988,  0.0460,  0.0452],\n",
       "         [-0.0535, -0.1464,  0.1217,  ..., -0.1593, -0.1014,  0.0892],\n",
       "         [-0.0216, -0.0822,  0.0185,  ..., -0.0079,  0.3145,  0.1541],\n",
       "         ...,\n",
       "         [-0.1473,  0.0253, -0.0071,  ..., -0.0023, -0.0278, -0.0009],\n",
       "         [ 0.3037, -0.0680,  0.0512,  ..., -0.0131, -0.1829, -0.1847],\n",
       "         [-0.0150, -0.1372, -0.0367,  ...,  0.0043, -0.1564,  0.1874]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.self_attn.linear_query.bias': tensor([-0.1130, -0.0684, -0.0753,  ..., -0.3828, -0.1595,  0.0504],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.self_attn.final_linear.weight': tensor([[ 0.1996,  0.2776,  0.0723,  ...,  0.6182, -0.4133, -0.1593],\n",
       "         [-0.1759,  0.1315,  0.0181,  ...,  0.3464,  0.4951, -0.2612],\n",
       "         [-0.2367,  0.1771,  0.1803,  ..., -0.5269, -0.2527, -0.2202],\n",
       "         ...,\n",
       "         [ 0.0767, -0.0532, -0.1245,  ...,  0.1116,  0.5005, -0.0312],\n",
       "         [-0.0836,  0.0947,  0.0076,  ...,  0.2340, -0.0242, -0.0476],\n",
       "         [ 0.0997,  0.3879, -0.1774,  ...,  0.1447,  0.4998,  0.0815]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.self_attn.final_linear.bias': tensor([-0.2966,  0.2942, -0.3508,  ..., -0.3872,  0.0909,  0.3867],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.layer_norm_1.weight': tensor([0.3369, 0.3384, 0.3252,  ..., 0.2742, 0.3064, 0.2991],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.layer_norm_1.bias': tensor([ 0.0210,  0.0098,  0.0302,  ..., -0.0051,  0.0017, -0.0262],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.context_attn.linear_keys.weight': tensor([[ 0.3401,  0.1907,  0.0227,  ...,  0.1218, -0.2205, -0.1613],\n",
       "         [-0.1348,  0.0866,  0.0611,  ...,  0.0736,  0.0831, -0.0015],\n",
       "         [-0.2825,  0.1018,  0.0981,  ..., -0.0231,  0.2522, -0.1433],\n",
       "         ...,\n",
       "         [-0.0963,  0.3123, -0.3767,  ...,  0.2382, -0.0230,  0.0461],\n",
       "         [ 0.3325, -0.4280, -0.1127,  ..., -0.1572,  0.1757,  0.0476],\n",
       "         [ 0.1704,  0.1230, -0.1737,  ..., -0.1663, -0.2683, -0.0545]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.context_attn.linear_keys.bias': tensor([-0.0267, -0.0164,  0.0116,  ...,  0.0257, -0.0025,  0.0022],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.context_attn.linear_values.weight': tensor([[ 1.6235e-01, -8.6121e-02, -4.5074e-02,  ...,  1.9666e-01,\n",
       "          -3.8672e-01,  8.2092e-03],\n",
       "         [-2.8613e-01,  3.4271e-02, -4.3677e-01,  ..., -5.0964e-02,\n",
       "          -6.2012e-02,  7.8857e-02],\n",
       "         [-2.7466e-01, -2.5659e-01,  4.3921e-01,  ..., -4.0820e-01,\n",
       "          -3.2642e-01,  3.2349e-02],\n",
       "         ...,\n",
       "         [-4.6753e-02,  2.1582e-01,  1.4502e-01,  ...,  6.9336e-02,\n",
       "           6.7322e-02, -1.3863e-02],\n",
       "         [-7.0251e-02, -2.0645e-02, -1.1169e-01,  ...,  1.6626e-01,\n",
       "           8.9355e-02,  2.6276e-02],\n",
       "         [ 5.1416e-01, -5.3525e-05,  3.3600e-02,  ...,  2.0093e-01,\n",
       "           2.9614e-01,  3.6316e-02]], dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.context_attn.linear_values.bias': tensor([-0.0952, -0.1188, -0.1442,  ..., -0.0037,  0.0580,  0.0503],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.context_attn.linear_query.weight': tensor([[ 0.2246, -0.0369, -0.0668,  ..., -0.2939, -0.1271,  0.0820],\n",
       "         [-0.2778, -0.0588,  0.1204,  ..., -0.2460,  0.1610, -0.0326],\n",
       "         [ 0.1885,  0.0735,  0.1874,  ..., -0.1624,  0.1464, -0.0825],\n",
       "         ...,\n",
       "         [-0.1677,  0.0193, -0.3943,  ...,  0.0600, -0.1323, -0.0068],\n",
       "         [-0.2080, -0.0361,  0.1622,  ..., -0.1429,  0.1573, -0.1770],\n",
       "         [-0.0502,  0.0901,  0.1974,  ...,  0.0373,  0.0597,  0.1471]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.context_attn.linear_query.bias': tensor([-0.0816, -0.0087, -0.1379,  ...,  0.0420,  0.0195, -0.1461],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.context_attn.final_linear.weight': tensor([[-0.0215, -0.2637, -0.1755,  ..., -0.4727, -0.0643,  0.0902],\n",
       "         [ 0.0960,  0.2106,  0.0665,  ...,  0.1514, -0.2832, -0.0386],\n",
       "         [ 0.2603, -0.0852,  0.4165,  ..., -0.0095, -0.2157, -0.1486],\n",
       "         ...,\n",
       "         [-0.0221,  0.1790, -0.2588,  ..., -0.1587, -0.1099, -0.0893],\n",
       "         [-0.1489,  0.0752,  0.2411,  ...,  0.0696, -0.1183, -0.0610],\n",
       "         [-0.1570, -0.2546, -0.0327,  ...,  0.0521, -0.1205,  0.4062]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.context_attn.final_linear.bias': tensor([ 0.4001,  0.2559,  0.4946,  ...,  0.2712, -0.3901, -0.4368],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.layer_norm_2.weight': tensor([0.1088, 0.1266, 0.1223,  ..., 0.0982, 0.0980, 0.0988],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.layer_norm_2.bias': tensor([-0.0053, -0.0124, -0.0187,  ..., -0.0555, -0.0078, -0.0295],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.feed_forward.w_1.weight': tensor([[-0.0851,  0.0754,  0.1345,  ...,  0.3159,  0.1464,  0.0422],\n",
       "         [ 0.2727,  0.1659, -0.0242,  ..., -0.1598, -0.0930,  0.0657],\n",
       "         [ 0.1584, -0.0405, -0.1112,  ..., -0.0997, -0.1469, -0.2573],\n",
       "         ...,\n",
       "         [ 0.2394, -0.2903, -0.0901,  ...,  0.3789,  0.2290, -0.3430],\n",
       "         [ 0.0126,  0.0057, -0.0182,  ...,  0.0438, -0.3665, -0.0600],\n",
       "         [-0.3196,  0.0820,  0.0243,  ...,  0.1395, -0.0446, -0.4631]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.feed_forward.w_1.bias': tensor([-0.1229, -0.1085, -0.0858,  ..., -0.3970, -0.1575,  0.0030],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.feed_forward.w_2.weight': tensor([[-0.1260, -0.0411, -0.0211,  ..., -0.1797,  0.1743,  0.2720],\n",
       "         [-0.2678,  0.2886,  0.2220,  ...,  0.0752,  0.0397,  0.1187],\n",
       "         [-0.2213, -0.2236, -0.0878,  ..., -0.0323, -0.0224, -0.0335],\n",
       "         ...,\n",
       "         [-0.1696,  0.0558,  0.2617,  ..., -0.2012, -0.1853, -0.0159],\n",
       "         [-0.1678, -0.0100, -0.1174,  ..., -0.1779,  0.0478,  0.1150],\n",
       "         [ 0.1125, -0.0822,  0.0068,  ..., -0.2668, -0.2499,  0.3943]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.feed_forward.w_2.bias': tensor([ 0.0576, -0.2017,  0.3735,  ..., -0.0267,  0.5044,  0.0363],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.feed_forward.layer_norm.weight': tensor([0.6333, 0.6772, 0.6577,  ..., 0.6431, 0.6191, 0.6387],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.6.feed_forward.layer_norm.bias': tensor([-0.0312,  0.2100,  0.0613,  ..., -0.2198, -0.0563,  0.0056],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.self_attn.linear_keys.weight': tensor([[-0.2045,  0.0935, -0.1895,  ..., -0.0121, -0.1221, -0.0599],\n",
       "         [-0.0090, -0.3213, -0.0044,  ..., -0.0103, -0.0870,  0.2140],\n",
       "         [-0.0508, -0.1552, -0.1685,  ..., -0.0257, -0.1137, -0.3550],\n",
       "         ...,\n",
       "         [ 0.0738,  0.0740, -0.0399,  ...,  0.0630,  0.0160, -0.0910],\n",
       "         [ 0.0360, -0.0048, -0.0846,  ..., -0.0117,  0.0858, -0.0630],\n",
       "         [-0.0284,  0.0037, -0.2744,  ..., -0.1581, -0.0492, -0.3699]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.self_attn.linear_keys.bias': tensor([ 0.0037,  0.0206,  0.0255,  ..., -0.0012, -0.0041, -0.0044],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.self_attn.linear_values.weight': tensor([[-0.0089,  0.4580,  0.1262,  ...,  0.0078,  0.3901, -0.1656],\n",
       "         [ 0.1880, -0.0881,  0.2271,  ..., -0.1149, -0.0731, -0.2908],\n",
       "         [-0.1429, -0.2030, -0.2288,  ..., -0.1512,  0.1527, -0.1395],\n",
       "         ...,\n",
       "         [ 0.1218,  0.2458, -0.0825,  ...,  0.2137,  0.2061, -0.0891],\n",
       "         [ 0.1573,  0.2622, -0.0113,  ...,  0.0221, -0.0740,  0.0135],\n",
       "         [-0.1108, -0.0303,  0.2396,  ..., -0.0332, -0.0999,  0.1989]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.self_attn.linear_values.bias': tensor([-0.0156,  0.0625,  0.0502,  ..., -0.3411,  0.2294,  0.1083],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.self_attn.linear_query.weight': tensor([[-0.1903, -0.0818,  0.0698,  ..., -0.1096, -0.1135, -0.2006],\n",
       "         [-0.1033,  0.3027,  0.1921,  ...,  0.1516, -0.0990,  0.0273],\n",
       "         [ 0.1155,  0.1646, -0.0541,  ..., -0.2522,  0.0373, -0.0517],\n",
       "         ...,\n",
       "         [-0.1730, -0.1442,  0.3608,  ...,  0.0540, -0.0877,  0.0861],\n",
       "         [ 0.1680,  0.1840, -0.1689,  ...,  0.0338,  0.0041,  0.0251],\n",
       "         [ 0.0446,  0.1721, -0.0281,  ...,  0.0358,  0.1898,  0.2507]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.self_attn.linear_query.bias': tensor([ 0.0382, -0.0410, -0.0156,  ..., -0.2959, -0.1064, -0.0396],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.self_attn.final_linear.weight': tensor([[-0.1278, -0.2954, -0.0012,  ...,  0.0865, -0.0617,  0.0590],\n",
       "         [-0.0119,  0.1688,  0.1304,  ...,  0.3594,  0.0177, -0.3987],\n",
       "         [-0.4302,  0.1376,  0.2698,  ...,  0.2532,  0.1486,  0.0579],\n",
       "         ...,\n",
       "         [ 0.1769,  0.1301,  0.2328,  ...,  0.0317, -0.1260, -0.1396],\n",
       "         [ 0.1765,  0.1703,  0.3884,  ...,  0.1753, -0.0198,  0.1588],\n",
       "         [-0.3267, -0.1447, -0.5879,  ..., -0.1171,  0.1121,  0.1244]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.self_attn.final_linear.bias': tensor([-0.1833,  0.3335, -0.2705,  ..., -0.1660, -0.0508,  0.5049],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.layer_norm_1.weight': tensor([0.3381, 0.3489, 0.3391,  ..., 0.2952, 0.3115, 0.3103],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.layer_norm_1.bias': tensor([ 0.0134,  0.0132,  0.0375,  ..., -0.0148,  0.0051, -0.0371],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.context_attn.linear_keys.weight': tensor([[-0.3711, -0.0614,  0.0996,  ..., -0.1182,  0.0873, -0.0322],\n",
       "         [-0.2235,  0.3223,  0.1847,  ..., -0.1969, -0.2405,  0.1166],\n",
       "         [-0.2136,  0.0717, -0.1069,  ...,  0.2681,  0.1412,  0.0034],\n",
       "         ...,\n",
       "         [-0.2046, -0.1971, -0.0922,  ..., -0.1748,  0.0704, -0.0338],\n",
       "         [-0.2839,  0.0338, -0.0923,  ..., -0.1671, -0.1193, -0.1566],\n",
       "         [ 0.1528,  0.1070,  0.4490,  ..., -0.1133,  0.0535, -0.0446]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.context_attn.linear_keys.bias': tensor([-0.0200, -0.0157, -0.0066,  ..., -0.0196,  0.0060, -0.0199],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.context_attn.linear_values.weight': tensor([[ 0.1058, -0.1161, -0.4517,  ..., -0.2382,  0.5146,  0.0323],\n",
       "         [-0.3267, -0.0266, -0.2542,  ...,  0.1765,  0.0455,  0.0431],\n",
       "         [-0.0636, -0.2844, -0.0081,  ...,  0.1559,  0.4092, -0.1288],\n",
       "         ...,\n",
       "         [ 0.1447,  0.1050,  0.2108,  ..., -0.6055,  0.3354, -0.0129],\n",
       "         [ 0.3230,  0.1869,  0.1417,  ...,  0.0695, -0.3398, -0.0172],\n",
       "         [ 0.1238, -0.2754,  0.4731,  ..., -0.1932, -0.1099, -0.0350]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.context_attn.linear_values.bias': tensor([-0.1930, -0.0421,  0.1481,  ..., -0.1722, -0.0246,  0.0468],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.context_attn.linear_query.weight': tensor([[-0.4705, -0.0850, -0.0084,  ..., -0.3408,  0.0473,  0.0865],\n",
       "         [ 0.1898,  0.0959,  0.1753,  ...,  0.1469, -0.1388,  0.1073],\n",
       "         [ 0.1635,  0.0573, -0.0794,  ..., -0.0566, -0.0975, -0.1176],\n",
       "         ...,\n",
       "         [ 0.0769, -0.1356, -0.0036,  ..., -0.0539,  0.0759, -0.1608],\n",
       "         [ 0.1654, -0.2303,  0.0085,  ..., -0.2002, -0.1268, -0.1071],\n",
       "         [-0.1133, -0.1595, -0.2695,  ..., -0.3528,  0.2029,  0.0522]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.context_attn.linear_query.bias': tensor([-0.0659,  0.3047,  0.0176,  ..., -0.0298, -0.1838,  0.0766],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.context_attn.final_linear.weight': tensor([[ 0.1005, -0.2866,  0.0150,  ...,  0.0687,  0.0201,  0.1119],\n",
       "         [-0.3127,  0.0046, -0.4910,  ...,  0.1282, -0.5044, -0.1580],\n",
       "         [-0.0880,  0.1805, -0.3584,  ...,  0.2240, -0.0374, -0.4968],\n",
       "         ...,\n",
       "         [-0.4656,  0.1459, -0.1335,  ..., -0.4026, -0.0428, -0.1377],\n",
       "         [ 0.1560,  0.2075,  0.0927,  ..., -0.0337,  0.0627,  0.0725],\n",
       "         [-0.0769,  0.4978,  0.0244,  ...,  0.6108, -0.0981,  0.1642]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.context_attn.final_linear.bias': tensor([ 0.1737,  0.0965,  0.4023,  ..., -0.2581, -0.4963, -0.3105],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.layer_norm_2.weight': tensor([0.1185, 0.1268, 0.1243,  ..., 0.0994, 0.1035, 0.1086],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.layer_norm_2.bias': tensor([-0.0175, -0.0222, -0.0108,  ..., -0.0543, -0.0084, -0.0273],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.feed_forward.w_1.weight': tensor([[ 0.0210, -0.0683,  0.2157,  ...,  0.0250,  0.3140,  0.4192],\n",
       "         [ 0.2238,  0.2529,  0.1068,  ...,  0.2678,  0.0637,  0.1896],\n",
       "         [ 0.4622,  0.0760,  0.2435,  ...,  0.1342,  0.2158, -0.1066],\n",
       "         ...,\n",
       "         [-0.0657,  0.0519, -0.1526,  ...,  0.2031,  0.1222, -0.0950],\n",
       "         [ 0.3167,  0.2002,  0.4006,  ...,  0.2532,  0.0899,  0.1620],\n",
       "         [ 0.0201, -0.0225, -0.0485,  ...,  0.0050, -0.2136, -0.0174]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.feed_forward.w_1.bias': tensor([-0.1454,  0.0207, -0.2444,  ..., -0.2520, -0.2686, -0.0136],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.feed_forward.w_2.weight': tensor([[ 0.0240,  0.0366, -0.3574,  ..., -0.0224, -0.1803, -0.0250],\n",
       "         [-0.1412, -0.0702, -0.0832,  ...,  0.1190,  0.1544, -0.0342],\n",
       "         [ 0.1305, -0.0353, -0.0725,  ..., -0.1155,  0.4028, -0.3269],\n",
       "         ...,\n",
       "         [-0.0705, -0.2150,  0.0446,  ...,  0.0066,  0.0831,  0.1426],\n",
       "         [-0.1654,  0.1814, -0.2573,  ..., -0.2487,  0.2815, -0.0829],\n",
       "         [-0.1708,  0.1843,  0.0834,  ...,  0.1592,  0.1464, -0.4170]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.feed_forward.w_2.bias': tensor([ 0.3928, -0.1920,  0.4954,  ..., -0.3877,  0.5059,  0.1106],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.feed_forward.layer_norm.weight': tensor([0.7974, 0.8071, 0.7432,  ..., 0.8247, 0.7603, 0.7881],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.7.feed_forward.layer_norm.bias': tensor([-0.1459,  0.1334,  0.0034,  ..., -0.1862, -0.0635, -0.1029],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.self_attn.linear_keys.weight': tensor([[-0.1423,  0.0493, -0.0364,  ..., -0.2412, -0.0038, -0.1906],\n",
       "         [-0.1034,  0.0400, -0.0563,  ...,  0.0095,  0.1847,  0.1011],\n",
       "         [ 0.0593, -0.0185, -0.1671,  ...,  0.0114, -0.2832, -0.0166],\n",
       "         ...,\n",
       "         [-0.0690, -0.1224,  0.2441,  ..., -0.0893,  0.0546,  0.2991],\n",
       "         [-0.0170, -0.0854,  0.1444,  ..., -0.0768,  0.1268,  0.1306],\n",
       "         [ 0.1249,  0.1434,  0.1725,  ..., -0.2191, -0.0253, -0.0129]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.self_attn.linear_keys.bias': tensor([-0.0060,  0.0152,  0.0233,  ..., -0.0166,  0.0221, -0.0134],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.self_attn.linear_values.weight': tensor([[ 0.1410,  0.1727, -0.0191,  ..., -0.1436,  0.0425,  0.3655],\n",
       "         [ 0.2812, -0.0403,  0.1802,  ..., -0.1816, -0.2754, -0.1415],\n",
       "         [ 0.1836,  0.2639, -0.3894,  ..., -0.3167,  0.1504, -0.4910],\n",
       "         ...,\n",
       "         [ 0.2883, -0.0306, -0.3403,  ...,  0.0630, -0.2426, -0.0954],\n",
       "         [-0.0223,  0.1888, -0.0462,  ..., -0.0820,  0.1877, -0.0966],\n",
       "         [-0.1565, -0.0103,  0.1691,  ..., -0.1262, -0.2935,  0.1974]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.self_attn.linear_values.bias': tensor([ 0.0117,  0.1648,  0.0165,  ..., -0.2168,  0.2529, -0.0457],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.self_attn.linear_query.weight': tensor([[ 0.0427, -0.1158, -0.1973,  ..., -0.0760,  0.0096,  0.0612],\n",
       "         [ 0.0319, -0.3293,  0.0804,  ...,  0.0157,  0.0489,  0.0158],\n",
       "         [ 0.1538,  0.0602,  0.1025,  ...,  0.0074, -0.0712, -0.3633],\n",
       "         ...,\n",
       "         [-0.0150, -0.0189,  0.0870,  ...,  0.0556,  0.0825,  0.0517],\n",
       "         [ 0.0912, -0.2279, -0.0597,  ...,  0.1247, -0.0776, -0.0311],\n",
       "         [ 0.0519,  0.0293, -0.0393,  ...,  0.1355,  0.0712, -0.2561]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.self_attn.linear_query.bias': tensor([-0.1656, -0.0432, -0.0004,  ...,  0.0256, -0.1768, -0.0460],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.self_attn.final_linear.weight': tensor([[ 0.4148,  0.2620,  0.1461,  ..., -0.2103, -0.0971, -0.0137],\n",
       "         [ 0.0315,  0.1788, -0.1793,  ...,  0.2517, -0.2411, -0.2437],\n",
       "         [ 0.2203, -0.0828, -0.3955,  ...,  0.0433, -0.3066, -0.2430],\n",
       "         ...,\n",
       "         [ 0.2292,  0.0508, -0.0446,  ..., -0.0126,  0.1786, -0.0538],\n",
       "         [-0.1774, -0.1196, -0.0755,  ...,  0.2372, -0.1290,  0.0710],\n",
       "         [-0.3435,  0.0744,  0.2435,  ...,  0.1345, -0.1583, -0.0632]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.self_attn.final_linear.bias': tensor([ 0.1243,  0.2539, -0.2976,  ...,  0.2218,  0.1137,  0.5005],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.layer_norm_1.weight': tensor([0.4102, 0.3804, 0.3806,  ..., 0.3447, 0.3757, 0.3708],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.layer_norm_1.bias': tensor([ 0.0182,  0.0095,  0.0290,  ..., -0.0136,  0.0106, -0.0280],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.context_attn.linear_keys.weight': tensor([[-0.1570, -0.1323,  0.1772,  ...,  0.0469, -0.0352,  0.0019],\n",
       "         [ 0.1349, -0.2822,  0.0867,  ...,  0.0389,  0.1478, -0.0174],\n",
       "         [ 0.3291,  0.3396, -0.0086,  ...,  0.1729, -0.1600, -0.0617],\n",
       "         ...,\n",
       "         [ 0.0868, -0.1453,  0.1772,  ...,  0.0400,  0.0132,  0.0280],\n",
       "         [ 0.2019, -0.1984, -0.2167,  ..., -0.0485,  0.0724,  0.0322],\n",
       "         [-0.0590, -0.1819,  0.2715,  ..., -0.0208,  0.1816,  0.0951]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.context_attn.linear_keys.bias': tensor([-0.0289,  0.0023, -0.0247,  ..., -0.0080, -0.0143, -0.0205],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.context_attn.linear_values.weight': tensor([[ 0.1135,  0.0676,  0.0016,  ...,  0.1982,  0.0425, -0.0825],\n",
       "         [ 0.2925, -0.0800, -0.0367,  ...,  0.3589,  0.2947,  0.0084],\n",
       "         [ 0.2712, -0.0820,  0.3889,  ..., -0.0808, -0.3374, -0.0158],\n",
       "         ...,\n",
       "         [ 0.2910,  0.1021, -0.0634,  ..., -0.1815,  0.0561,  0.0302],\n",
       "         [-0.0091,  0.0112,  0.0781,  ...,  0.1381, -0.3035, -0.0421],\n",
       "         [ 0.1682, -0.0823, -0.3062,  ..., -0.4304,  0.1691, -0.1249]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.context_attn.linear_values.bias': tensor([ 0.0173, -0.0172,  0.1008,  ..., -0.0360,  0.2008,  0.3875],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.context_attn.linear_query.weight': tensor([[-0.1891, -0.1931, -0.1881,  ..., -0.2788, -0.0663, -0.0147],\n",
       "         [ 0.1151, -0.2732, -0.0383,  ..., -0.3081, -0.1140,  0.0378],\n",
       "         [ 0.0862,  0.1174,  0.1058,  ...,  0.0028,  0.0516,  0.1337],\n",
       "         ...,\n",
       "         [ 0.1978, -0.2303,  0.2014,  ...,  0.0764, -0.0551, -0.0450],\n",
       "         [-0.0210,  0.0884,  0.1086,  ...,  0.0174, -0.0796,  0.1521],\n",
       "         [ 0.0110, -0.0436, -0.0723,  ..., -0.0287, -0.0842, -0.0739]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.context_attn.linear_query.bias': tensor([ 0.0989,  0.0967, -0.0564,  ..., -0.0964, -0.0172,  0.1020],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.context_attn.final_linear.weight': tensor([[-0.1542,  0.2229, -0.0826,  ..., -0.1195, -0.4167,  0.3042],\n",
       "         [ 0.1630, -0.0679,  0.4854,  ..., -0.0095,  0.1709,  0.2983],\n",
       "         [ 0.0172,  0.0249, -0.0252,  ..., -0.3215,  0.0625,  0.4380],\n",
       "         ...,\n",
       "         [-0.0400, -0.0425,  0.1798,  ...,  0.2079,  0.4265, -0.1339],\n",
       "         [-0.0088,  0.0418, -0.2290,  ..., -0.1038, -0.1912,  0.1637],\n",
       "         [-0.0147,  0.0756,  0.0246,  ..., -0.1266, -0.1960,  0.1256]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.context_attn.final_linear.bias': tensor([-0.3616,  0.0895, -0.0173,  ...,  0.2443, -0.3513, -0.4993],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.layer_norm_2.weight': tensor([0.1331, 0.1266, 0.1376,  ..., 0.1132, 0.1064, 0.1123],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.layer_norm_2.bias': tensor([-0.0070, -0.0088, -0.0046,  ..., -0.0548, -0.0182, -0.0306],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.feed_forward.w_1.weight': tensor([[-0.4417, -0.2146,  0.1429,  ..., -0.1768, -0.0359,  0.2349],\n",
       "         [-0.0397, -0.4207,  0.0406,  ...,  0.1848, -0.0008, -0.0076],\n",
       "         [ 0.1375, -0.1194,  0.0920,  ..., -0.1770,  0.4021, -0.0606],\n",
       "         ...,\n",
       "         [-0.0806, -0.1650,  0.2084,  ...,  0.1735,  0.2954, -0.1643],\n",
       "         [ 0.5210, -0.0171,  0.2417,  ...,  0.1959, -0.0807, -0.0129],\n",
       "         [-0.1121,  0.0619, -0.0764,  ..., -0.1229, -0.0679, -0.0245]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.feed_forward.w_1.bias': tensor([-0.1028, -0.3286, -0.2461,  ..., -0.3433, -0.4426, -0.0970],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.feed_forward.w_2.weight': tensor([[ 0.0093,  0.0911, -0.0565,  ..., -0.0317, -0.2344, -0.0305],\n",
       "         [-0.1876,  0.0094,  0.1968,  ..., -0.0558, -0.0459,  0.0405],\n",
       "         [ 0.0945, -0.0292,  0.0138,  ..., -0.2668,  0.0869,  0.0090],\n",
       "         ...,\n",
       "         [-0.2488,  0.2688,  0.2932,  ...,  0.2343, -0.2141, -0.0137],\n",
       "         [ 0.0183,  0.1749, -0.2096,  ...,  0.0723,  0.1492,  0.2289],\n",
       "         [ 0.0099, -0.0686,  0.1202,  ...,  0.0895,  0.4971,  0.1537]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.feed_forward.w_2.bias': tensor([ 0.5020, -0.3313,  0.4246,  ..., -0.5049,  0.5127, -0.2952],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.feed_forward.layer_norm.weight': tensor([1.0195, 1.0068, 0.9004,  ..., 1.0664, 0.9517, 1.0547],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.8.feed_forward.layer_norm.bias': tensor([-0.0638,  0.1104, -0.0249,  ..., -0.1152, -0.0183, -0.0844],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.self_attn.linear_keys.weight': tensor([[ 0.0412,  0.0670, -0.1750,  ...,  0.0121,  0.0238, -0.1610],\n",
       "         [-0.0029, -0.0763, -0.0480,  ..., -0.0039, -0.0032, -0.0043],\n",
       "         [-0.1422, -0.0167, -0.0263,  ..., -0.0453,  0.0587,  0.0488],\n",
       "         ...,\n",
       "         [-0.0135, -0.2300, -0.1558,  ...,  0.0684, -0.0237, -0.0024],\n",
       "         [ 0.0916, -0.0360,  0.0775,  ...,  0.0323,  0.3794,  0.0118],\n",
       "         [ 0.0227, -0.2444, -0.0505,  ...,  0.1031,  0.0501, -0.0388]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.self_attn.linear_keys.bias': tensor([ 0.0125, -0.0178, -0.0150,  ...,  0.0234, -0.0055, -0.0001],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.self_attn.linear_values.weight': tensor([[ 0.0773, -0.0530,  0.0605,  ..., -0.1372,  0.0030,  0.0672],\n",
       "         [ 0.2700,  0.0829, -0.3567,  ...,  0.2375, -0.0533,  0.0267],\n",
       "         [ 0.0301,  0.2338,  0.0894,  ..., -0.2131, -0.3374,  0.2045],\n",
       "         ...,\n",
       "         [-0.2666, -0.2839, -0.1636,  ...,  0.3601,  0.0040, -0.3984],\n",
       "         [-0.0587,  0.0031, -0.1780,  ...,  0.2126,  0.2250,  0.1445],\n",
       "         [-0.2861,  0.4861,  0.2120,  ..., -0.1197,  0.0020, -0.3445]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.self_attn.linear_values.bias': tensor([ 0.2822,  0.0126,  0.1081,  ..., -0.1305, -0.0354, -0.1240],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.self_attn.linear_query.weight': tensor([[ 0.0190,  0.2964, -0.2443,  ..., -0.0494,  0.1162, -0.1262],\n",
       "         [-0.0831,  0.1069, -0.2057,  ...,  0.0015, -0.0763, -0.0235],\n",
       "         [-0.0543, -0.1862, -0.0384,  ...,  0.0033,  0.2153,  0.0687],\n",
       "         ...,\n",
       "         [ 0.1157,  0.0988,  0.0569,  ...,  0.2520,  0.2612, -0.1421],\n",
       "         [ 0.1771, -0.0110, -0.0386,  ...,  0.0368, -0.1282,  0.1573],\n",
       "         [ 0.0934, -0.0471,  0.1450,  ...,  0.0489, -0.0100, -0.0621]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.self_attn.linear_query.bias': tensor([ 0.0312, -0.0041,  0.0637,  ...,  0.0500,  0.0525, -0.0266],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.self_attn.final_linear.weight': tensor([[ 0.0059,  0.2668,  0.2118,  ..., -0.2654, -0.0499,  0.2559],\n",
       "         [-0.2656,  0.0468, -0.2856,  ...,  0.0208, -0.1278, -0.0139],\n",
       "         [ 0.3335, -0.2891,  0.0821,  ...,  0.1823, -0.1914,  0.5107],\n",
       "         ...,\n",
       "         [ 0.3220, -0.3181,  0.3914,  ..., -0.2502, -0.1149, -0.1750],\n",
       "         [ 0.0428,  0.4807, -0.2045,  ..., -0.2837,  0.4756,  0.1137],\n",
       "         [ 0.2433,  0.1946,  0.1001,  ..., -0.3577,  0.1799,  0.0641]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.self_attn.final_linear.bias': tensor([ 0.1238, -0.4033, -0.2258,  ...,  0.2472,  0.3831,  0.5000],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.layer_norm_1.weight': tensor([0.4631, 0.4277, 0.4272,  ..., 0.4153, 0.4421, 0.4478],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.layer_norm_1.bias': tensor([ 0.0149,  0.0303,  0.0506,  ..., -0.0093,  0.0063, -0.0349],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.context_attn.linear_keys.weight': tensor([[ 0.0770,  0.0722,  0.1160,  ...,  0.3164,  0.0057, -0.0553],\n",
       "         [ 0.3330, -0.0047,  0.2705,  ..., -0.0791,  0.2482,  0.0441],\n",
       "         [-0.3704, -0.2062, -0.1801,  ..., -0.1881, -0.1003,  0.1001],\n",
       "         ...,\n",
       "         [-0.1610, -0.0007,  0.0804,  ..., -0.1913,  0.0624, -0.0886],\n",
       "         [ 0.3018,  0.2052, -0.0257,  ..., -0.0631,  0.1400,  0.0320],\n",
       "         [ 0.1862,  0.1296,  0.1224,  ..., -0.0197, -0.2944, -0.0590]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.context_attn.linear_keys.bias': tensor([-0.0226, -0.0145,  0.0011,  ...,  0.0308, -0.0099,  0.0002],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.context_attn.linear_values.weight': tensor([[-0.3394,  0.0433, -0.1200,  ...,  0.0173, -0.2808, -0.0782],\n",
       "         [-0.0045,  0.4124,  0.1859,  ..., -0.4805,  0.1549,  0.0018],\n",
       "         [-0.4233,  0.1075,  0.4949,  ...,  0.1077,  0.1019, -0.0264],\n",
       "         ...,\n",
       "         [ 0.2230, -0.0882, -0.2074,  ...,  0.0633,  0.1359, -0.0247],\n",
       "         [-0.5117,  0.3386,  0.1121,  ..., -0.0284, -0.0029, -0.0371],\n",
       "         [ 0.0726,  0.0780,  0.4983,  ...,  0.0255, -0.1683, -0.0468]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.context_attn.linear_values.bias': tensor([ 0.3779, -0.3152,  0.2307,  ...,  0.1636,  0.0676,  0.1646],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.context_attn.linear_query.weight': tensor([[ 0.2583,  0.1307,  0.1001,  ..., -0.1404,  0.1581, -0.2004],\n",
       "         [-0.3770,  0.1218, -0.2306,  ..., -0.0723, -0.1956, -0.0639],\n",
       "         [-0.0841, -0.5142, -0.1659,  ..., -0.0734, -0.1177,  0.3169],\n",
       "         ...,\n",
       "         [-0.2815,  0.1479,  0.0992,  ..., -0.1552, -0.0829, -0.0184],\n",
       "         [-0.2330, -0.3279,  0.1219,  ..., -0.0037,  0.1787, -0.1523],\n",
       "         [-0.0017,  0.0161,  0.3259,  ..., -0.0775,  0.1696,  0.1904]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.context_attn.linear_query.bias': tensor([ 0.0332,  0.0112, -0.0296,  ...,  0.0381, -0.1301,  0.1093],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.context_attn.final_linear.weight': tensor([[ 0.1061, -0.4470, -0.2705,  ...,  0.2384,  0.1384, -0.0640],\n",
       "         [-0.0952,  0.2932,  0.1537,  ..., -0.0050,  0.5049,  0.0322],\n",
       "         [-0.0566,  0.2688,  0.1437,  ...,  0.0014, -0.3093, -0.1046],\n",
       "         ...,\n",
       "         [ 0.1476, -0.1459,  0.0959,  ...,  0.0439,  0.0358,  0.0531],\n",
       "         [-0.2729,  0.0743, -0.1349,  ..., -0.3132, -0.3823,  0.0961],\n",
       "         [-0.0887,  0.0021,  0.0411,  ...,  0.0129,  0.2395, -0.0094]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.context_attn.final_linear.bias': tensor([-0.2170, -0.0152,  0.1914,  ...,  0.2715, -0.2610,  0.1967],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.layer_norm_2.weight': tensor([0.1378, 0.1429, 0.1512,  ..., 0.1249, 0.1181, 0.1212],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.layer_norm_2.bias': tensor([-0.0044, -0.0116, -0.0141,  ..., -0.0624, -0.0088, -0.0331],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.feed_forward.w_1.weight': tensor([[-0.1227, -0.1321,  0.0471,  ..., -0.0699,  0.3000,  0.0210],\n",
       "         [ 0.0005, -0.0152,  0.2771,  ...,  0.0279,  0.3499,  0.0677],\n",
       "         [ 0.1163, -0.1272,  0.0693,  ..., -0.0362, -0.1906, -0.1945],\n",
       "         ...,\n",
       "         [ 0.0235,  0.0792, -0.1301,  ..., -0.2754,  0.1683,  0.1143],\n",
       "         [ 0.2629,  0.1120,  0.1351,  ..., -0.4358,  0.3750,  0.1439],\n",
       "         [-0.0416, -0.0441, -0.0393,  ...,  0.2874, -0.1884, -0.1794]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.feed_forward.w_1.bias': tensor([-0.1962,  0.0445, -0.2059,  ..., -0.0576, -0.1960, -0.2871],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.feed_forward.w_2.weight': tensor([[ 8.3008e-02, -5.6122e-02, -7.4219e-02,  ...,  1.0742e-01,\n",
       "           1.8079e-01,  3.5797e-02],\n",
       "         [ 6.8481e-02, -3.7988e-01,  6.8176e-02,  ..., -1.2708e-01,\n",
       "          -2.2437e-01, -1.4900e-02],\n",
       "         [ 6.8115e-02,  1.5976e-02, -5.3986e-02,  ...,  9.3079e-03,\n",
       "          -7.8064e-02, -2.0206e-04],\n",
       "         ...,\n",
       "         [ 1.0767e-01, -2.2559e-01,  6.6956e-02,  ...,  1.7554e-01,\n",
       "          -1.1255e-01, -1.7712e-01],\n",
       "         [ 1.7139e-01, -2.7878e-02,  2.8882e-01,  ..., -3.6530e-02,\n",
       "           3.8892e-01, -3.6774e-02],\n",
       "         [ 1.1041e-01,  1.1151e-01,  4.4946e-01,  ...,  2.5220e-01,\n",
       "          -1.1469e-01,  2.3083e-01]], dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.feed_forward.w_2.bias': tensor([ 0.4521, -0.3462,  0.2539,  ..., -0.4795,  0.3608, -0.4741],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.feed_forward.layer_norm.weight': tensor([1.1299, 1.1377, 1.1543,  ..., 1.1816, 1.1719, 1.1152],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.9.feed_forward.layer_norm.bias': tensor([-0.0729,  0.0707,  0.0654,  ..., -0.0680, -0.0355, -0.0500],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.self_attn.linear_keys.weight': tensor([[-0.0400,  0.3137,  0.2144,  ..., -0.0206, -0.0992,  0.0430],\n",
       "         [ 0.0554, -0.1588, -0.0986,  ...,  0.0203,  0.0802,  0.0438],\n",
       "         [ 0.0055,  0.3245,  0.2040,  ...,  0.0994,  0.1707, -0.0129],\n",
       "         ...,\n",
       "         [ 0.2585,  0.1141, -0.0263,  ...,  0.3691, -0.1616,  0.1261],\n",
       "         [-0.1812, -0.1230, -0.0220,  ...,  0.2556,  0.0544, -0.1011],\n",
       "         [ 0.0005,  0.0013, -0.0595,  ...,  0.1917,  0.1915, -0.1076]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.self_attn.linear_keys.bias': tensor([-0.0079, -0.0251, -0.0022,  ...,  0.0141, -0.0074, -0.0267],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.self_attn.linear_values.weight': tensor([[ 0.0881, -0.0816, -0.3635,  ..., -0.1008,  0.2979, -0.1902],\n",
       "         [-0.4851,  0.2356, -0.1570,  ...,  0.4988,  0.2627, -0.2037],\n",
       "         [-0.4224,  0.0797, -0.1847,  ...,  0.1333,  0.2605,  0.1112],\n",
       "         ...,\n",
       "         [-0.1384,  0.1107,  0.3318,  ...,  0.0625, -0.3311,  0.2041],\n",
       "         [ 0.2361, -0.0741,  0.0787,  ...,  0.1681,  0.2666,  0.1646],\n",
       "         [ 0.0750,  0.3904,  0.2969,  ..., -0.0723,  0.3203,  0.2676]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.self_attn.linear_values.bias': tensor([ 0.0285,  0.0306, -0.0096,  ...,  0.1098, -0.0996,  0.0311],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.self_attn.linear_query.weight': tensor([[-0.0460, -0.0241, -0.1788,  ...,  0.0292,  0.1790,  0.0556],\n",
       "         [ 0.1548,  0.5137,  0.0047,  ...,  0.0356, -0.0297, -0.0916],\n",
       "         [ 0.2031,  0.1603,  0.0542,  ...,  0.1442, -0.1305, -0.3298],\n",
       "         ...,\n",
       "         [ 0.1368, -0.1372,  0.1624,  ..., -0.0352,  0.0616,  0.0765],\n",
       "         [-0.0724,  0.2190,  0.0684,  ...,  0.4541,  0.0286,  0.0825],\n",
       "         [ 0.0134,  0.2773, -0.0466,  ...,  0.0743, -0.0175,  0.0654]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.self_attn.linear_query.bias': tensor([-0.1167,  0.1196,  0.5557,  ...,  0.0797,  0.0830, -0.0103],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.self_attn.final_linear.weight': tensor([[ 0.1730, -0.1925,  0.2759,  ..., -0.1399,  0.3088, -0.1022],\n",
       "         [ 0.3936,  0.5000,  0.0141,  ..., -0.1937, -0.4775,  0.0814],\n",
       "         [ 0.2512, -0.1377, -0.1252,  ..., -0.0349, -0.4585, -0.1528],\n",
       "         ...,\n",
       "         [ 0.0133,  0.1696,  0.0926,  ..., -0.1819,  0.3171, -0.1846],\n",
       "         [ 0.3127,  0.3918,  0.3259,  ...,  0.1276,  0.2471, -0.1536],\n",
       "         [-0.2988, -0.4556,  0.1615,  ...,  0.1554,  0.1527,  0.1873]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.self_attn.final_linear.bias': tensor([-0.0384, -0.5000, -0.2482,  ...,  0.2445,  0.2729,  0.2881],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.layer_norm_1.weight': tensor([0.4563, 0.4343, 0.4404,  ..., 0.4568, 0.4783, 0.4636],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.layer_norm_1.bias': tensor([ 0.0194,  0.0422,  0.0615,  ..., -0.0120,  0.0118, -0.0328],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.context_attn.linear_keys.weight': tensor([[ 3.4149e-02, -5.1575e-02, -6.8474e-03,  ..., -2.6270e-01,\n",
       "          -1.0559e-01,  3.8544e-02],\n",
       "         [-1.1383e-02,  1.3562e-01,  5.2887e-02,  ..., -3.1143e-02,\n",
       "           4.0924e-02, -1.0577e-01],\n",
       "         [ 1.4014e-01, -5.2338e-02,  1.0590e-01,  ...,  8.5938e-02,\n",
       "          -1.3757e-01, -5.6519e-02],\n",
       "         ...,\n",
       "         [ 8.4595e-02,  1.6101e-01,  5.9387e-02,  ...,  2.5620e-02,\n",
       "           1.3062e-01,  8.1665e-02],\n",
       "         [-1.1390e-04,  1.9577e-02, -4.6783e-02,  ...,  6.4331e-02,\n",
       "           2.7808e-01,  7.2205e-02],\n",
       "         [ 1.6769e-02,  1.3708e-01,  1.8848e-01,  ...,  1.6357e-01,\n",
       "          -3.4241e-02,  4.7729e-02]], dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.context_attn.linear_keys.bias': tensor([-0.0041, -0.0047, -0.0187,  ..., -0.0097,  0.0097, -0.0026],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.context_attn.linear_values.weight': tensor([[-0.3379, -0.1038, -0.1940,  ...,  0.1746, -0.0856,  0.0266],\n",
       "         [-0.2671,  0.0333, -0.0767,  ...,  0.4155, -0.5005, -0.0095],\n",
       "         [ 0.3232, -0.4922, -0.1841,  ..., -0.3206, -0.0970, -0.0087],\n",
       "         ...,\n",
       "         [ 0.1046,  0.0024, -0.0066,  ...,  0.4270, -0.3845, -0.0042],\n",
       "         [-0.1265, -0.1398,  0.5059,  ..., -0.2610, -0.1061, -0.1057],\n",
       "         [-0.2502,  0.1376, -0.0500,  ...,  0.3198, -0.2014, -0.0424]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.context_attn.linear_values.bias': tensor([-0.1736, -0.1248, -0.0724,  ..., -0.0193,  0.0529, -0.0103],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.context_attn.linear_query.weight': tensor([[ 0.2465,  0.3572, -0.1578,  ...,  0.1051,  0.1056,  0.2705],\n",
       "         [ 0.1396,  0.0517,  0.1371,  ..., -0.1715,  0.2111,  0.0310],\n",
       "         [ 0.2795, -0.0452,  0.0254,  ...,  0.0745, -0.2004, -0.0875],\n",
       "         ...,\n",
       "         [ 0.2090,  0.1552,  0.0294,  ...,  0.0163,  0.1880, -0.0748],\n",
       "         [-0.0975,  0.2720, -0.0205,  ..., -0.2959,  0.0607, -0.1752],\n",
       "         [-0.0417,  0.0686,  0.0341,  ..., -0.0052, -0.0607,  0.1924]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.context_attn.linear_query.bias': tensor([-0.0300, -0.0188,  0.1372,  ...,  0.0365,  0.1879, -0.0723],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.context_attn.final_linear.weight': tensor([[-0.3167,  0.0122,  0.5063,  ..., -0.1279, -0.4751, -0.0692],\n",
       "         [ 0.3191, -0.3286, -0.1993,  ...,  0.2517, -0.3118,  0.1114],\n",
       "         [-0.2600, -0.3374,  0.1562,  ..., -0.0804, -0.0756,  0.4536],\n",
       "         ...,\n",
       "         [-0.4307,  0.0817, -0.2385,  ...,  0.0475, -0.0667, -0.0591],\n",
       "         [-0.3049, -0.0431, -0.3372,  ...,  0.0893, -0.2561, -0.6758],\n",
       "         [ 0.0142, -0.3330, -0.1852,  ...,  0.0263,  0.1252,  0.2230]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.context_attn.final_linear.bias': tensor([-0.4207,  0.3188, -0.1364,  ...,  0.4988,  0.1904,  0.2290],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.layer_norm_2.weight': tensor([0.1527, 0.1567, 0.1797,  ..., 0.1515, 0.1356, 0.1373],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.layer_norm_2.bias': tensor([ 0.0061, -0.0064, -0.0070,  ..., -0.0638, -0.0081, -0.0396],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.feed_forward.w_1.weight': tensor([[ 0.1241, -0.1385, -0.1334,  ..., -0.0159,  0.2927,  0.0763],\n",
       "         [ 0.0085,  0.0656,  0.0775,  ..., -0.0494, -0.1176,  0.4116],\n",
       "         [-0.4534,  0.1737,  0.0739,  ...,  0.1304,  0.3660, -0.1552],\n",
       "         ...,\n",
       "         [ 0.4180,  0.0042, -0.1140,  ..., -0.1505, -0.3511, -0.1108],\n",
       "         [-0.1115,  0.1649,  0.0151,  ...,  0.1686,  0.1980, -0.2250],\n",
       "         [ 0.4780,  0.0598, -0.0205,  ...,  0.0529,  0.2556, -0.0136]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.feed_forward.w_1.bias': tensor([ 0.0577, -0.0452, -0.0366,  ..., -0.1164, -0.1792, -0.0677],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.feed_forward.w_2.weight': tensor([[-0.0407, -0.1617,  0.1550,  ...,  0.1205, -0.5635,  0.1567],\n",
       "         [-0.4338,  0.2764, -0.1478,  ..., -0.0103, -0.0054,  0.1014],\n",
       "         [-0.3440, -0.4995, -0.1874,  ...,  0.0639, -0.1431, -0.2632],\n",
       "         ...,\n",
       "         [-0.0983,  0.1627,  0.1613,  ..., -0.0656, -0.2231,  0.2571],\n",
       "         [ 0.2744, -0.3892,  0.2253,  ..., -0.0503,  0.2094,  0.0518],\n",
       "         [ 0.1449, -0.2666,  0.1943,  ..., -0.1539, -0.0191, -0.0632]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.feed_forward.w_2.bias': tensor([ 0.2487, -0.1239, -0.1879,  ..., -0.2219, -0.0636, -0.2351],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.feed_forward.layer_norm.weight': tensor([1.2422, 1.2334, 1.1982,  ..., 1.0586, 1.2021, 1.0361],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.10.feed_forward.layer_norm.bias': tensor([-0.0576,  0.0120,  0.0236,  ..., -0.0124,  0.0219, -0.0676],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.self_attn.linear_keys.weight': tensor([[ 0.2886,  0.1121, -0.0573,  ...,  0.0407, -0.0726, -0.2524],\n",
       "         [-0.0529, -0.0389,  0.0386,  ..., -0.1946, -0.0904,  0.0975],\n",
       "         [ 0.0696, -0.2507,  0.1560,  ..., -0.0846, -0.1680,  0.0520],\n",
       "         ...,\n",
       "         [-0.1136,  0.1085,  0.0013,  ...,  0.1771,  0.0459,  0.0127],\n",
       "         [-0.0032, -0.1432,  0.0987,  ...,  0.0447, -0.1344, -0.1840],\n",
       "         [ 0.0046, -0.1639, -0.0983,  ...,  0.0025, -0.1478,  0.0221]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.self_attn.linear_keys.bias': tensor([-0.0265, -0.0275, -0.0131,  ..., -0.0024,  0.0249,  0.0030],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.self_attn.linear_values.weight': tensor([[-0.0421,  0.3135,  0.3291,  ...,  0.0999,  0.2000, -0.1260],\n",
       "         [ 0.4084, -0.0457,  0.0492,  ..., -0.3721,  0.1077, -0.1754],\n",
       "         [ 0.2480, -0.3030, -0.0977,  ..., -0.2354,  0.1797,  0.2527],\n",
       "         ...,\n",
       "         [-0.5186, -0.2668,  0.1226,  ..., -0.4153, -0.1384, -0.2952],\n",
       "         [ 0.1610, -0.0661,  0.2795,  ..., -0.1015,  0.3125,  0.0173],\n",
       "         [ 0.3323, -0.0244, -0.1360,  ...,  0.2456, -0.2400,  0.1328]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.self_attn.linear_values.bias': tensor([-0.1528,  0.0600,  0.0855,  ..., -0.1096,  0.1088,  0.2185],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.self_attn.linear_query.weight': tensor([[ 0.0587,  0.0670, -0.1146,  ..., -0.2434, -0.0790, -0.1637],\n",
       "         [ 0.0241,  0.0916,  0.1638,  ...,  0.0948,  0.0159, -0.2111],\n",
       "         [ 0.2725,  0.0554, -0.0481,  ...,  0.0247, -0.1354, -0.2389],\n",
       "         ...,\n",
       "         [-0.1547,  0.1963,  0.0953,  ...,  0.1492,  0.0555, -0.2155],\n",
       "         [-0.1432, -0.2502, -0.0406,  ...,  0.1307, -0.1010,  0.1482],\n",
       "         [ 0.0346,  0.0297,  0.2104,  ...,  0.0958,  0.0427,  0.0003]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.self_attn.linear_query.bias': tensor([ 0.0385, -0.0245,  0.0283,  ..., -0.2791, -0.0375,  0.2209],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.self_attn.final_linear.weight': tensor([[-0.0576,  0.1973,  0.3860,  ...,  0.0765, -0.0016, -0.0147],\n",
       "         [ 0.2068,  0.3779, -0.3418,  ..., -0.0335, -0.2578, -0.1578],\n",
       "         [ 0.1189,  0.2546, -0.0859,  ..., -0.3020, -0.1877, -0.3271],\n",
       "         ...,\n",
       "         [ 0.0778,  0.1593, -0.0295,  ...,  0.2583,  0.0644, -0.0288],\n",
       "         [ 0.1534,  0.1674,  0.3032,  ..., -0.2220, -0.2607, -0.3271],\n",
       "         [-0.0537,  0.2104,  0.3301,  ...,  0.2390, -0.0082,  0.1754]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.self_attn.final_linear.bias': tensor([ 0.1097, -0.2463, -0.2507,  ...,  0.0800,  0.3486, -0.2583],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.layer_norm_1.weight': tensor([0.4829, 0.4480, 0.4556,  ..., 0.5693, 0.4761, 0.5493],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.layer_norm_1.bias': tensor([ 0.0308,  0.0459,  0.0575,  ...,  0.0363,  0.0300, -0.0277],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.context_attn.linear_keys.weight': tensor([[ 0.0150, -0.1483,  0.0195,  ..., -0.0609,  0.2163, -0.0399],\n",
       "         [ 0.0737,  0.1042,  0.1449,  ..., -0.0804,  0.0560,  0.0888],\n",
       "         [ 0.0198, -0.0088, -0.1322,  ...,  0.1057, -0.1063,  0.0030],\n",
       "         ...,\n",
       "         [ 0.2419, -0.2389, -0.0453,  ...,  0.0304,  0.2489, -0.0426],\n",
       "         [-0.1217, -0.0621, -0.0611,  ..., -0.0334, -0.2382,  0.0494],\n",
       "         [ 0.1508,  0.0376, -0.2196,  ...,  0.0021, -0.0352,  0.0072]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.context_attn.linear_keys.bias': tensor([ 0.0108, -0.0240, -0.0222,  ..., -0.0309,  0.0018,  0.0179],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.context_attn.linear_values.weight': tensor([[ 0.0915, -0.2383,  0.2522,  ..., -0.1445,  0.1812, -0.0104],\n",
       "         [-0.0807, -0.5171,  0.2520,  ...,  0.1231, -0.4302, -0.0272],\n",
       "         [-0.3323, -0.1627, -0.3203,  ...,  0.1217,  0.1506, -0.0217],\n",
       "         ...,\n",
       "         [ 0.2469,  0.3311,  0.0141,  ...,  0.2886, -0.0831,  0.0104],\n",
       "         [-0.2668,  0.0880, -0.2100,  ...,  0.3708,  0.2345,  0.0215],\n",
       "         [-0.4988,  0.3843, -0.3296,  ..., -0.2578,  0.1772,  0.0237]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.context_attn.linear_values.bias': tensor([-0.0620, -0.0496, -0.1109,  ..., -0.0281, -0.0598, -0.0185],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.context_attn.linear_query.weight': tensor([[ 0.0280,  0.3601, -0.0068,  ..., -0.0675, -0.1942, -0.1337],\n",
       "         [ 0.0245, -0.2771,  0.0118,  ...,  0.1318,  0.0584, -0.0584],\n",
       "         [ 0.2629,  0.1715,  0.2456,  ..., -0.2177,  0.0575, -0.1232],\n",
       "         ...,\n",
       "         [ 0.1587,  0.0914, -0.1600,  ..., -0.0190,  0.1428,  0.0207],\n",
       "         [ 0.1343,  0.1017, -0.2053,  ..., -0.0431, -0.0784,  0.0717],\n",
       "         [ 0.4167,  0.2212,  0.1477,  ...,  0.2322,  0.1138,  0.1298]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.context_attn.linear_query.bias': tensor([-0.0859,  0.0497, -0.0948,  ..., -0.1085,  0.3074,  0.0983],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.context_attn.final_linear.weight': tensor([[ 0.1483, -0.3293,  0.1842,  ...,  0.1281,  0.3018,  0.0837],\n",
       "         [-0.2546, -0.1025,  0.0375,  ..., -0.1661,  0.2649, -0.0537],\n",
       "         [ 0.5557, -0.2404,  0.2537,  ...,  0.2489, -0.4336, -0.2517],\n",
       "         ...,\n",
       "         [ 0.1147, -0.0643,  0.2102,  ...,  0.0982,  0.2448,  0.1059],\n",
       "         [ 0.1241, -0.3018, -0.0366,  ...,  0.0471, -0.0987, -0.0247],\n",
       "         [-0.1238,  0.0363,  0.0179,  ..., -0.1735,  0.0165,  0.0449]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.context_attn.final_linear.bias': tensor([-0.4980, -0.1772, -0.2944,  ...,  0.3176,  0.3970, -0.3376],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.layer_norm_2.weight': tensor([0.2180, 0.2142, 0.2432,  ..., 0.2505, 0.1937, 0.5459],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.layer_norm_2.bias': tensor([ 0.0131, -0.0211, -0.0297,  ..., -0.0921, -0.0276, -0.1627],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.feed_forward.w_1.weight': tensor([[ 0.2133,  0.1367,  0.1583,  ...,  0.1154,  0.1157,  0.2507],\n",
       "         [ 0.5381,  0.2512,  0.0615,  ..., -0.1866,  0.0807, -0.4976],\n",
       "         [ 0.1731, -0.0155,  0.1274,  ..., -0.0911, -0.0814,  0.0748],\n",
       "         ...,\n",
       "         [-0.4023,  0.0764,  0.1492,  ...,  0.0858, -0.1229, -0.1460],\n",
       "         [ 0.3167, -0.4993, -0.1406,  ..., -0.0420, -0.0975, -0.1920],\n",
       "         [ 0.0118, -0.0053,  0.3694,  ..., -0.1689,  0.1758,  0.3420]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.feed_forward.w_1.bias': tensor([-0.0565, -0.1241,  0.1722,  ..., -0.1564, -0.1884, -0.1355],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.feed_forward.w_2.weight': tensor([[-0.2155,  0.0026, -0.0821,  ...,  0.2438,  0.4438, -0.1379],\n",
       "         [-0.0099,  0.0284,  0.1877,  ..., -0.3665, -0.3655, -0.4993],\n",
       "         [ 0.2686, -0.4487, -0.1062,  ..., -0.1671,  0.3542,  0.2520],\n",
       "         ...,\n",
       "         [-0.0320,  0.1042,  0.0439,  ...,  0.0439, -0.1384,  0.1189],\n",
       "         [-0.0164, -0.0094, -0.0580,  ..., -0.0873,  0.1727,  0.1772],\n",
       "         [-0.0598, -0.1796, -0.1490,  ...,  0.0716, -0.3953,  0.0061]],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.feed_forward.w_2.bias': tensor([ 0.1031,  0.1510, -0.1302,  ...,  0.2443, -0.0541, -0.2505],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.feed_forward.layer_norm.weight': tensor([1.0381, 1.0693, 1.0654,  ..., 1.0059, 1.0098, 1.0000],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.transformer_layers.11.feed_forward.layer_norm.bias': tensor([ 0.1394,  0.1179,  0.1831,  ..., -0.1677,  0.1306, -0.0412],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.layer_norm.weight': tensor([0.6274, 1.0703, 1.1914,  ..., 0.8398, 0.4060, 0.4321],\n",
       "        dtype=torch.float16),\n",
       " 'decoder.layer_norm.bias': tensor([ 0.0264,  0.0236,  0.0949,  ...,  0.3755, -0.0079, -0.0789],\n",
       "        dtype=torch.float16)}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omnt_checkpoint['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weight': tensor([[-0.0321,  0.0348,  0.0181,  ...,  0.0312, -0.0099, -0.0133],\n",
       "         [-0.0039,  0.0104, -0.0156,  ...,  0.0290, -0.0138, -0.0134],\n",
       "         [-0.0245, -0.0283, -0.0295,  ...,  0.9712, -0.0255, -0.0273],\n",
       "         ...,\n",
       "         [-0.0123, -0.0031, -0.0089,  ...,  0.0645, -0.0182, -0.0740],\n",
       "         [ 0.0085, -0.0088, -0.0091,  ...,  0.0571, -0.0035, -0.1298],\n",
       "         [-0.0076, -0.0107, -0.0051,  ...,  1.0264, -0.0338, -0.1175]],\n",
       "        dtype=torch.float16),\n",
       " 'bias': tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omnt_checkpoint['generator']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From FB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['args', 'cfg', 'model', 'criterion', 'optimizer_history', 'task_state', 'extra_state', 'last_optimizer_state'])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "checkpoint = torch.load('checkpoint.pt', map_location=torch.device('cpu'))\n",
    "checkpoint.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['_name', 'common', 'common_eval', 'distributed_training', 'dataset', 'optimization', 'checkpoint', 'bmuf', 'generation', 'eval_lm', 'interactive', 'model', 'task', 'criterion', 'optimizer', 'lr_scheduler', 'scoring', 'bpe', 'tokenizer', 'ema'])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['cfg'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(config='', save_config=None, data={}, skip_empty_level='silent', save_data='', overwrite=False, n_sample=0, dump_transforms=False, src_vocab='', tgt_vocab='', share_vocab=True, src_feats_vocab=None, src_vocab_size=256206, tgt_vocab_size=256206, vocab_size_multiple=1, src_words_min_frequency=1, tgt_words_min_frequency=1, src_seq_length_trunc=None, tgt_seq_length_trunc=None, both_embeddings=None, src_embeddings=None, tgt_embeddings=None, embeddings_type=None, switchout_temperature=1.0, tokendrop_temperature=1.0, tokenmask_temperature=1.0, reversible_tokenization='joiner', prior_tokenization=False, src_subword_model='', tgt_subword_model='', src_subword_nbest=1, tgt_subword_nbest=1, src_subword_alpha=0.0, tgt_subword_alpha=0.0, src_subword_vocab='', tgt_subword_vocab='', src_vocab_threshold=0, tgt_vocab_threshold=0, src_subword_type='none', tgt_subword_type='none', src_onmttok_kwargs=\"{'mode': 'none'}\", tgt_onmttok_kwargs=\"{'mode': 'none'}\", src_seq_length=150, tgt_seq_length=150, src_prefix='', tgt_prefix='', permute_sent_ratio=0.0, rotate_ratio=0.0, insert_ratio=0.0, random_ratio=0.0, mask_ratio=0.0, mask_length='subword', poisson_lambda=3.0, replace_length=-1, src_word_vec_size=1024, tgt_word_vec_size=1024, word_vec_size=1024, share_decoder_embeddings=True, share_embeddings=True, position_encoding=True, position_encoding_type='SinusoidalConcat', update_vocab=False, feat_merge='concat', feat_vec_size=-1, feat_vec_exponent=0.7, model_task='seq2seq', model_type='text', model_dtype='fp16', encoder_type='transformer', decoder_type='transformer', freeze_encoder=False, freeze_decoder=False, layers=-1, enc_layers=12, dec_layers=12, hidden_size=1024, enc_hid_size=1024, dec_hid_size=1024, cnn_kernel_width=3, pos_ffn_activation_fn='relu', input_feed=1, bridge=False, rnn_type='LSTM', context_gate=None, bridge_extra_node=True, bidir_edges=True, state_dim=512, n_edge_types=2, n_node=2, n_steps=2, src_ggnn_size=0, global_attention='general', global_attention_function='softmax', self_attn_type='scaled-dot', max_relative_positions=0, heads=16, transformer_ff=4096, aan_useffn=False, add_qkvbias=True, lambda_align=0.0, alignment_layer=-3, alignment_heads=0, full_context_alignment=False, copy_attn=False, copy_attn_type='general', generator_function='softmax', copy_attn_force=False, reuse_copy_attn=False, copy_loss_by_seqlength=False, coverage_attn=False, lambda_coverage=0.0, lm_prior_model=None, lm_prior_lambda=0.0, lm_prior_tau=1.0, loss_scale=0, apex_opt_level='', data_type='text', save_model='nllb', save_checkpoint_steps=5000, keep_checkpoint=50, gpu_ranks=[0], world_size=1, gpu_backend='nccl', gpu_verbose_level=0, master_ip='localhost', master_port=10000, seed=1234, param_init=0.0, param_init_glorot=True, train_from='', reset_optim='none', pre_word_vecs_enc=None, pre_word_vecs_dec=None, freeze_word_vecs_enc=False, freeze_word_vecs_dec=False, num_workers=4, batch_size=8192, batch_size_multiple=1, batch_type='tokens', normalization='tokens', accum_count=[4], accum_steps=[0], valid_steps=5000, valid_batch_size=4096, train_steps=100000, single_pass=False, early_stopping=0, early_stopping_criteria=None, optim='', adagrad_accumulator_init=0, max_grad_norm=0.0, dropout=[0.1], attention_dropout=[0.1], dropout_steps=[0], truncated_decoder=0, adam_beta1=0.9, adam_beta2=0.98, label_smoothing=0.1, average_decay=0.0, average_every=1, learning_rate=5e-05, learning_rate_decay=0.5, start_decay_steps=50000, decay_steps=10000, decay_method='none', warmup_steps=4000, log_file='', log_file_level='0', verbose=False, train_eval_steps=200, train_metrics=[], valid_metrics=[], scoring_debug=False, dump_preds=None, report_every=100, exp_host='', exp='', tensorboard=False, tensorboard_log_dir='runs/onmt', bucket_size=262144, bucket_size_init=-1, bucket_size_increment=0, prefetch_factor=400, brnn=False, data_task='seq2seq', decoder_start_token='</s>', _all_transform={'filtertoolong'})"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omnt_checkpoint['opt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to add opt to the FB Checkpoiint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint['opt'] = omnt_checkpoint['opt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['args', 'cfg', 'model', 'criterion', 'optimizer_history', 'task_state', 'extra_state', 'last_optimizer_state', 'opt'])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(checkpoint, 'checkpoint_w_opt.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
